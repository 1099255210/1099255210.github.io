{"meta":{"version":1,"warehouse":"4.0.1"},"models":{"Asset":[{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/Hg-icon-10.png","path":"images/Hg-icon-10.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/ava.png","path":"images/ava.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/js/algolia-search.js","path":"js/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/local-search.js","path":"js/local-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/bookmark.js","path":"js/bookmark.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/next-boot.js","path":"js/next-boot.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/motion.js","path":"js/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/utils.js","path":"js/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/anime.min.js","path":"lib/anime.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/schemes/muse.js","path":"js/schemes/muse.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/schemes/pisces.js","path":"js/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/all.min.css","path":"lib/font-awesome/css/all.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-brands-400.woff2","path":"lib/font-awesome/webfonts/fa-brands-400.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-regular-400.woff2","path":"lib/font-awesome/webfonts/fa-regular-400.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-solid-900.woff2","path":"lib/font-awesome/webfonts/fa-solid-900.woff2","modified":1,"renderable":1}],"Cache":[{"_id":"source/index.md","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1661433212990},{"_id":"source/about/index.md","hash":"b35e0e104b6dcbca9462af372644aeff1e1edb1d","modified":1661433212989},{"_id":"source/home/index.md","hash":"ba11ebb6689f813e17d7d7a0224168ab827c0f3d","modified":1661433212990},{"_id":"source/_posts/CLIP 模型及其应用.md","hash":"f528fa1f981c2cf72278eed7ac16526464254213","modified":1662389002164},{"_id":"source/_posts/GSoC 2022 Proposal 申请书.md","hash":"2013b16421fc757a50421b0163db98a5857d223d","modified":1661433212983},{"_id":"source/_posts/CSGO 中的 cfg 系统.md","hash":"7e5b70d6a973da4ac7a1ae29987cfad0480880d1","modified":1661433212983},{"_id":"source/_posts/Hugging Face 学习(1).md","hash":"34d7db3a82138dd757120f4f7fc6cb4c070c567d","modified":1661433212984},{"_id":"source/_posts/Transformer 学习笔记.md","hash":"dc469a82857313eac2d350ff420c87a49023ac0a","modified":1661433212984},{"_id":"source/_posts/Ubuntu 换源.md","hash":"5ccd5c5b0179374298e23cbd79bd2f144e4cbf85","modified":1661566245083},{"_id":"source/_posts/gradio 本地部署页面空白的解决方案.md","hash":"fdc5f9426bbec32b350eb53f8bb786a200078093","modified":1662698096840},{"_id":"source/_posts/ubuntu 20.04 安装.md","hash":"3b2fa62e8fcba99cfd33bac8466b80f8f0502fce","modified":1661433212985},{"_id":"source/_posts/Windows 如何正确安装 cuda 与 cudnn.md","hash":"ef10a45de61ba6a8ee285557167df056adef6954","modified":1661604242226},{"_id":"source/_posts/windows 无法修改代理设置的解决方法.md","hash":"ed07fb1faed4b336c79dd5a59acb045c51a260b7","modified":1661465963133},{"_id":"source/_posts/在树莓派上从源码编译安装Paddlepaddle.md","hash":"9260e450200bca8b367a901665948f67e2f4f4be","modified":1682562249272},{"_id":"source/_posts/如何使用 Squoosh 命令行批量压缩图片.md","hash":"68e00566f249fbe1bdbf8a464a6420d3f24a4a66","modified":1663781050232},{"_id":"source/_posts/强化学习.md","hash":"793789e16821a818e7852f5e603a82aec885219f","modified":1661531823907},{"_id":"source/_posts/学着克服国内网络环境.jpg.md","hash":"539a413abf18b9db977a0d92b90506f05b08b5af","modified":1682561450475},{"_id":"source/_posts/微信小程序webp优化.md","hash":"0611d8daa348d6f9f53d4bb806a089c67e7d04bc","modified":1661433212986},{"_id":"source/_posts/每周报告 20220906.md","hash":"0f752520e7ce90543e0bc734d02e87f8727db00d","modified":1663155599860},{"_id":"source/_posts/搭建 CSGO 服务器(on windows).md","hash":"d172068000638ef856bec4ad77819a7e5038d16c","modified":1661433915077},{"_id":"source/_posts/注意力机制.md","hash":"e4a1ef0657f65eef0e19c22b0825d06992545a5f","modified":1661433212986},{"_id":"source/_posts/神经网络模型.md","hash":"b2a0dab666dc7415d3bfefeb586a6a104b653c55","modified":1661433212987},{"_id":"source/_posts/计算社会学 project.md","hash":"40bffa6d5599edde21602643e7a66d6ad81aa133","modified":1661433212988},{"_id":"source/_posts/知识蒸馏 (Knowledge Distillation).md","hash":"84303a73d2cde24b06dc4189decba942f178f941","modified":1661433212987},{"_id":"source/_posts/自动文本摘要(TF-ISF).md","hash":"858f0b05e8164de5c4470e05cf2c29f8c07538d5","modified":1661433212988},{"_id":"source/_posts/键盘笔记一.md","hash":"1cdb3440d5fffe07086a955cfa52133b7554f543","modified":1661433212988},{"_id":"themes/next/.editorconfig","hash":"731c650ddad6eb0fc7c3d4a91cad1698fe7ad311","modified":1661433212991},{"_id":"themes/next/.eslintrc.json","hash":"d3c11de434171d55d70daadd3914bc33544b74b8","modified":1661433212991},{"_id":"themes/next/.gitattributes","hash":"3e00e1fb043438cd820d94ee3dc9ffb6718996f3","modified":1661433212992},{"_id":"themes/next/.gitignore","hash":"83418530da80e6a78501e1d62a89c3bf5cbaec3d","modified":1661433213001},{"_id":"themes/next/.stylintrc","hash":"6259e2a0b65d46865ab89564b88fc67638668295","modified":1661433213001},{"_id":"themes/next/.travis.yml","hash":"379f31a140ce41e441442add6f673bf397d863ea","modified":1661433213002},{"_id":"themes/next/LICENSE.md","hash":"0a9c7399f102b4eb0a6950dd31264be421557c7d","modified":1661433213002},{"_id":"themes/next/_config.yml","hash":"c6a33e013a7fa5d4d0af05249a6b4c9832555ec7","modified":1661433213003},{"_id":"themes/next/README.md","hash":"7d56751b580d042559b2acf904fca4b42bcb30a7","modified":1661433213003},{"_id":"themes/next/gulpfile.js","hash":"0c76a1ac610ee8cbe8e2cc9cca1c925ffd0edf98","modified":1661433213016},{"_id":"themes/next/crowdin.yml","hash":"4a53f5985e545c635cb56b2a57ed290cb8cf8942","modified":1661433213004},{"_id":"themes/next/package.json","hash":"b099e7cea4406e209130410d13de87988ba37b2a","modified":1661433213072},{"_id":"themes/next/.github/CODE_OF_CONDUCT.md","hash":"778b7e052993ed59f21ed266ba7119ee2e5253fb","modified":1661433212993},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"5ddde54fb50d11dc08cec899a3588addb56aa386","modified":1661433212993},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"d2f8e6b65783e31787feb05d2ccea86151f53f35","modified":1661433212996},{"_id":"themes/next/.github/config.yml","hash":"df3d970700e6b409edc3d23be8d553db78d5ba3f","modified":1661433212996},{"_id":"themes/next/.github/issue_label_bot.yaml","hash":"533fbe6b2f87d7e7ec6949063bb7ea7eb4fbe52d","modified":1661433212998},{"_id":"themes/next/.github/lock.yml","hash":"3ce3d0a26030a1cd52b273cc6a6d444d7c8d85c2","modified":1661433212998},{"_id":"themes/next/.github/release-drafter.yml","hash":"09c3352b2d643acdc6839601ceb38abc38ab97c5","modified":1661433212999},{"_id":"themes/next/.github/mergeable.yml","hash":"1c1cb77a62df1e3654b151c2da34b4a10d351170","modified":1661433212999},{"_id":"themes/next/.github/stale.yml","hash":"590b65aca710e0fba75d3cf5361a64d13b6b0f63","modified":1661433213000},{"_id":"themes/next/.github/issue-close-app.yml","hash":"b14756e65546eb9ecc9d4393f0c9a84a3dac1824","modified":1661433212997},{"_id":"themes/next/.github/support.yml","hash":"7ce2722d6904c31a086444c422dc49b6aa310651","modified":1661433213000},{"_id":"themes/next/docs/ALGOLIA-SEARCH.md","hash":"60c7e9ef0c578deebad43e9395c958fa61096baf","modified":1661433213005},{"_id":"themes/next/docs/AGPL3.md","hash":"f463f95b169d64983f59fa6f3e4b6760290a0e6b","modified":1661433213004},{"_id":"themes/next/docs/DATA-FILES.md","hash":"980fb8d37701f7fd96b30bb911519de3bbb473d1","modified":1661433213006},{"_id":"themes/next/docs/AUTHORS.md","hash":"cde7cc095ac31b421a573042cf61060f90d9ad0d","modified":1661433213005},{"_id":"themes/next/docs/INSTALLATION.md","hash":"07ea00bee149a1bdc9073e903ee6b411e9f2f818","modified":1661433213006},{"_id":"themes/next/docs/LEANCLOUD-COUNTER-SECURITY.md","hash":"6cc663db5e99fd86bb993c10d446ad26ada88e58","modified":1661433213007},{"_id":"themes/next/docs/LICENSE.txt","hash":"ae5ad07e4f4106bad55535dba042221539e6c7f9","modified":1661433213007},{"_id":"themes/next/docs/MATH.md","hash":"f56946053ade0915ff7efa74d43c38b8dd9e63bb","modified":1661433213008},{"_id":"themes/next/docs/UPDATE-FROM-5.1.X.md","hash":"1e86d32063b490d204baa9d45d8d3cb22c24a37d","modified":1661433213008},{"_id":"themes/next/languages/de.yml","hash":"15078b7ede1b084e8a6a15d271f0db9c325bd698","modified":1661433213017},{"_id":"themes/next/languages/ar.yml","hash":"abcf220bd615cec0dd50e4d98da56580169d77e1","modified":1661433213017},{"_id":"themes/next/languages/en.yml","hash":"dbb64776f9c001c54d0058256c415a9a0724ed5d","modified":1661433213018},{"_id":"themes/next/languages/default.yml","hash":"ea5e6aee4cb14510793ac4593a3bddffe23e530c","modified":1661433213017},{"_id":"themes/next/languages/es.yml","hash":"f064c793d56a5e0f20cda93b6f0e355044efc7d8","modified":1661433213018},{"_id":"themes/next/languages/fa.yml","hash":"6c0a7d5bcc26eb45a9f3e02f13117c668e77fffd","modified":1661433213019},{"_id":"themes/next/languages/fr.yml","hash":"3e2f89d4bb4441d33ecc7b5a4ee114f627603391","modified":1661433213019},{"_id":"themes/next/languages/it.yml","hash":"46222f468e66789e9ba13095809eb5e5b63edf30","modified":1661433213021},{"_id":"themes/next/languages/hu.yml","hash":"0ea89ffaefd02a10494995f05a2a59d5e5679a28","modified":1661433213020},{"_id":"themes/next/languages/ja.yml","hash":"bf279d0eb1911806d01a12f27261fbc76a3bb3f9","modified":1661433213021},{"_id":"themes/next/languages/ko.yml","hash":"af4be6cb394abd4e2e9a728418897d2ed4cc5315","modified":1661433213022},{"_id":"themes/next/languages/id.yml","hash":"7599bb0ecf278beb8fde3d17bfc148a3241aef82","modified":1661433213020},{"_id":"themes/next/languages/nl.yml","hash":"9749cf90b250e631dd550a4f32ada3bb20f66dd0","modified":1661433213022},{"_id":"themes/next/languages/pt.yml","hash":"f6606dd0b916a465c233f24bd9a70adce34dc8d6","modified":1661433213023},{"_id":"themes/next/languages/pt-BR.yml","hash":"69aa3bef5710b61dc9a0f3b3a8f52f88c4d08c00","modified":1661433213023},{"_id":"themes/next/languages/tr.yml","hash":"c4e9ab7e047ae13a19f147c6bec163c3ba2c6898","modified":1661433213024},{"_id":"themes/next/languages/uk.yml","hash":"69ef00b1b8225920fcefff6a6b6f2f3aad00b4ce","modified":1661433213024},{"_id":"themes/next/languages/ru.yml","hash":"012abc694cf9de281a0610f95f79c594f0a16562","modified":1661433213024},{"_id":"themes/next/languages/vi.yml","hash":"6a578cc28773bd764f4418110500478f185d6efa","modified":1661433213025},{"_id":"themes/next/languages/zh-CN.yml","hash":"81d73e21402dad729053a3041390435f43136a68","modified":1661433213025},{"_id":"themes/next/languages/zh-HK.yml","hash":"92ccee40c234626bf0142152949811ebe39fcef2","modified":1661433213026},{"_id":"themes/next/layout/_layout.swig","hash":"9554bd0f5c5a0438aa7b64065be5561c374d260e","modified":1661433213027},{"_id":"themes/next/languages/zh-TW.yml","hash":"cf0740648725983fb88409d6501876f8b79db41d","modified":1661433213026},{"_id":"themes/next/layout/archive.swig","hash":"d9bca77f6dcfef71e300a294f731bead11ce199f","modified":1661433213070},{"_id":"themes/next/layout/category.swig","hash":"c546b017a956faaa5f5643c7c8a363af7ac9d6b9","modified":1661433213070},{"_id":"themes/next/layout/index.swig","hash":"8dfd96fb6f833dd5d037de800813105654e8e8e6","modified":1661433213070},{"_id":"themes/next/layout/page.swig","hash":"357d916694d4c9a0fd1140fa56d3d17e067d8b52","modified":1661433213071},{"_id":"themes/next/layout/tag.swig","hash":"d44ff8755727f6532e86fc9fc8dc631200ffe161","modified":1661433213072},{"_id":"themes/next/layout/post.swig","hash":"5f0b5ba2e0a5b763be5e7e96611865e33bba24d7","modified":1661433213071},{"_id":"themes/next/scripts/renderer.js","hash":"e3658eea97b1183ee2e9f676231e53f7994741f6","modified":1661433213085},{"_id":"themes/next/.github/ISSUE_TEMPLATE/feature-request.md","hash":"6beeca0f45a429cd932b6e648617f548ff64c27c","modified":1661433212995},{"_id":"themes/next/.github/ISSUE_TEMPLATE/bug-report.md","hash":"e67146befddec3a0dc47dc80d1109070c71d5d04","modified":1661433212994},{"_id":"themes/next/.github/ISSUE_TEMPLATE/question.md","hash":"59275aa0582f793fee7be67904dcf52ad33a7181","modified":1661433212996},{"_id":"themes/next/.github/ISSUE_TEMPLATE/other.md","hash":"d5aa1a3323639a36bcd9a401484b67537043cd3c","modified":1661433212995},{"_id":"themes/next/docs/ru/DATA-FILES.md","hash":"54e6a067ed95268eab6be2ba040a7e9b1907928e","modified":1661433213009},{"_id":"themes/next/docs/ru/README.md","hash":"1e5ddb26ad6f931f8c06ce2120f257ff38b74fdf","modified":1661433213010},{"_id":"themes/next/docs/ru/INSTALLATION.md","hash":"a9cfe5ac9ef727a8650b2b6584482751a26b1460","modified":1661433213009},{"_id":"themes/next/docs/ru/UPDATE-FROM-5.1.X.md","hash":"cb8e39c377fc4a14aaf133b4d1338a48560e9e65","modified":1661433213010},{"_id":"themes/next/docs/zh-CN/ALGOLIA-SEARCH.md","hash":"3202be9a8d31986caac640e7a4c7ce22e99917eb","modified":1661433213011},{"_id":"themes/next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"7e6f227f2aaf30f400d4c065650a4e3d0d61b9e1","modified":1661433213011},{"_id":"themes/next/docs/zh-CN/DATA-FILES.md","hash":"2d868cd271d78b08775e28c5b976de8836da4455","modified":1661433213013},{"_id":"themes/next/docs/zh-CN/INSTALLATION.md","hash":"716111dd36d276f463c707dfcc9937fea2a1cf7a","modified":1661433213013},{"_id":"themes/next/docs/zh-CN/CONTRIBUTING.md","hash":"611f2930c2b281b80543531b1bf33d082531456a","modified":1661433213012},{"_id":"themes/next/docs/zh-CN/MATH.md","hash":"0d46f9f50cf2e4183970adce705d1041155b0d37","modified":1661433213014},{"_id":"themes/next/docs/zh-CN/README.md","hash":"8f7c0d0b766024152591d4ccfac715c8e18b37f3","modified":1661433213015},{"_id":"themes/next/docs/zh-CN/UPDATE-FROM-5.1.X.md","hash":"b3201934b966bc731eaf8a4dad4ba4bdcd300c10","modified":1661433213016},{"_id":"themes/next/docs/zh-CN/LEANCLOUD-COUNTER-SECURITY.md","hash":"50ab381c27611d5bf97bb3907b5ca9998f28187d","modified":1661433213014},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"30ade8c806d7826cc50a4a3e46a9e6213fddf333","modified":1661433213027},{"_id":"themes/next/layout/_macro/post.swig","hash":"c3fd56bac90ce45a0c79ddfe68beb223ad0d72b4","modified":1661433213028},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"5bffdb1448caca7db7b1f84e1693e6657a106d50","modified":1661433213029},{"_id":"themes/next/layout/_partials/comments.swig","hash":"142efb4c6b73d8f736f6784804b40d5871333172","modified":1661433213030},{"_id":"themes/next/layout/_partials/footer.swig","hash":"e031914c98f082d918ece4c35fdd0a5be1c4e845","modified":1661433213031},{"_id":"themes/next/layout/_partials/languages.swig","hash":"c3ea82604a5853fb44c5f4e4663cbe912aa5dcf8","modified":1661433213037},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"2de77d533c91532a8a4052000244d0c1693370df","modified":1661433213040},{"_id":"themes/next/layout/_partials/widgets.swig","hash":"5392dcbb504266f0f61d5b8219914068ef9cdc25","modified":1661433213046},{"_id":"themes/next/layout/_third-party/baidu-push.swig","hash":"28b0a7e843ec4365db1963646659a153753cd746","modified":1661433213054},{"_id":"themes/next/layout/_third-party/index.swig","hash":"c6b63cbc80938e6e09578b8c67e01adf13a9e3bd","modified":1661433213059},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"269102fc5e46bd1ce75abdcce161f0570ae70e2f","modified":1661433213063},{"_id":"themes/next/layout/_third-party/quicklink.swig","hash":"5ae5adcd6f63ed98b2071e4f7e5e38c4d7d24e1b","modified":1661433213062},{"_id":"themes/next/layout/_scripts/index.swig","hash":"1822eaf55bbb4bec88871c324fc18ad95580ccb4","modified":1661433213047},{"_id":"themes/next/layout/_scripts/noscript.swig","hash":"7b9e0f776a5be6c3f95bc7f394e1424ba02ba93b","modified":1661433213047},{"_id":"themes/next/layout/_scripts/pjax.swig","hash":"ccff5a773644d33ff22f6b45b6734f52b048f22b","modified":1661433213049},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"244ca2d74ee0d497c87572c6a26b43c62a952673","modified":1661433213051},{"_id":"themes/next/layout/_scripts/three.swig","hash":"6b092c6d882b2dfa5273e1b3f60b244cb7c29fcd","modified":1661433213051},{"_id":"themes/next/scripts/filters/default-injects.js","hash":"ad321db012cea520066deb0639335e9bc0dcc343","modified":1661433213080},{"_id":"themes/next/scripts/events/index.js","hash":"5c355f10fe8c948a7f7cd28bd8120adb7595ebde","modified":1661433213073},{"_id":"themes/next/scripts/filters/front-matter.js","hash":"305d03c1e45782988809298c3e3b3c5d5ee438aa","modified":1661433213081},{"_id":"themes/next/scripts/filters/locals.js","hash":"a5e7d05d3bd2ae6dcffad5a8ea0f72c6e55dbd02","modified":1661433213081},{"_id":"themes/next/scripts/helpers/engine.js","hash":"eb6b8bbc1dce4846cd5e0fac0452dbff56d07b5d","modified":1661433213083},{"_id":"themes/next/scripts/filters/minify.js","hash":"21196a48cb127bf476ce598f25f24e8a53ef50c2","modified":1661433213082},{"_id":"themes/next/scripts/helpers/font.js","hash":"8fb1c0fc745df28e20b96222974402aab6d13a79","modified":1661433213083},{"_id":"themes/next/scripts/filters/post.js","hash":"57f2d817578dd97e206942604365e936a49854de","modified":1661433213082},{"_id":"themes/next/scripts/helpers/next-url.js","hash":"4044129368d0e2811859a9661cad8ab47118bc32","modified":1661433213084},{"_id":"themes/next/scripts/helpers/next-config.js","hash":"b8d7ddfa4baa9b8d6b9066a634aa81c6243beec9","modified":1661433213084},{"_id":"themes/next/scripts/tags/button.js","hash":"bb0e8abbc0a6d5b3a1a75a23976f2ac3075aab31","modified":1661433213085},{"_id":"themes/next/scripts/tags/caniuse.js","hash":"840536754121e0da5968f5ad235f29200fc5d769","modified":1661433213086},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"e2d0184bc4a557e1017395b80ff46880078d8537","modified":1661433213086},{"_id":"themes/next/scripts/tags/label.js","hash":"fc83f4e1be2c34e81cb79938f4f99973eba1ea60","modified":1661433213087},{"_id":"themes/next/scripts/tags/note.js","hash":"1fdf4f95810fdb983bfd5ad4c4f13fedd4ea2f8d","modified":1661433213088},{"_id":"themes/next/scripts/tags/mermaid.js","hash":"81134494ff0134c0dae1b3815caf6606fccd4e46","modified":1661433213088},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"93ccd3f99d3cb42674f29183c756df63acb5d7f8","modified":1661433213087},{"_id":"themes/next/scripts/tags/pdf.js","hash":"37b53661ad00a01a2ca7d2e4a5ad3a926073f8e2","modified":1661433213088},{"_id":"themes/next/scripts/tags/tabs.js","hash":"c70a4a66fd0c28c98ccb6c5d5f398972e5574d28","modified":1661433213089},{"_id":"themes/next/scripts/tags/video.js","hash":"944293fec96e568d9b09bc1280d5dbc9ee1bbd17","modified":1661433213089},{"_id":"themes/next/source/css/_colors.styl","hash":"11aef31a8e76f0f332a274a8bfd4537b73d4f88f","modified":1661433213090},{"_id":"themes/next/source/css/_mixins.styl","hash":"072a3fa473c19b20ccd7536a656cda044dbdae0a","modified":1661433213125},{"_id":"themes/next/source/css/main.styl","hash":"815ef30987d02f3d76dbe4b5ee3a72135a152678","modified":1661433213138},{"_id":"themes/next/source/images/Hg-icon-10.png","hash":"32d48c64dafba86f1d5ec1d15d05f3f55610cddf","modified":1661433213139},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1661433213140},{"_id":"themes/next/source/images/avatar.gif","hash":"18c53e15eb0c84b139995f9334ed8522b40aeaf6","modified":1661433213141},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1661433213142},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1661433213143},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1661433213140},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1661433213142},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1661433213144},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1661433213143},{"_id":"themes/next/source/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1661433213144},{"_id":"themes/next/source/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1661433213145},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"81b853c74dd83f724541efe707e025dff32b5c05","modified":1661433213145},{"_id":"themes/next/source/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1661433213146},{"_id":"themes/next/source/js/algolia-search.js","hash":"6a813410e33824d7acc65a369a2983912bb3420c","modified":1661433213147},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"2da5363c37d62c34e044d5442f3ccf19266731ec","modified":1661433213146},{"_id":"themes/next/source/js/local-search.js","hash":"cfa6a0f3f9c2bc759ee507668a21f4e8f250f42a","modified":1661433213148},{"_id":"themes/next/source/js/bookmark.js","hash":"9f05fd3672789311dc0cf5b37e40dc654cb04a2a","modified":1661433213147},{"_id":"themes/next/source/js/next-boot.js","hash":"250d8dcd6322e69e3fbadd0f3e37081c97b47c52","modified":1661433213149},{"_id":"themes/next/source/js/motion.js","hash":"d5aa1a08cdf3c8d1d8d550fb1801274cc41e5874","modified":1661433213148},{"_id":"themes/next/source/js/utils.js","hash":"26a82e46fdcadc7c3c2c56a7267284b61a26f7f3","modified":1661433213151},{"_id":"themes/next/source/lib/anime.min.js","hash":"960be51132134acd65c2017cc8a5d69cb419a0cd","modified":1661433213152},{"_id":"themes/next/layout/_partials/head/head.swig","hash":"90cce9f407e9490756ba99580e3eb09f55b05eaa","modified":1661433213032},{"_id":"themes/next/layout/_partials/head/head-unique.swig","hash":"7d638e413f2548fc990c4a467dd03de6c81fc960","modified":1661433213032},{"_id":"themes/next/layout/_partials/header/menu-item.swig","hash":"4baa86ca631168fc6388d27f4b1b501b40c877a8","modified":1661433213035},{"_id":"themes/next/layout/_partials/header/brand.swig","hash":"91056a6c98cca63ff8cc6956e531ee3faf4b8ad9","modified":1661433213034},{"_id":"themes/next/layout/_partials/header/index.swig","hash":"0dd316f153c492c0a03bd0273d50fa322bc81f11","modified":1661433213034},{"_id":"themes/next/layout/_partials/header/menu.swig","hash":"90d3eaba6fbe69bee465ddd67c467fd2c0239dc4","modified":1661433213036},{"_id":"themes/next/layout/_partials/header/sub-menu.swig","hash":"bed6cc2b48cf2655036ba39c9bae73a295228a4d","modified":1661433213037},{"_id":"themes/next/layout/_partials/page/breadcrumb.swig","hash":"91c0addb33006619faa4c32e5d66874e25f1e9b3","modified":1661433213039},{"_id":"themes/next/layout/_partials/page/page-header.swig","hash":"8d4e3dd0d3631ce0b21bc15c259f6ac886de631d","modified":1661433213039},{"_id":"themes/next/layout/_partials/post/post-copyright.swig","hash":"f2eb455c8bf13533427254f0c9b4b17b2498168b","modified":1661433213041},{"_id":"themes/next/layout/_partials/post/post-followme.swig","hash":"d8f785c062c6b0763a778bd4a252e6f5fee0e432","modified":1661433213041},{"_id":"themes/next/layout/_partials/post/post-footer.swig","hash":"ce712c110b5ce8aacba7a86b0558ff89700675c9","modified":1661433213042},{"_id":"themes/next/layout/_partials/post/post-reward.swig","hash":"f349a226e5370075bb6924e60da8b0170c7cfcc1","modified":1661433213043},{"_id":"themes/next/layout/_partials/search/algolia-search.swig","hash":"98fd1f5df044f4534e1d4ca9ab092ba5761739a9","modified":1661433213044},{"_id":"themes/next/layout/_partials/search/index.swig","hash":"a6c761d5193cb6f22e9422dbbcf209e05471b0ed","modified":1661433213045},{"_id":"themes/next/layout/_partials/sidebar/site-overview.swig","hash":"7b2ef5db9615267a24b884388925de1e9b447c1f","modified":1661433213046},{"_id":"themes/next/layout/_partials/post/post-related.swig","hash":"bc7b047a6246df07767373644b1637d91c3a88b1","modified":1661433213043},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"128f7d679bb4d53b29203d598d217f029a66dee7","modified":1661433213045},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"b8819bd056f8a580c5556d4415836a906ed5d7a4","modified":1661433213053},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"84adaadd83ce447fa9da2cff19006334c9fcbff9","modified":1661433213052},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"85b60e222712ca3b2c4dc2039de2dc36b8d82940","modified":1661433213054},{"_id":"themes/next/layout/_third-party/analytics/growingio.swig","hash":"91c2cb900c76224c5814eeb842d1d5f517f9bf05","modified":1661433213053},{"_id":"themes/next/layout/_third-party/chat/chatra.swig","hash":"2642e8aef5afbe23a2a76efdc955dab2ee04ed48","modified":1661433213055},{"_id":"themes/next/layout/_third-party/chat/tidio.swig","hash":"fb94ee487d75e484e59b7fba96e989f699ff8a83","modified":1661433213056},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"9298e6d6c4a62a0862fc0f4060ed99779d7b68cb","modified":1661433213056},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"1b29b99fa921f12c25d3dc95facdf84ef7bb1b5c","modified":1661433213057},{"_id":"themes/next/layout/_third-party/comments/disqusjs.swig","hash":"a42f97eda3748583bac2253c47fe5dfa54f07b8f","modified":1661433213057},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"ae2707d6e47582bb470c075649ec7bad86a6d5a9","modified":1661433213059},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"3d91899ca079e84d95087b882526d291e6f53918","modified":1661433213058},{"_id":"themes/next/layout/_third-party/comments/gitalk.swig","hash":"606ad14a29320157df9b8f33738282c51bb393d9","modified":1661433213058},{"_id":"themes/next/layout/_third-party/math/katex.swig","hash":"276f523e414d4aa7f350a8f2fd3df8a3d8ea9656","modified":1661433213061},{"_id":"themes/next/layout/_third-party/math/index.swig","hash":"59df21fcfe9d0ada8cee3188cb1075529c1c3eb8","modified":1661433213060},{"_id":"themes/next/layout/_third-party/math/mathjax.swig","hash":"1f34b2d3c753a3589ab6c462880bd4eb7df09914","modified":1661433213062},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"58296a5c1883f26464c2a5ccf734c19f5fbf395a","modified":1661433213064},{"_id":"themes/next/layout/_third-party/search/swiftype.swig","hash":"aa6ab95b8b76611694613defb4bf25003d1b927f","modified":1661433213065},{"_id":"themes/next/layout/_third-party/tags/mermaid.swig","hash":"619338ddacf01e3df812e66a997e778f672f4726","modified":1661433213068},{"_id":"themes/next/layout/_third-party/search/algolia-search.swig","hash":"fd726aad77a57b288f07d6998ec29291c67c7cbb","modified":1661433213064},{"_id":"themes/next/layout/_third-party/tags/pdf.swig","hash":"5a223b60406cee7438cfe3a5e41d1284425aa7a5","modified":1661433213069},{"_id":"themes/next/layout/_third-party/statistics/busuanzi-counter.swig","hash":"d2f0e4c598410ec33785abe302c7ea7492bb791a","modified":1661433213066},{"_id":"themes/next/layout/_third-party/statistics/cnzz-analytics.swig","hash":"53a0760c75d5aaabb3ce8e8aa8e003510d59807f","modified":1661433213066},{"_id":"themes/next/layout/_third-party/statistics/firestore.swig","hash":"01d94354d07e72cad47100482068b6be69fcc033","modified":1661433213067},{"_id":"themes/next/layout/_third-party/statistics/lean-analytics.swig","hash":"c171ea94e9afbba97f06856904264da331559463","modified":1661433213068},{"_id":"themes/next/layout/_scripts/pages/schedule.swig","hash":"34c05e9d73b0f081db70990c296b6d6a0f8ea2ca","modified":1661433213048},{"_id":"themes/next/layout/_third-party/statistics/index.swig","hash":"964cd6bac668cf6d211a2624fbef3948cfdece55","modified":1661433213067},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"0b44f400ec00d2b5add5ee96c11d22465c432376","modified":1661433213050},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"34495d408e8467555afee489500b8aad98c52079","modified":1661433213049},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"0b44f400ec00d2b5add5ee96c11d22465c432376","modified":1661433213050},{"_id":"themes/next/scripts/events/lib/config.js","hash":"aefe3b38a22bc155d485e39187f23e4f2ee5680a","modified":1661433213074},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"34495d408e8467555afee489500b8aad98c52079","modified":1661433213050},{"_id":"themes/next/scripts/events/lib/injects-point.js","hash":"08496b71c9939718e7955704d219e44d7109247b","modified":1661433213074},{"_id":"themes/next/scripts/events/lib/injects.js","hash":"e73f697bb160b223fdde783237148be5f41c1d78","modified":1661433213075},{"_id":"themes/next/scripts/filters/comment/changyan.js","hash":"2f22f48f7370470cef78561a47c2a47c78035385","modified":1661433213076},{"_id":"themes/next/scripts/filters/comment/default-config.js","hash":"0c3bea89d64bc12c1bbe6f208a83773c6fb5375a","modified":1661433213077},{"_id":"themes/next/scripts/filters/comment/common.js","hash":"713056d33dbcd8e9748205c5680b456c21174f4e","modified":1661433213076},{"_id":"themes/next/scripts/filters/comment/disqusjs.js","hash":"67cf90d9a2428c14eb113a64bdd213c22a019aef","modified":1661433213078},{"_id":"themes/next/scripts/filters/comment/disqus.js","hash":"3a80559df0b670ccb065ea9d3bb587d0b61be3a4","modified":1661433213078},{"_id":"themes/next/scripts/filters/comment/gitalk.js","hash":"323a47df6ded894944a2647db44556d6163e67c4","modified":1661433213079},{"_id":"themes/next/scripts/filters/comment/livere.js","hash":"a4f3153ac76a7ffdf6cc70f52f1b2cc218ed393e","modified":1661433213079},{"_id":"themes/next/scripts/filters/comment/valine.js","hash":"851359f5ff90f733a9bd7fe677edbee8b8ac714c","modified":1661433213080},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"583ff1e7a2ca889f1f54eb0ca793894466823c7c","modified":1661433213135},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"5980abbbbeacd8541121f436fa414d24ad5e97c2","modified":1661433213136},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"c22b58af3327236ec54d5706501aa5a20e15012e","modified":1661433213136},{"_id":"themes/next/source/css/_variables/base.styl","hash":"a9550e5dd239895a9e10f745e9f26b5ea36a4e7e","modified":1661433213137},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"4e33774b1fe6d0a51f3a428c54c5e600e83bf154","modified":1661433213137},{"_id":"themes/next/source/js/schemes/muse.js","hash":"a18559a9c332199efad0100cf84bb0c23fc0f17a","modified":1661433213150},{"_id":"themes/next/source/js/schemes/pisces.js","hash":"b85a6e2af1387fe64b51e7cd3e2da8616e6f5a3f","modified":1661433213150},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"bf172816a9c57f9040e3d19c24e181a142daf92b","modified":1661433213157},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"dde584994ac13dc601836e86f4cf490e418d9723","modified":1661433213157},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"510a6f0ba7485dd54ce347cca890ab52c4957081","modified":1661433213091},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"0534b329d279a6f255112b3305ff92c810f31724","modified":1661433213092},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"d17236df3b4d6def1e4e81133ef4729c390de3ac","modified":1661433213092},{"_id":"themes/next/source/css/_common/components/reading-progress.styl","hash":"c52648a7b09f9fe37858f5694fcc1ffc709ad147","modified":1661433213101},{"_id":"themes/next/source/css/_common/outline/mobile.styl","hash":"a2ee16cac29a82cfce26804c160286fcbee94161","modified":1661433213109},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"7a95c27762e1303bf06ee808c63f616cb192fcaf","modified":1661433213109},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"5540c9259cb7895a5f10a289c7937e5470a7c134","modified":1661433213115},{"_id":"themes/next/source/css/_common/scaffolding/buttons.styl","hash":"45f4badac6ec45cf24355f6157aece1d4d3f1134","modified":1661433213116},{"_id":"themes/next/source/css/_common/scaffolding/comments.styl","hash":"4b068d0d898f4e624937503f0e1428993050bd65","modified":1661433213116},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ff27192c6c18e74bbc7f898d3ff16f4b120381b8","modified":1661433213119},{"_id":"themes/next/source/css/_common/scaffolding/pagination.styl","hash":"b619f39e18398422e0ac4999d8f042a5eaebe9cd","modified":1661433213119},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"f317d2e3886e94f5fbb8781c2e68edd19669ff58","modified":1661433213120},{"_id":"themes/next/source/css/_common/scaffolding/toggles.styl","hash":"20e0e3e3eba384930c022e21511214d244b4c9e7","modified":1661433213124},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"43045d115f8fe95732c446aa45bf1c97609ff2a5","modified":1661433213120},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"e342b8f8e11a3a6aa5a029912c9778c25bf5d135","modified":1661433213126},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"b9e87d32da24264bda247c1526afe140c858b0ef","modified":1661433213126},{"_id":"themes/next/source/css/_schemes/Mist/_layout.styl","hash":"12b265f82840f27112ca2b1be497677f20f87545","modified":1661433213127},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expand.styl","hash":"e1c29b81a32273a0dedd926cda199a71aea72624","modified":1661433213128},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"716e8b0f056bf6393e6bc6969ac84598ab8e7a6f","modified":1661433213127},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"c5142739e01e9f25c8b32b2209af85c787bb2b42","modified":1661433213128},{"_id":"themes/next/source/css/_schemes/Muse/_header.styl","hash":"8674bd88df076a1dfe4023ed6750ded1f5b00223","modified":1661433213129},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"49c76bc723d3952abb613d9d68398ed7305da999","modified":1661433213129},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"4b7f057dbb53efd7cbe7eac7835a793ab3cbb135","modified":1661433213130},{"_id":"themes/next/source/css/_schemes/Muse/_sidebar.styl","hash":"9898323ee5a7ac2a5d4f633c653112280beb2643","modified":1661433213130},{"_id":"themes/next/source/css/_schemes/Muse/_sub-menu.styl","hash":"2d3e05015796a790abd9d68957a5c698c0c9f9b6","modified":1661433213131},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"25c2a7930da14f023329df20f38df2728057fb4d","modified":1661433213131},{"_id":"themes/next/source/css/_schemes/Pisces/_header.styl","hash":"558794fced306339b98dc2b0ee7f0576802f1355","modified":1661433213132},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"5de34e1d8a290751641ae456c942410852d5e809","modified":1661433213133},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"0a9f0d9eb042595502d200fb8c65efb0e6c89aa9","modified":1661433213133},{"_id":"themes/next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"b69ac38b9da8c9c1b7de696fdeea7f9d7705213a","modified":1661433213134},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"dc9318992ce2eb086ebaa2fe56b325e56d24098b","modified":1661433213134},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"25c2a7930da14f023329df20f38df2728057fb4d","modified":1661433213135},{"_id":"themes/next/source/lib/font-awesome/css/all.min.css","hash":"82e34d28f8a1169b20b60101d5bb0446deba3514","modified":1661433213153},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-regular-400.woff2","hash":"260bb01acd44d88dcb7f501a238ab968f86bef9e","modified":1661433213155},{"_id":"themes/next/source/css/_common/components/pages/breadcrumb.styl","hash":"236a039b0900f4267de566b46f62314ad967d30f","modified":1661433213093},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"6cf78a379bb656cc0abb4ab80fcae60152ce41ad","modified":1661433213094},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"18edddb2ffb3f85a68e4367f81e06c461e07bc25","modified":1661433213094},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"f6f05f02d50f742c84ee5122016c0563a8bb2cf9","modified":1661433213094},{"_id":"themes/next/source/css/_common/components/pages/tag-cloud.styl","hash":"97974c231b4659b8aa5e9321c4d54db5c816d0db","modified":1661433213095},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"a52f8cae599099231866298ed831fdf76c9b6717","modified":1661433213096},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"9af620eba5ccceea21a0e3bc69f6f1fa7637c2f3","modified":1661433213096},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"70b3eb9d36543ab92796ac163544e9cf51b7c1e6","modified":1661433213097},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"97dec98d0403097d66822f1c90b50b2890c84698","modified":1661433213097},{"_id":"themes/next/source/css/_common/components/post/post-followme.styl","hash":"57b9a179675f1536e017cba457b6ac575e397c4f","modified":1661433213097},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"0dfb97703a519d9438f64f9e41ab1dd37381f733","modified":1661433213098},{"_id":"themes/next/source/css/_common/components/post/post-header.styl","hash":"93ba8172c0d2c37d738e6dbd44fcd5a2e23b92f3","modified":1661433213098},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"2c24829d95c742eb9e8316ebf2fbe9f2c168b59a","modified":1661433213099},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"09dda2667628d1f91b2e37d8fc6df1413f961b64","modified":1661433213100},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"66fc406796b6efe6cea76550573b7a632112406a","modified":1661433213099},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"fcd64c23d17775b3635325f6758b648d932e79b5","modified":1661433213101},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"5cc9e7394c927065c688cba5edd6e0a27587f1d8","modified":1661433213100},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"b266d2ce5e2b117be01537889e839a69004dc0bb","modified":1661433213100},{"_id":"themes/next/source/css/_common/components/third-party/search.styl","hash":"bad99f4cccb93b3cefe990a2c85124e60698d32e","modified":1661433213104},{"_id":"themes/next/source/css/_common/components/third-party/related-posts.styl","hash":"8ed7a9d5dfac592de703421b543978095129aa5b","modified":1661433213103},{"_id":"themes/next/source/css/_common/components/third-party/gitalk.styl","hash":"b87f4a06c0db893df4f756f24be182e1a4751f24","modified":1661433213102},{"_id":"themes/next/source/css/_common/components/third-party/math.styl","hash":"d83102771df652769e51ddfd041cf5f4ca1a041d","modified":1661433213102},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"1f6b0d3ab227697ca115e57fd61122ea7950e19d","modified":1661433213104},{"_id":"themes/next/source/css/_common/outline/footer/footer.styl","hash":"7eeb22c5696f8e0c95161dc57703973cf81c8c12","modified":1661433213105},{"_id":"themes/next/source/css/_common/outline/header/bookmark.styl","hash":"b4f4bae437d4f994af93cf142494ffcd86bae46b","modified":1661433213106},{"_id":"themes/next/source/css/_common/outline/header/github-banner.styl","hash":"b31c86d1a4f89837f9187bed646bda96b2cd286c","modified":1661433213106},{"_id":"themes/next/source/css/_common/outline/header/headerband.styl","hash":"6d5f26646e2914474f295de8bf6dc327d4acd529","modified":1661433213107},{"_id":"themes/next/source/css/_common/outline/header/menu.styl","hash":"7a3a56b10ab714c0e2ed240d0939deeecdcad167","modified":1661433213107},{"_id":"themes/next/source/css/_common/outline/header/header.styl","hash":"300058ca12e81013e77ba01fe66ac210525768b6","modified":1661433213107},{"_id":"themes/next/source/css/_common/outline/header/site-meta.styl","hash":"3d16ac0f4ccaeed868c246d4d49bde543d1f62cb","modified":1661433213108},{"_id":"themes/next/source/css/_common/outline/header/site-nav.styl","hash":"b8c816fba0a9b4a35fbae03ba5b1b2da96ba2687","modified":1661433213108},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-author-links.styl","hash":"49722d555a2edb18094bb2cb3d7336dd72051b93","modified":1661433213110},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-author.styl","hash":"357f825f0a649b2e28cba1481d4c9a0cb402e43a","modified":1661433213111},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-button.styl","hash":"525242ce9e912c4adfe5134347c67dbdb9e98e3d","modified":1661433213112},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-blogroll.styl","hash":"096f908c08ce553e482aadfd3e767a0145191093","modified":1661433213111},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-dimmer.styl","hash":"12f7eaf6b56624cbc411528562d6bb848ff97039","modified":1661433213112},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-nav.styl","hash":"b11b04737a1a0fea3bd9f0081d96ee6c015358d4","modified":1661433213113},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-toc.styl","hash":"fa0a2ea57b7b4ce75b5d18c264af2d92ea3192f9","modified":1661433213113},{"_id":"themes/next/source/css/_common/outline/sidebar/site-state.styl","hash":"67a1fcb33535122d41acd24f1f49cf02c89b88fa","modified":1661433213114},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar.styl","hash":"5d540f683018745a5ed1d6f635df28ea610c1244","modified":1661433213114},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-toggle.styl","hash":"098b4bdf49c7300490f959386d5d1185a32543f6","modified":1661433213114},{"_id":"themes/next/source/css/_common/scaffolding/highlight/copy-code.styl","hash":"4079e616fbf36112dec0674c1e0713d1d9769068","modified":1661433213117},{"_id":"themes/next/source/css/_common/scaffolding/highlight/diff.styl","hash":"83bd737f663a8461e66985af8ddbfc0a731fc939","modified":1661433213118},{"_id":"themes/next/source/css/_common/scaffolding/highlight/highlight.styl","hash":"80488259271bcfe38031f4c2e902463daba9336b","modified":1661433213118},{"_id":"themes/next/source/css/_common/scaffolding/highlight/theme.styl","hash":"c911045b2ce9a66e38d9dd30c7ed078abbc10cbf","modified":1661433213119},{"_id":"themes/next/source/css/_common/scaffolding/tags/blockquote-center.styl","hash":"ceacfa6218f6084c71a230b086e5d2708d29927e","modified":1661433213121},{"_id":"themes/next/source/css/_common/scaffolding/tags/group-pictures.styl","hash":"aca7bb220fc14ef2a8f96282d2a95a96a9238d46","modified":1661433213121},{"_id":"themes/next/source/css/_common/scaffolding/tags/note.styl","hash":"adaf0f580fccf4158169eeaf534a18005b39a760","modified":1661433213122},{"_id":"themes/next/source/css/_common/scaffolding/tags/label.styl","hash":"8b7aafb911850c73074cdb6cc87abe4ac8c12e99","modified":1661433213122},{"_id":"themes/next/source/css/_common/scaffolding/tags/pdf.styl","hash":"03a5bcecc0b12231462ef6ffe432fa77ee71beff","modified":1661433213123},{"_id":"themes/next/source/css/_common/scaffolding/tags/tabs.styl","hash":"3256e39f281f06751a1c0145d9806a0e56d68170","modified":1661433213123},{"_id":"themes/next/source/css/_common/scaffolding/tags/tags.styl","hash":"51d46fa3c7c6b691c61a2c2b0ac005c97cfbf72b","modified":1661433213124},{"_id":"themes/next/source/images/ava.png","hash":"9f1ccc090035bd6a3c44d7860c0e57c086163526","modified":1661433213141},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-brands-400.woff2","hash":"509988477da79c146cb93fb728405f18e923c2de","modified":1661433213154},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-solid-900.woff2","hash":"75a88815c47a249eadb5f0edc1675957f860cca7","modified":1661433213155},{"_id":"public/about/index.html","hash":"2c78031ed1577ade5a9c23f1e003a1af9f5c5f4a","modified":1682562548677},{"_id":"public/index.html","hash":"d180b50274a71d30fde5737f31a8dae581866d97","modified":1682562548677},{"_id":"public/home/index.html","hash":"e45cb85771d794cc4c71e73923762053880850ce","modified":1682562548677},{"_id":"public/2022/08/08/强化学习/index.html","hash":"7efcede15299ceba19302df11a6cd53463c00d4c","modified":1682562548677},{"_id":"public/2020/10/01/Ubuntu 换源/index.html","hash":"8779388def3a7a02e0fa0ca8eee33ec594a1457f","modified":1682562548677},{"_id":"public/archives/page/3/index.html","hash":"3d5337926ec4c91cbb1a78f281334b83f07a392d","modified":1682562548677},{"_id":"public/archives/2020/index.html","hash":"b0acba618ae4c8d162256937a454f543519f9cb9","modified":1682562548677},{"_id":"public/archives/2020/08/index.html","hash":"b3df7ac0240fab77dbeb5631aff017b9d952ea56","modified":1682562548677},{"_id":"public/archives/2020/09/index.html","hash":"6b73d66f1a079e4032e15bfc1bcdb72f8c0ef8ed","modified":1682562548677},{"_id":"public/archives/2020/10/index.html","hash":"e6069c469d03888c248ab7b3b16c415551f68852","modified":1682562548677},{"_id":"public/archives/2021/index.html","hash":"e90446e476a26f7d69842540e2318908ccbad895","modified":1682562548677},{"_id":"public/archives/2021/01/index.html","hash":"541c1857102dbed9ef9a10200f5b194953ecd3c3","modified":1682562548677},{"_id":"public/archives/2021/05/index.html","hash":"2f027e8fd54622b47617f3d9caafc94afef3c780","modified":1682562548677},{"_id":"public/archives/2022/page/2/index.html","hash":"b8fcaa839521a68824257e45a7d3a787c6314f4c","modified":1682562548677},{"_id":"public/archives/2022/01/index.html","hash":"058479dc5e7cc5315c89386fde6640e36bfa9a22","modified":1682562548677},{"_id":"public/archives/2022/04/index.html","hash":"66db549647d1bb7809423d1e461ebbc98f1fc0e1","modified":1682562548677},{"_id":"public/archives/2022/07/index.html","hash":"02cdf1aaeaabb78899971e09a24e4a333aacc0d5","modified":1682562548677},{"_id":"public/archives/2022/08/index.html","hash":"78479dba5680501ef681dffd197b1ef978050124","modified":1682562548677},{"_id":"public/archives/2022/09/index.html","hash":"d20a168f5e3884a5bf282e151d45060c0f76049d","modified":1682562548677},{"_id":"public/archives/2023/index.html","hash":"5f1a64f57ef72571c043a0652f73b404c976bc56","modified":1682562548677},{"_id":"public/archives/2023/03/index.html","hash":"1f3a4d5a3a6003b6185b5cc7a55d13f05b5a58f0","modified":1682562548677},{"_id":"public/archives/2023/04/index.html","hash":"9cf59faee540f5f0169e3872d01afb2faa3d124a","modified":1682562548677},{"_id":"public/page/5/index.html","hash":"4e894016e00bb3f42b3b82d1242b051194883cb8","modified":1682562548677},{"_id":"public/page/7/index.html","hash":"4748f398f3c80589a7c71d5b576383bbebfa952f","modified":1682562548677},{"_id":"public/page/9/index.html","hash":"dd435c167f62c413016e5d293554b094afe5e149","modified":1682562548677},{"_id":"public/page/14/index.html","hash":"94bd9d5161ba906a060b0452e503f0aed23e34d6","modified":1682562548677},{"_id":"public/page/20/index.html","hash":"d1d75ef8cd53a3118fe54df5bb5815974ecdff71","modified":1682562548677},{"_id":"public/2023/04/27/学着克服国内网络环境.jpg/index.html","hash":"d43b6a18e90370685dfaa4b20f31df7f75b6c6b3","modified":1682562548677},{"_id":"public/2023/03/17/在树莓派上从源码编译安装Paddlepaddle/index.html","hash":"a8a0ec1b09b59e7a45083d603c6fde9aeefaf12e","modified":1682562548677},{"_id":"public/2022/09/22/如何使用 Squoosh 命令行批量压缩图片/index.html","hash":"70a805168d3e238643aa69750915ab866821f304","modified":1682562548677},{"_id":"public/2022/09/12/每周报告 20220906/index.html","hash":"8fcc87c72b314dfdbaea71e4756a3e8f3b580f13","modified":1682562548677},{"_id":"public/2022/09/06/gradio 本地部署页面空白的解决方案/index.html","hash":"ab6c584310efb4510ca15b623e1f2c5feede5935","modified":1682562548677},{"_id":"public/2022/09/04/CLIP 模型及其应用/index.html","hash":"d1b7670e1026c625b12f759593d9dd99dbcff11c","modified":1682562548677},{"_id":"public/2022/08/26/Windows 如何正确安装 cuda 与 cudnn/index.html","hash":"5934d22d8cb00df4b9733069ab43bcd8616f9794","modified":1682562548677},{"_id":"public/2022/08/25/windows 无法修改代理设置的解决方法/index.html","hash":"9dc76d09b9b4f5f09bce22bb9de3990ac87c3bb4","modified":1682562548677},{"_id":"public/2022/08/05/知识蒸馏 (Knowledge Distillation)/index.html","hash":"41f3a02fac041499b02fc84bb922abfb2d9805c9","modified":1682562548677},{"_id":"public/2022/07/25/注意力机制/index.html","hash":"bbe616d74f2fbf7f3758cac305334b8b19dd2086","modified":1682562548677},{"_id":"public/2022/07/26/神经网络模型/index.html","hash":"f26791b6a8f51516b0f005e37ac1d7cc1ed545dd","modified":1682562548677},{"_id":"public/2022/07/17/Transformer 学习笔记/index.html","hash":"2dd53211b89582414b7f5732bca597442690caf3","modified":1682562548677},{"_id":"public/2022/07/12/Hugging Face 学习(1)/index.html","hash":"8b432e8cb8882967128a0453e742dea14716c683","modified":1682562548677},{"_id":"public/2022/07/10/自动文本摘要(TF-ISF)/index.html","hash":"ba10c95f15dd7c66e331fdd16117bb1d3ee8c081","modified":1682562548677},{"_id":"public/2022/04/08/GSoC 2022 Proposal 申请书/index.html","hash":"c159c9413e3a5cf1cc6b6fc621a12a1c1e05344c","modified":1682562548677},{"_id":"public/2022/01/19/计算社会学 project/index.html","hash":"3426b5f8ddea7bc741f5f7639532eab68cac7752","modified":1682562548677},{"_id":"public/2021/05/12/键盘笔记一/index.html","hash":"afc7679ee59043b7005ea927fbc055b8b8458a3c","modified":1682562548677},{"_id":"public/2021/01/01/搭建 CSGO 服务器(on windows)/index.html","hash":"7670b7b93fee914b750ed8cf80753a8ce4506eae","modified":1682562548677},{"_id":"public/2020/09/30/ubuntu 20.04 安装/index.html","hash":"880286fbd80528d0a07fc2a89bd4b92209763c33","modified":1682562548677},{"_id":"public/2020/09/28/CSGO 中的 cfg 系统/index.html","hash":"e028be35564e12e5f5f3fc13aef90ed76f66ffaf","modified":1682562548677},{"_id":"public/2020/08/18/微信小程序webp优化/index.html","hash":"0ce112a6e29de9fc6165906e836b0afc80fe7f02","modified":1682562548677},{"_id":"public/archives/index.html","hash":"e26d33b82ee29bfc6148b84085de0d8b893b2020","modified":1682562548677},{"_id":"public/archives/page/2/index.html","hash":"b5a3e2aa3a7e710ced79e77a36ad1d9fdf09117b","modified":1682562548677},{"_id":"public/archives/2022/index.html","hash":"5dd4d34ea7179155a020da0bd5b79fc3dd9ee9a3","modified":1682562548677},{"_id":"public/page/2/index.html","hash":"e8ef2d75c3c557a9191c8944a3383c046c0da007","modified":1682562548677},{"_id":"public/page/3/index.html","hash":"5afcbac193332242f2d2e241f55f53ee5afb89d4","modified":1682562548677},{"_id":"public/page/4/index.html","hash":"4502c0616b7a05de4085b4face13659dbba3cb33","modified":1682562548677},{"_id":"public/page/6/index.html","hash":"cdc5a507a09a3d9a290fd68674c76623b979708a","modified":1682562548677},{"_id":"public/page/8/index.html","hash":"97b1d3f17bb34af877e7fd56ac05dc271c421c95","modified":1682562548677},{"_id":"public/page/10/index.html","hash":"b41f2a56bb0b62e83452a99d6006d6b8827a8ec7","modified":1682562548677},{"_id":"public/page/11/index.html","hash":"91f1177ef0b8dd1f1c9bf753d8d80bc9a0f8f4eb","modified":1682562548677},{"_id":"public/page/12/index.html","hash":"07503a332b2d0d6e2676df0f16e8878fb491e1d8","modified":1682562548677},{"_id":"public/page/13/index.html","hash":"982d19e1a4afe039fb89136855f81a3f958b993e","modified":1682562548677},{"_id":"public/page/15/index.html","hash":"22b8a86ef87e45bee195b3a22eaadf94a0408547","modified":1682562548677},{"_id":"public/page/16/index.html","hash":"84c88bbfb34ba6b9f8fe5e9ed69a4ac5d50aeb8b","modified":1682562548677},{"_id":"public/page/17/index.html","hash":"878d2d4c9869643dbec608efea9bceab0948e2ba","modified":1682562548677},{"_id":"public/page/18/index.html","hash":"62aecf7715d5fdb82d066c59df940a82a8b5d952","modified":1682562548677},{"_id":"public/page/19/index.html","hash":"3140d1d3ce159bfc2b5b28610363024f069a2044","modified":1682562548677},{"_id":"public/page/21/index.html","hash":"0479d5a4dd4c4e02a510e90c2c44c0f8837684fc","modified":1682562548677},{"_id":"public/page/22/index.html","hash":"05534704b542c1dccdee83e207ef2a6845f28e9d","modified":1682562548677},{"_id":"public/page/23/index.html","hash":"9d8dde20f7e01be179856d3644619015ae905026","modified":1682562548677},{"_id":"public/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1682562548677},{"_id":"public/images/avatar.gif","hash":"18c53e15eb0c84b139995f9334ed8522b40aeaf6","modified":1682562548677},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1682562548677},{"_id":"public/images/Hg-icon-10.png","hash":"32d48c64dafba86f1d5ec1d15d05f3f55610cddf","modified":1682562548677},{"_id":"public/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1682562548677},{"_id":"public/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1682562548677},{"_id":"public/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1682562548677},{"_id":"public/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1682562548677},{"_id":"public/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1682562548677},{"_id":"public/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1682562548677},{"_id":"public/images/favicon-16x16-next.png","hash":"81b853c74dd83f724541efe707e025dff32b5c05","modified":1682562548677},{"_id":"public/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1682562548677},{"_id":"public/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1682562548677},{"_id":"public/images/favicon-32x32-next.png","hash":"2da5363c37d62c34e044d5442f3ccf19266731ec","modified":1682562548677},{"_id":"public/lib/font-awesome/webfonts/fa-regular-400.woff2","hash":"260bb01acd44d88dcb7f501a238ab968f86bef9e","modified":1682562548677},{"_id":"public/images/ava.png","hash":"9f1ccc090035bd6a3c44d7860c0e57c086163526","modified":1682562548677},{"_id":"public/lib/font-awesome/webfonts/fa-brands-400.woff2","hash":"509988477da79c146cb93fb728405f18e923c2de","modified":1682562548677},{"_id":"public/lib/font-awesome/webfonts/fa-solid-900.woff2","hash":"75a88815c47a249eadb5f0edc1675957f860cca7","modified":1682562548677},{"_id":"public/js/local-search.js","hash":"35ccf100d8f9c0fd6bfbb7fa88c2a76c42a69110","modified":1682562548677},{"_id":"public/js/algolia-search.js","hash":"498d233eb5c7af6940baf94c1a1c36fdf1dd2636","modified":1682562548677},{"_id":"public/js/next-boot.js","hash":"a1b0636423009d4a4e4cea97bcbf1842bfab582c","modified":1682562548677},{"_id":"public/js/bookmark.js","hash":"9734ebcb9b83489686f5c2da67dc9e6157e988ad","modified":1682562548677},{"_id":"public/js/motion.js","hash":"72df86f6dfa29cce22abeff9d814c9dddfcf13a9","modified":1682562548677},{"_id":"public/js/utils.js","hash":"730cca7f164eaf258661a61ff3f769851ff1e5da","modified":1682562548677},{"_id":"public/js/schemes/pisces.js","hash":"0ac5ce155bc58c972fe21c4c447f85e6f8755c62","modified":1682562548677},{"_id":"public/js/schemes/muse.js","hash":"1eb9b88103ddcf8827b1a7cbc56471a9c5592d53","modified":1682562548677},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1682562548677},{"_id":"public/css/main.css","hash":"661c0d12254c48475cc39841583740e57bd55a9f","modified":1682562548677},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1682562548677},{"_id":"public/lib/anime.min.js","hash":"47cb482a8a488620a793d50ba8f6752324b46af3","modified":1682562548677},{"_id":"public/lib/font-awesome/css/all.min.css","hash":"0038dc97c79451578b7bd48af60ba62282b4082b","modified":1682562548677}],"Category":[],"Data":[],"Page":[{"title":"关于我","date":"2022-07-19T03:51:06.000Z","type":"about","_content":"\n<style>\n  .col {\n    display: flex;\n    flex-direction: row;\n  }\n  .col-left {\n    flex: 1;\n  }\n  .col-right {\n    flex: 1;\n  }\n</style>\n\n<div class=\"col\">\n  <div class=\"col-left\">\n    我是一名西北工业大学计算机专业的学生 <br/>\n    喜欢编程 音乐 设计 以及研究有意思的事情 <br/>\n    <a href=\"https://space.bilibili.com/7405917\">B站：不是吴昊的wh</a>\n    <br/>\n    <a href=\"https://github.com/1099255210\">Github</a>\n    <br/>\n    <a href=\"mailto:1099255210@qq.com\">Email 电邮联系</a>\n  </div>\n\n  <div class=\"col-right\">\n    <img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/icon_bb-ci-512.png\" alt=\"\">\n  </div>\n</div>\n","source":"about/index.md","raw":"---\ntitle: 关于我\ndate: 2022-07-19 11:51:06\ntype: about\n---\n\n<style>\n  .col {\n    display: flex;\n    flex-direction: row;\n  }\n  .col-left {\n    flex: 1;\n  }\n  .col-right {\n    flex: 1;\n  }\n</style>\n\n<div class=\"col\">\n  <div class=\"col-left\">\n    我是一名西北工业大学计算机专业的学生 <br/>\n    喜欢编程 音乐 设计 以及研究有意思的事情 <br/>\n    <a href=\"https://space.bilibili.com/7405917\">B站：不是吴昊的wh</a>\n    <br/>\n    <a href=\"https://github.com/1099255210\">Github</a>\n    <br/>\n    <a href=\"mailto:1099255210@qq.com\">Email 电邮联系</a>\n  </div>\n\n  <div class=\"col-right\">\n    <img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/icon_bb-ci-512.png\" alt=\"\">\n  </div>\n</div>\n","updated":"2022-08-25T13:13:32.989Z","path":"about/index.html","comments":1,"layout":"page","_id":"clgyiartl0000jku78uwi9yf6","content":"<style>\n  .col {\n    display: flex;\n    flex-direction: row;\n  }\n  .col-left {\n    flex: 1;\n  }\n  .col-right {\n    flex: 1;\n  }\n</style>\n\n<div class=\"col\">\n  <div class=\"col-left\">\n    我是一名西北工业大学计算机专业的学生 <br/>\n    喜欢编程 音乐 设计 以及研究有意思的事情 <br/>\n    <a href=\"https://space.bilibili.com/7405917\">B站：不是吴昊的wh</a>\n    <br/>\n    <a href=\"https://github.com/1099255210\">Github</a>\n    <br/>\n    <a href=\"mailto:1099255210@qq.com\">Email 电邮联系</a>\n  </div>\n\n  <div class=\"col-right\">\n    <img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/icon_bb-ci-512.png\" alt=\"\">\n  </div>\n</div>\n","site":{"data":{}},"excerpt":"","more":"<style>\n  .col {\n    display: flex;\n    flex-direction: row;\n  }\n  .col-left {\n    flex: 1;\n  }\n  .col-right {\n    flex: 1;\n  }\n</style>\n\n<div class=\"col\">\n  <div class=\"col-left\">\n    我是一名西北工业大学计算机专业的学生 <br/>\n    喜欢编程 音乐 设计 以及研究有意思的事情 <br/>\n    <a href=\"https://space.bilibili.com/7405917\">B站：不是吴昊的wh</a>\n    <br/>\n    <a href=\"https://github.com/1099255210\">Github</a>\n    <br/>\n    <a href=\"mailto:1099255210@qq.com\">Email 电邮联系</a>\n  </div>\n\n  <div class=\"col-right\">\n    <img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/icon_bb-ci-512.png\" alt=\"\">\n  </div>\n</div>\n"},{"_content":"","source":"index.md","raw":"","date":"2022-08-25T13:13:32.990Z","updated":"2022-08-25T13:13:32.990Z","path":"index.html","title":"","comments":1,"layout":"page","_id":"clgyiartq0002jku75pqu4k81","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"以 120% 的缩放浏览本博客以得到最佳效果","date":"2022-07-19T03:51:06.000Z","type":"Home","_content":"\n\n","source":"home/index.md","raw":"---\ntitle: 以 120% 的缩放浏览本博客以得到最佳效果\ndate: 2022-07-19 11:51:06\ntype: Home\n---\n\n\n","updated":"2022-08-25T13:13:32.990Z","path":"home/index.html","comments":1,"layout":"page","_id":"clgyiarts0004jku71q4shnog","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"Clip 模型与其应用","date":"2022-09-04T12:09:08.000Z","mathjax":false,"_content":"\n# Clip 模型与其应用\n\n## CLIP 模型的介绍\n\nCLIP 模型使用的方法：对比学习，预测 n*n 对图像与文本数据，将图片分类任务转换成图文匹配任务。这个过程实际上就是引入了 NLP 给出的监督信号。\n\n![image-20220904193225832](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904193225832.png)\n\n\n\n图中左侧，得到文本特征与图片的特征后可以看到，对角线上的元素都是图文匹配的，共有 N 个正样本，其余的元素都是负样本，共有 N*N-N 个。其中，文本数据使用Transformer，图片数据用了两种模型，ResNet 和 Vision Transformer (ViT)。\n\nCLIP 模型训练所用的数据集较为庞大，包含从互联网上各种公开资源收集的4亿对图像、文本，CLIP是从头开始训练的，没有使用预训练的初始参数。\n\n## CLIP 的应用\n\n## 利用 CLIP 做简单的图片检索\n\n在 jupyter notebook 中做一个简单的问答系统：\n\n```python\nimport torch\nimport clip\nimport pandas as pd\nimport numpy as np\nimport os\nfrom PIL import Image\nfrom IPython.display import display\nfrom tqdm.notebook import tqdm\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\n\n# load images\n\ndata_location =  \"./imgs\"\nimg_dict = {}\nfor inx, f in enumerate(os.listdir(data_location)):\n  img_dict[inx] = f\nimg_nums = len(img_dict)\nprint(\"There are {} images.\".format(img_nums))\n\nif img_nums == 0:\n  print('no image in the folder.')\n```\n\n这一段用来加载模型 以及 读取文件夹中所有的图片\n\n```python\n# get text input from user && text encode\ninstr = input('Pleaze input description:')\ntext_input = clip.tokenize(instr).to(device)\nwith torch.no_grad():\n  text_f = model.encode_text(text_input)\n\nsim = {}\npbar = tqdm(total=img_nums)\n\nfor i in range(img_nums):\n  \n  image_path = f'{data_location}/{img_dict[i]}'\n  img = Image.open(image_path)\n  img_input = preprocess(img).unsqueeze(0).to(device)\n\n  # image encode\n  with torch.no_grad():\n    img_f = model.encode_image(img_input)\n\n  # calculate similarity\n  img_f /= img_f.norm(dim=-1, keepdim=True)\n  text_f /= text_f.norm(dim=-1, keepdim=True)\n  similarity = 100 * img_f @ text_f.T\n  sim[i] = similarity\n  pbar.update(1)\n\n# display top3 result\nres = sorted(sim.items(), key=lambda s:s[1], reverse=True)\nprint(f'query:{instr}')\nMAX_SIZE = (300, 300)\nfor i in range(3):\n  image_path = f'{data_location}/{img_dict[res[i][0]]}'\n  img = Image.open(image_path)\n  img.thumbnail(MAX_SIZE)\n  display(img)\n```\n\n这里先读入用户的输入，即搜索图片的关键词/句，然后将文本编码得到特征，然后分别对应所有图片的特征计算相似度，取相似度最高的三张图片输出。\n\n图片我没有找网上的一些数据集（因为大部分应该已经有人测试过了），我想看一看这个模型到底有多强大，于是我选取了26张我自己拍的一些生活照，进行了一下图像压缩，控制在500kb之内，作为本实验的数据集。\n\n经过我的一些尝试，发现准确度还是很高的，得到的结果令人满意。下面展示一些查询的结果：\n\n\n![image-20220904195212924](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904195212924.png)\n\n![image-20220904195756669](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904195756669.png)\n\n![image-20220904200643952](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904200643952.png)\n\n\n\n值得一提的是，一开始我并没有预料到他能学习到图像中的文本特征，但是实验中我发现，如果图片中有文字的话，也能被它检索到，比如这个关键词：\n\n\n![image-20220904200822620](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904200822620.png)\n\n![image-20220904200950871](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904200950871.png)\n\n\n我这里是输入了这个midi键盘的名称（是刻在键盘上的文字），结果它也能给我识别出来，感觉非常的神奇。\n\n","source":"_posts/CLIP 模型及其应用.md","raw":"---\ntitle: Clip 模型与其应用\ndate: 2022-09-04 20:09:08\ntags:\nmathjax: false\n---\n\n# Clip 模型与其应用\n\n## CLIP 模型的介绍\n\nCLIP 模型使用的方法：对比学习，预测 n*n 对图像与文本数据，将图片分类任务转换成图文匹配任务。这个过程实际上就是引入了 NLP 给出的监督信号。\n\n![image-20220904193225832](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904193225832.png)\n\n\n\n图中左侧，得到文本特征与图片的特征后可以看到，对角线上的元素都是图文匹配的，共有 N 个正样本，其余的元素都是负样本，共有 N*N-N 个。其中，文本数据使用Transformer，图片数据用了两种模型，ResNet 和 Vision Transformer (ViT)。\n\nCLIP 模型训练所用的数据集较为庞大，包含从互联网上各种公开资源收集的4亿对图像、文本，CLIP是从头开始训练的，没有使用预训练的初始参数。\n\n## CLIP 的应用\n\n## 利用 CLIP 做简单的图片检索\n\n在 jupyter notebook 中做一个简单的问答系统：\n\n```python\nimport torch\nimport clip\nimport pandas as pd\nimport numpy as np\nimport os\nfrom PIL import Image\nfrom IPython.display import display\nfrom tqdm.notebook import tqdm\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\n\n# load images\n\ndata_location =  \"./imgs\"\nimg_dict = {}\nfor inx, f in enumerate(os.listdir(data_location)):\n  img_dict[inx] = f\nimg_nums = len(img_dict)\nprint(\"There are {} images.\".format(img_nums))\n\nif img_nums == 0:\n  print('no image in the folder.')\n```\n\n这一段用来加载模型 以及 读取文件夹中所有的图片\n\n```python\n# get text input from user && text encode\ninstr = input('Pleaze input description:')\ntext_input = clip.tokenize(instr).to(device)\nwith torch.no_grad():\n  text_f = model.encode_text(text_input)\n\nsim = {}\npbar = tqdm(total=img_nums)\n\nfor i in range(img_nums):\n  \n  image_path = f'{data_location}/{img_dict[i]}'\n  img = Image.open(image_path)\n  img_input = preprocess(img).unsqueeze(0).to(device)\n\n  # image encode\n  with torch.no_grad():\n    img_f = model.encode_image(img_input)\n\n  # calculate similarity\n  img_f /= img_f.norm(dim=-1, keepdim=True)\n  text_f /= text_f.norm(dim=-1, keepdim=True)\n  similarity = 100 * img_f @ text_f.T\n  sim[i] = similarity\n  pbar.update(1)\n\n# display top3 result\nres = sorted(sim.items(), key=lambda s:s[1], reverse=True)\nprint(f'query:{instr}')\nMAX_SIZE = (300, 300)\nfor i in range(3):\n  image_path = f'{data_location}/{img_dict[res[i][0]]}'\n  img = Image.open(image_path)\n  img.thumbnail(MAX_SIZE)\n  display(img)\n```\n\n这里先读入用户的输入，即搜索图片的关键词/句，然后将文本编码得到特征，然后分别对应所有图片的特征计算相似度，取相似度最高的三张图片输出。\n\n图片我没有找网上的一些数据集（因为大部分应该已经有人测试过了），我想看一看这个模型到底有多强大，于是我选取了26张我自己拍的一些生活照，进行了一下图像压缩，控制在500kb之内，作为本实验的数据集。\n\n经过我的一些尝试，发现准确度还是很高的，得到的结果令人满意。下面展示一些查询的结果：\n\n\n![image-20220904195212924](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904195212924.png)\n\n![image-20220904195756669](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904195756669.png)\n\n![image-20220904200643952](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904200643952.png)\n\n\n\n值得一提的是，一开始我并没有预料到他能学习到图像中的文本特征，但是实验中我发现，如果图片中有文字的话，也能被它检索到，比如这个关键词：\n\n\n![image-20220904200822620](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904200822620.png)\n\n![image-20220904200950871](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904200950871.png)\n\n\n我这里是输入了这个midi键盘的名称（是刻在键盘上的文字），结果它也能给我识别出来，感觉非常的神奇。\n\n","slug":"CLIP 模型及其应用","published":1,"updated":"2022-09-05T14:43:22.164Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clgyiarto0001jku7bty1f7dn","content":"<h1 id=\"Clip-模型与其应用\"><a href=\"#Clip-模型与其应用\" class=\"headerlink\" title=\"Clip 模型与其应用\"></a>Clip 模型与其应用</h1><h2 id=\"CLIP-模型的介绍\"><a href=\"#CLIP-模型的介绍\" class=\"headerlink\" title=\"CLIP 模型的介绍\"></a>CLIP 模型的介绍</h2><p>CLIP 模型使用的方法：对比学习，预测 n*n 对图像与文本数据，将图片分类任务转换成图文匹配任务。这个过程实际上就是引入了 NLP 给出的监督信号。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904193225832.png\" alt=\"image-20220904193225832\"></p>\n<p>图中左侧，得到文本特征与图片的特征后可以看到，对角线上的元素都是图文匹配的，共有 N 个正样本，其余的元素都是负样本，共有 N*N-N 个。其中，文本数据使用Transformer，图片数据用了两种模型，ResNet 和 Vision Transformer (ViT)。</p>\n<p>CLIP 模型训练所用的数据集较为庞大，包含从互联网上各种公开资源收集的4亿对图像、文本，CLIP是从头开始训练的，没有使用预训练的初始参数。</p>\n<h2 id=\"CLIP-的应用\"><a href=\"#CLIP-的应用\" class=\"headerlink\" title=\"CLIP 的应用\"></a>CLIP 的应用</h2><h2 id=\"利用-CLIP-做简单的图片检索\"><a href=\"#利用-CLIP-做简单的图片检索\" class=\"headerlink\" title=\"利用 CLIP 做简单的图片检索\"></a>利用 CLIP 做简单的图片检索</h2><p>在 jupyter notebook 中做一个简单的问答系统：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> clip</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">from</span> PIL <span class=\"keyword\">import</span> Image</span><br><span class=\"line\"><span class=\"keyword\">from</span> IPython.display <span class=\"keyword\">import</span> display</span><br><span class=\"line\"><span class=\"keyword\">from</span> tqdm.notebook <span class=\"keyword\">import</span> tqdm</span><br><span class=\"line\"></span><br><span class=\"line\">device = <span class=\"string\">&quot;cuda&quot;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&quot;cpu&quot;</span></span><br><span class=\"line\">model, preprocess = clip.load(<span class=\"string\">&quot;ViT-B/32&quot;</span>, device=device)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># load images</span></span><br><span class=\"line\"></span><br><span class=\"line\">data_location =  <span class=\"string\">&quot;./imgs&quot;</span></span><br><span class=\"line\">img_dict = &#123;&#125;</span><br><span class=\"line\"><span class=\"keyword\">for</span> inx, f <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(os.listdir(data_location)):</span><br><span class=\"line\">  img_dict[inx] = f</span><br><span class=\"line\">img_nums = <span class=\"built_in\">len</span>(img_dict)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;There are &#123;&#125; images.&quot;</span>.<span class=\"built_in\">format</span>(img_nums))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> img_nums == <span class=\"number\">0</span>:</span><br><span class=\"line\">  <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;no image in the folder.&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>这一段用来加载模型 以及 读取文件夹中所有的图片</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># get text input from user &amp;&amp; text encode</span></span><br><span class=\"line\">instr = <span class=\"built_in\">input</span>(<span class=\"string\">&#x27;Pleaze input description:&#x27;</span>)</span><br><span class=\"line\">text_input = clip.tokenize(instr).to(device)</span><br><span class=\"line\"><span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">  text_f = model.encode_text(text_input)</span><br><span class=\"line\"></span><br><span class=\"line\">sim = &#123;&#125;</span><br><span class=\"line\">pbar = tqdm(total=img_nums)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(img_nums):</span><br><span class=\"line\">  </span><br><span class=\"line\">  image_path = <span class=\"string\">f&#x27;<span class=\"subst\">&#123;data_location&#125;</span>/<span class=\"subst\">&#123;img_dict[i]&#125;</span>&#x27;</span></span><br><span class=\"line\">  img = Image.<span class=\"built_in\">open</span>(image_path)</span><br><span class=\"line\">  img_input = preprocess(img).unsqueeze(<span class=\"number\">0</span>).to(device)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\"># image encode</span></span><br><span class=\"line\">  <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">    img_f = model.encode_image(img_input)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\"># calculate similarity</span></span><br><span class=\"line\">  img_f /= img_f.norm(dim=-<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\">  text_f /= text_f.norm(dim=-<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\">  similarity = <span class=\"number\">100</span> * img_f @ text_f.T</span><br><span class=\"line\">  sim[i] = similarity</span><br><span class=\"line\">  pbar.update(<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># display top3 result</span></span><br><span class=\"line\">res = <span class=\"built_in\">sorted</span>(sim.items(), key=<span class=\"keyword\">lambda</span> s:s[<span class=\"number\">1</span>], reverse=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;query:<span class=\"subst\">&#123;instr&#125;</span>&#x27;</span>)</span><br><span class=\"line\">MAX_SIZE = (<span class=\"number\">300</span>, <span class=\"number\">300</span>)</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">3</span>):</span><br><span class=\"line\">  image_path = <span class=\"string\">f&#x27;<span class=\"subst\">&#123;data_location&#125;</span>/<span class=\"subst\">&#123;img_dict[res[i][<span class=\"number\">0</span>]]&#125;</span>&#x27;</span></span><br><span class=\"line\">  img = Image.<span class=\"built_in\">open</span>(image_path)</span><br><span class=\"line\">  img.thumbnail(MAX_SIZE)</span><br><span class=\"line\">  display(img)</span><br></pre></td></tr></table></figure>\n<p>这里先读入用户的输入，即搜索图片的关键词/句，然后将文本编码得到特征，然后分别对应所有图片的特征计算相似度，取相似度最高的三张图片输出。</p>\n<p>图片我没有找网上的一些数据集（因为大部分应该已经有人测试过了），我想看一看这个模型到底有多强大，于是我选取了26张我自己拍的一些生活照，进行了一下图像压缩，控制在500kb之内，作为本实验的数据集。</p>\n<p>经过我的一些尝试，发现准确度还是很高的，得到的结果令人满意。下面展示一些查询的结果：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904195212924.png\" alt=\"image-20220904195212924\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904195756669.png\" alt=\"image-20220904195756669\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904200643952.png\" alt=\"image-20220904200643952\"></p>\n<p>值得一提的是，一开始我并没有预料到他能学习到图像中的文本特征，但是实验中我发现，如果图片中有文字的话，也能被它检索到，比如这个关键词：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904200822620.png\" alt=\"image-20220904200822620\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904200950871.png\" alt=\"image-20220904200950871\"></p>\n<p>我这里是输入了这个midi键盘的名称（是刻在键盘上的文字），结果它也能给我识别出来，感觉非常的神奇。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Clip-模型与其应用\"><a href=\"#Clip-模型与其应用\" class=\"headerlink\" title=\"Clip 模型与其应用\"></a>Clip 模型与其应用</h1><h2 id=\"CLIP-模型的介绍\"><a href=\"#CLIP-模型的介绍\" class=\"headerlink\" title=\"CLIP 模型的介绍\"></a>CLIP 模型的介绍</h2><p>CLIP 模型使用的方法：对比学习，预测 n*n 对图像与文本数据，将图片分类任务转换成图文匹配任务。这个过程实际上就是引入了 NLP 给出的监督信号。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904193225832.png\" alt=\"image-20220904193225832\"></p>\n<p>图中左侧，得到文本特征与图片的特征后可以看到，对角线上的元素都是图文匹配的，共有 N 个正样本，其余的元素都是负样本，共有 N*N-N 个。其中，文本数据使用Transformer，图片数据用了两种模型，ResNet 和 Vision Transformer (ViT)。</p>\n<p>CLIP 模型训练所用的数据集较为庞大，包含从互联网上各种公开资源收集的4亿对图像、文本，CLIP是从头开始训练的，没有使用预训练的初始参数。</p>\n<h2 id=\"CLIP-的应用\"><a href=\"#CLIP-的应用\" class=\"headerlink\" title=\"CLIP 的应用\"></a>CLIP 的应用</h2><h2 id=\"利用-CLIP-做简单的图片检索\"><a href=\"#利用-CLIP-做简单的图片检索\" class=\"headerlink\" title=\"利用 CLIP 做简单的图片检索\"></a>利用 CLIP 做简单的图片检索</h2><p>在 jupyter notebook 中做一个简单的问答系统：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> clip</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">from</span> PIL <span class=\"keyword\">import</span> Image</span><br><span class=\"line\"><span class=\"keyword\">from</span> IPython.display <span class=\"keyword\">import</span> display</span><br><span class=\"line\"><span class=\"keyword\">from</span> tqdm.notebook <span class=\"keyword\">import</span> tqdm</span><br><span class=\"line\"></span><br><span class=\"line\">device = <span class=\"string\">&quot;cuda&quot;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&quot;cpu&quot;</span></span><br><span class=\"line\">model, preprocess = clip.load(<span class=\"string\">&quot;ViT-B/32&quot;</span>, device=device)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># load images</span></span><br><span class=\"line\"></span><br><span class=\"line\">data_location =  <span class=\"string\">&quot;./imgs&quot;</span></span><br><span class=\"line\">img_dict = &#123;&#125;</span><br><span class=\"line\"><span class=\"keyword\">for</span> inx, f <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(os.listdir(data_location)):</span><br><span class=\"line\">  img_dict[inx] = f</span><br><span class=\"line\">img_nums = <span class=\"built_in\">len</span>(img_dict)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;There are &#123;&#125; images.&quot;</span>.<span class=\"built_in\">format</span>(img_nums))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> img_nums == <span class=\"number\">0</span>:</span><br><span class=\"line\">  <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;no image in the folder.&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>这一段用来加载模型 以及 读取文件夹中所有的图片</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># get text input from user &amp;&amp; text encode</span></span><br><span class=\"line\">instr = <span class=\"built_in\">input</span>(<span class=\"string\">&#x27;Pleaze input description:&#x27;</span>)</span><br><span class=\"line\">text_input = clip.tokenize(instr).to(device)</span><br><span class=\"line\"><span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">  text_f = model.encode_text(text_input)</span><br><span class=\"line\"></span><br><span class=\"line\">sim = &#123;&#125;</span><br><span class=\"line\">pbar = tqdm(total=img_nums)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(img_nums):</span><br><span class=\"line\">  </span><br><span class=\"line\">  image_path = <span class=\"string\">f&#x27;<span class=\"subst\">&#123;data_location&#125;</span>/<span class=\"subst\">&#123;img_dict[i]&#125;</span>&#x27;</span></span><br><span class=\"line\">  img = Image.<span class=\"built_in\">open</span>(image_path)</span><br><span class=\"line\">  img_input = preprocess(img).unsqueeze(<span class=\"number\">0</span>).to(device)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\"># image encode</span></span><br><span class=\"line\">  <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">    img_f = model.encode_image(img_input)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\"># calculate similarity</span></span><br><span class=\"line\">  img_f /= img_f.norm(dim=-<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\">  text_f /= text_f.norm(dim=-<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\">  similarity = <span class=\"number\">100</span> * img_f @ text_f.T</span><br><span class=\"line\">  sim[i] = similarity</span><br><span class=\"line\">  pbar.update(<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># display top3 result</span></span><br><span class=\"line\">res = <span class=\"built_in\">sorted</span>(sim.items(), key=<span class=\"keyword\">lambda</span> s:s[<span class=\"number\">1</span>], reverse=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;query:<span class=\"subst\">&#123;instr&#125;</span>&#x27;</span>)</span><br><span class=\"line\">MAX_SIZE = (<span class=\"number\">300</span>, <span class=\"number\">300</span>)</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">3</span>):</span><br><span class=\"line\">  image_path = <span class=\"string\">f&#x27;<span class=\"subst\">&#123;data_location&#125;</span>/<span class=\"subst\">&#123;img_dict[res[i][<span class=\"number\">0</span>]]&#125;</span>&#x27;</span></span><br><span class=\"line\">  img = Image.<span class=\"built_in\">open</span>(image_path)</span><br><span class=\"line\">  img.thumbnail(MAX_SIZE)</span><br><span class=\"line\">  display(img)</span><br></pre></td></tr></table></figure>\n<p>这里先读入用户的输入，即搜索图片的关键词/句，然后将文本编码得到特征，然后分别对应所有图片的特征计算相似度，取相似度最高的三张图片输出。</p>\n<p>图片我没有找网上的一些数据集（因为大部分应该已经有人测试过了），我想看一看这个模型到底有多强大，于是我选取了26张我自己拍的一些生活照，进行了一下图像压缩，控制在500kb之内，作为本实验的数据集。</p>\n<p>经过我的一些尝试，发现准确度还是很高的，得到的结果令人满意。下面展示一些查询的结果：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904195212924.png\" alt=\"image-20220904195212924\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904195756669.png\" alt=\"image-20220904195756669\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904200643952.png\" alt=\"image-20220904200643952\"></p>\n<p>值得一提的是，一开始我并没有预料到他能学习到图像中的文本特征，但是实验中我发现，如果图片中有文字的话，也能被它检索到，比如这个关键词：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904200822620.png\" alt=\"image-20220904200822620\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904200950871.png\" alt=\"image-20220904200950871\"></p>\n<p>我这里是输入了这个midi键盘的名称（是刻在键盘上的文字），结果它也能给我识别出来，感觉非常的神奇。</p>\n"},{"title":"GSoC 2022 Proposal 申请书","date":"2022-04-08T13:17:08.000Z","_content":"\n今年终于鼓起勇气参加了 GSoC (Google Summer of Code)，加入的组织是去年我就看上的 GNU Octave。\n\n今年的 GSoC 我还是做了一些准备工作，写下了一些官方相关文档的解释，感兴趣的可以查看：\n\n[Time management for contributors 开发时间安排](https://hackmd.io/@1099255210/S117Bf6Qq)\n\n[GSoC 2022 Timeline 时间线](https://hackmd.io/@1099255210/S1SUtEKQc)\n\n以下是我的 GSoC 2022 Proposal 申请书。\n\n（2022/5/21 注：这篇申请书最终没有通过，我的 GSoC 2022 计划最终只能搁浅。具体原因可能有很多吧，不过我还是挺希望自己之后能最终独立完成这个项目。）\n\n# YAML Support for GNU-Octave\n\n> Name: Han Wang\n>\n> Email: jmfymxx@gmail.com\n>\n> Github: https://github.com/1099255210\n>\n> Accout on [Octave Discourse](https://octave.discourse.group/): wh-1099255210\n\n## Project Description\n\nYAML, as well as JSON, is a very common data format. GNU Octave has built-in support of JSON, but still lacks of YAML support. The goal of the project is to implementing YAML support into Octave with [Rapid YAML](https://github.com/biojppm/rapidyaml) (a fast C++ library). During the project, an Octave package containing `yamldecode()` & `yamlencode()` functions with proper documentation and unit tests will be created, and the package will be considered to be merged into core Octave. \n\n## Related Work\n\nIn GSoC 2020, JSON format has been successfully implemented. The package [pkg-json](https://github.com/gnu-octave/pkg-json) is a good example on how to encode and decode this kind of data format. The pkg-json used [rapidjson](https://github.com/Tencent/rapidjson) (a JSON parser and generator for C++), while in my project I decide to use [Rapid YAML](https://github.com/biojppm/rapidyaml), which is a C++ library to parse and emit YAML.\n\n## My Project Plan\n\nFirst, I have to get familiar with YAML data format in case to know the proper conversion between YAML data types and Octave data types. I have made a table on this:\n\nFrom YAML to Octave data (will be used in `yamldecode()`)\n\n| YAML Data Type                              | Octave Data Type            |\n| ------------------------------------------- | --------------------------- |\n| Number                                      | scalar double               |\n| Boolean                                     | scalar logical              |\n| String                                      | character vector            |\n| Dictionary                                  | scalar struct               |\n| List, of Numbers                            | double array                |\n| List, of Booleans                           | logical array               |\n| List, of Strings                            | string array                |\n| List, of Dictionary - Same field names      | struct array                |\n| List, of Dictionary - Different field names | cell array of scalar struct |\n| null, in numeric lists                      | NaN                         |\n| null, in nonnumeric lists                   | Empty double array          |\n\nFrom Octave data to YAML (will be used in `yamlencode()`)\n\n| Octave Data Type   | YAML Data Type                                        |\n| ------------------ | ----------------------------------------------------- |\n| scalar logical     | Boolean                                               |\n| logical vector     | List, of Booleans, reshaped to row vector             |\n| logical array      | List, of Booleans, nested                             |\n| scalar numeric     | Number                                                |\n| numeric vector     | List, of Numbers, reshaped to row vector              |\n| numeric array      | List, of Numbers, nested                              |\n| NaN, NA, Inf, -Inf | null (1) \\  \"NaN\", \"NaN\", \"Infinity\", \"-Infinity\" (2) |\n| character vector   | String                                                |\n| character array    | List, of Strings                                      |\n| scalar cell        | List                                                  |\n| cell vector        | List, reshaped to row vector                          |\n| cell array         | List, flattened to row vector                         |\n| scalar struct      | Dictionary                                            |\n| struct vector      | List, of Dictionaries, reshaped to row vector         |\n| struct array       | List, of Dictionaries, nested                         |\n\n(1) `ConvertInfAndNan = true`\n\n(2) `ConvertInfAndNan = false`\n\nWith tables above, it will be much easier to convert data from one side to the other side. I just need to recursively decompose data, convert them respectively to what they should be, then put them together as an output.\n\n## My Timeline\n\n![image-20220408144000601](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20220408144000601.png)\n\n## About Me\n\nI'm currently a junior CS student. I'm good at C/C++ programming, and work with JSON/YAML very often, so I'm familiar with them. I'm used to putting my codes on github and using git to manage them, but I've never really been a contributor in open source projects. So this is my first time to be a part of the open source community, and I'm really excited about it.\n\nDuring the past few weeks, I have joined the **GNU Octave Developer Community,** and have been a member in **Octave discourse**. Under the guidance of the mentor, I have created the [pkg-yaml](https://github.com/gnu-octave/pkg-yaml) repository on github, committed my code to it. The package can now be installed, and has the function to parse simple YAML data to Octave data. I also sent a pull request to [Rapid YAML](https://github.com/biojppm/rapidyaml) to help fix their amalgamate script to generate header files, and it [successfully got merged into master branch](https://github.com/biojppm/rapidyaml/commit/a04663f55fa1d5a58c873017e24849309a29c226).\n\nI believe with my enthusiasm and patience, I will be up to this job.","source":"_posts/GSoC 2022 Proposal 申请书.md","raw":"---\ntitle: GSoC 2022 Proposal 申请书\ndate: 2022-04-08 21:17:08\ntags:\n---\n\n今年终于鼓起勇气参加了 GSoC (Google Summer of Code)，加入的组织是去年我就看上的 GNU Octave。\n\n今年的 GSoC 我还是做了一些准备工作，写下了一些官方相关文档的解释，感兴趣的可以查看：\n\n[Time management for contributors 开发时间安排](https://hackmd.io/@1099255210/S117Bf6Qq)\n\n[GSoC 2022 Timeline 时间线](https://hackmd.io/@1099255210/S1SUtEKQc)\n\n以下是我的 GSoC 2022 Proposal 申请书。\n\n（2022/5/21 注：这篇申请书最终没有通过，我的 GSoC 2022 计划最终只能搁浅。具体原因可能有很多吧，不过我还是挺希望自己之后能最终独立完成这个项目。）\n\n# YAML Support for GNU-Octave\n\n> Name: Han Wang\n>\n> Email: jmfymxx@gmail.com\n>\n> Github: https://github.com/1099255210\n>\n> Accout on [Octave Discourse](https://octave.discourse.group/): wh-1099255210\n\n## Project Description\n\nYAML, as well as JSON, is a very common data format. GNU Octave has built-in support of JSON, but still lacks of YAML support. The goal of the project is to implementing YAML support into Octave with [Rapid YAML](https://github.com/biojppm/rapidyaml) (a fast C++ library). During the project, an Octave package containing `yamldecode()` & `yamlencode()` functions with proper documentation and unit tests will be created, and the package will be considered to be merged into core Octave. \n\n## Related Work\n\nIn GSoC 2020, JSON format has been successfully implemented. The package [pkg-json](https://github.com/gnu-octave/pkg-json) is a good example on how to encode and decode this kind of data format. The pkg-json used [rapidjson](https://github.com/Tencent/rapidjson) (a JSON parser and generator for C++), while in my project I decide to use [Rapid YAML](https://github.com/biojppm/rapidyaml), which is a C++ library to parse and emit YAML.\n\n## My Project Plan\n\nFirst, I have to get familiar with YAML data format in case to know the proper conversion between YAML data types and Octave data types. I have made a table on this:\n\nFrom YAML to Octave data (will be used in `yamldecode()`)\n\n| YAML Data Type                              | Octave Data Type            |\n| ------------------------------------------- | --------------------------- |\n| Number                                      | scalar double               |\n| Boolean                                     | scalar logical              |\n| String                                      | character vector            |\n| Dictionary                                  | scalar struct               |\n| List, of Numbers                            | double array                |\n| List, of Booleans                           | logical array               |\n| List, of Strings                            | string array                |\n| List, of Dictionary - Same field names      | struct array                |\n| List, of Dictionary - Different field names | cell array of scalar struct |\n| null, in numeric lists                      | NaN                         |\n| null, in nonnumeric lists                   | Empty double array          |\n\nFrom Octave data to YAML (will be used in `yamlencode()`)\n\n| Octave Data Type   | YAML Data Type                                        |\n| ------------------ | ----------------------------------------------------- |\n| scalar logical     | Boolean                                               |\n| logical vector     | List, of Booleans, reshaped to row vector             |\n| logical array      | List, of Booleans, nested                             |\n| scalar numeric     | Number                                                |\n| numeric vector     | List, of Numbers, reshaped to row vector              |\n| numeric array      | List, of Numbers, nested                              |\n| NaN, NA, Inf, -Inf | null (1) \\  \"NaN\", \"NaN\", \"Infinity\", \"-Infinity\" (2) |\n| character vector   | String                                                |\n| character array    | List, of Strings                                      |\n| scalar cell        | List                                                  |\n| cell vector        | List, reshaped to row vector                          |\n| cell array         | List, flattened to row vector                         |\n| scalar struct      | Dictionary                                            |\n| struct vector      | List, of Dictionaries, reshaped to row vector         |\n| struct array       | List, of Dictionaries, nested                         |\n\n(1) `ConvertInfAndNan = true`\n\n(2) `ConvertInfAndNan = false`\n\nWith tables above, it will be much easier to convert data from one side to the other side. I just need to recursively decompose data, convert them respectively to what they should be, then put them together as an output.\n\n## My Timeline\n\n![image-20220408144000601](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20220408144000601.png)\n\n## About Me\n\nI'm currently a junior CS student. I'm good at C/C++ programming, and work with JSON/YAML very often, so I'm familiar with them. I'm used to putting my codes on github and using git to manage them, but I've never really been a contributor in open source projects. So this is my first time to be a part of the open source community, and I'm really excited about it.\n\nDuring the past few weeks, I have joined the **GNU Octave Developer Community,** and have been a member in **Octave discourse**. Under the guidance of the mentor, I have created the [pkg-yaml](https://github.com/gnu-octave/pkg-yaml) repository on github, committed my code to it. The package can now be installed, and has the function to parse simple YAML data to Octave data. I also sent a pull request to [Rapid YAML](https://github.com/biojppm/rapidyaml) to help fix their amalgamate script to generate header files, and it [successfully got merged into master branch](https://github.com/biojppm/rapidyaml/commit/a04663f55fa1d5a58c873017e24849309a29c226).\n\nI believe with my enthusiasm and patience, I will be up to this job.","slug":"GSoC 2022 Proposal 申请书","published":1,"updated":"2022-08-25T13:13:32.983Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clgyiartr0003jku75ahwet8p","content":"<p>今年终于鼓起勇气参加了 GSoC (Google Summer of Code)，加入的组织是去年我就看上的 GNU Octave。</p>\n<p>今年的 GSoC 我还是做了一些准备工作，写下了一些官方相关文档的解释，感兴趣的可以查看：</p>\n<p><a href=\"https://hackmd.io/@1099255210/S117Bf6Qq\">Time management for contributors 开发时间安排</a></p>\n<p><a href=\"https://hackmd.io/@1099255210/S1SUtEKQc\">GSoC 2022 Timeline 时间线</a></p>\n<p>以下是我的 GSoC 2022 Proposal 申请书。</p>\n<p>（2022/5/21 注：这篇申请书最终没有通过，我的 GSoC 2022 计划最终只能搁浅。具体原因可能有很多吧，不过我还是挺希望自己之后能最终独立完成这个项目。）</p>\n<h1 id=\"YAML-Support-for-GNU-Octave\"><a href=\"#YAML-Support-for-GNU-Octave\" class=\"headerlink\" title=\"YAML Support for GNU-Octave\"></a>YAML Support for GNU-Octave</h1><blockquote>\n<p>Name: Han Wang</p>\n<p>Email: jmfymxx@gmail.com</p>\n<p>Github: <a href=\"https://github.com/1099255210\">https://github.com/1099255210</a></p>\n<p>Accout on <a href=\"https://octave.discourse.group/\">Octave Discourse</a>: wh-1099255210</p>\n</blockquote>\n<h2 id=\"Project-Description\"><a href=\"#Project-Description\" class=\"headerlink\" title=\"Project Description\"></a>Project Description</h2><p>YAML, as well as JSON, is a very common data format. GNU Octave has built-in support of JSON, but still lacks of YAML support. The goal of the project is to implementing YAML support into Octave with <a href=\"https://github.com/biojppm/rapidyaml\">Rapid YAML</a> (a fast C++ library). During the project, an Octave package containing <code>yamldecode()</code> &amp; <code>yamlencode()</code> functions with proper documentation and unit tests will be created, and the package will be considered to be merged into core Octave. </p>\n<h2 id=\"Related-Work\"><a href=\"#Related-Work\" class=\"headerlink\" title=\"Related Work\"></a>Related Work</h2><p>In GSoC 2020, JSON format has been successfully implemented. The package <a href=\"https://github.com/gnu-octave/pkg-json\">pkg-json</a> is a good example on how to encode and decode this kind of data format. The pkg-json used <a href=\"https://github.com/Tencent/rapidjson\">rapidjson</a> (a JSON parser and generator for C++), while in my project I decide to use <a href=\"https://github.com/biojppm/rapidyaml\">Rapid YAML</a>, which is a C++ library to parse and emit YAML.</p>\n<h2 id=\"My-Project-Plan\"><a href=\"#My-Project-Plan\" class=\"headerlink\" title=\"My Project Plan\"></a>My Project Plan</h2><p>First, I have to get familiar with YAML data format in case to know the proper conversion between YAML data types and Octave data types. I have made a table on this:</p>\n<p>From YAML to Octave data (will be used in <code>yamldecode()</code>)</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>YAML Data Type</th>\n<th>Octave Data Type</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Number</td>\n<td>scalar double</td>\n</tr>\n<tr>\n<td>Boolean</td>\n<td>scalar logical</td>\n</tr>\n<tr>\n<td>String</td>\n<td>character vector</td>\n</tr>\n<tr>\n<td>Dictionary</td>\n<td>scalar struct</td>\n</tr>\n<tr>\n<td>List, of Numbers</td>\n<td>double array</td>\n</tr>\n<tr>\n<td>List, of Booleans</td>\n<td>logical array</td>\n</tr>\n<tr>\n<td>List, of Strings</td>\n<td>string array</td>\n</tr>\n<tr>\n<td>List, of Dictionary - Same field names</td>\n<td>struct array</td>\n</tr>\n<tr>\n<td>List, of Dictionary - Different field names</td>\n<td>cell array of scalar struct</td>\n</tr>\n<tr>\n<td>null, in numeric lists</td>\n<td>NaN</td>\n</tr>\n<tr>\n<td>null, in nonnumeric lists</td>\n<td>Empty double array</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>From Octave data to YAML (will be used in <code>yamlencode()</code>)</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>Octave Data Type</th>\n<th>YAML Data Type</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>scalar logical</td>\n<td>Boolean</td>\n</tr>\n<tr>\n<td>logical vector</td>\n<td>List, of Booleans, reshaped to row vector</td>\n</tr>\n<tr>\n<td>logical array</td>\n<td>List, of Booleans, nested</td>\n</tr>\n<tr>\n<td>scalar numeric</td>\n<td>Number</td>\n</tr>\n<tr>\n<td>numeric vector</td>\n<td>List, of Numbers, reshaped to row vector</td>\n</tr>\n<tr>\n<td>numeric array</td>\n<td>List, of Numbers, nested</td>\n</tr>\n<tr>\n<td>NaN, NA, Inf, -Inf</td>\n<td>null (1) \\  “NaN”, “NaN”, “Infinity”, “-Infinity” (2)</td>\n</tr>\n<tr>\n<td>character vector</td>\n<td>String</td>\n</tr>\n<tr>\n<td>character array</td>\n<td>List, of Strings</td>\n</tr>\n<tr>\n<td>scalar cell</td>\n<td>List</td>\n</tr>\n<tr>\n<td>cell vector</td>\n<td>List, reshaped to row vector</td>\n</tr>\n<tr>\n<td>cell array</td>\n<td>List, flattened to row vector</td>\n</tr>\n<tr>\n<td>scalar struct</td>\n<td>Dictionary</td>\n</tr>\n<tr>\n<td>struct vector</td>\n<td>List, of Dictionaries, reshaped to row vector</td>\n</tr>\n<tr>\n<td>struct array</td>\n<td>List, of Dictionaries, nested</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>(1) <code>ConvertInfAndNan = true</code></p>\n<p>(2) <code>ConvertInfAndNan = false</code></p>\n<p>With tables above, it will be much easier to convert data from one side to the other side. I just need to recursively decompose data, convert them respectively to what they should be, then put them together as an output.</p>\n<h2 id=\"My-Timeline\"><a href=\"#My-Timeline\" class=\"headerlink\" title=\"My Timeline\"></a>My Timeline</h2><p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20220408144000601.png\" alt=\"image-20220408144000601\"></p>\n<h2 id=\"About-Me\"><a href=\"#About-Me\" class=\"headerlink\" title=\"About Me\"></a>About Me</h2><p>I’m currently a junior CS student. I’m good at C/C++ programming, and work with JSON/YAML very often, so I’m familiar with them. I’m used to putting my codes on github and using git to manage them, but I’ve never really been a contributor in open source projects. So this is my first time to be a part of the open source community, and I’m really excited about it.</p>\n<p>During the past few weeks, I have joined the <strong>GNU Octave Developer Community,</strong> and have been a member in <strong>Octave discourse</strong>. Under the guidance of the mentor, I have created the <a href=\"https://github.com/gnu-octave/pkg-yaml\">pkg-yaml</a> repository on github, committed my code to it. The package can now be installed, and has the function to parse simple YAML data to Octave data. I also sent a pull request to <a href=\"https://github.com/biojppm/rapidyaml\">Rapid YAML</a> to help fix their amalgamate script to generate header files, and it <a href=\"https://github.com/biojppm/rapidyaml/commit/a04663f55fa1d5a58c873017e24849309a29c226\">successfully got merged into master branch</a>.</p>\n<p>I believe with my enthusiasm and patience, I will be up to this job.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>今年终于鼓起勇气参加了 GSoC (Google Summer of Code)，加入的组织是去年我就看上的 GNU Octave。</p>\n<p>今年的 GSoC 我还是做了一些准备工作，写下了一些官方相关文档的解释，感兴趣的可以查看：</p>\n<p><a href=\"https://hackmd.io/@1099255210/S117Bf6Qq\">Time management for contributors 开发时间安排</a></p>\n<p><a href=\"https://hackmd.io/@1099255210/S1SUtEKQc\">GSoC 2022 Timeline 时间线</a></p>\n<p>以下是我的 GSoC 2022 Proposal 申请书。</p>\n<p>（2022/5/21 注：这篇申请书最终没有通过，我的 GSoC 2022 计划最终只能搁浅。具体原因可能有很多吧，不过我还是挺希望自己之后能最终独立完成这个项目。）</p>\n<h1 id=\"YAML-Support-for-GNU-Octave\"><a href=\"#YAML-Support-for-GNU-Octave\" class=\"headerlink\" title=\"YAML Support for GNU-Octave\"></a>YAML Support for GNU-Octave</h1><blockquote>\n<p>Name: Han Wang</p>\n<p>Email: jmfymxx@gmail.com</p>\n<p>Github: <a href=\"https://github.com/1099255210\">https://github.com/1099255210</a></p>\n<p>Accout on <a href=\"https://octave.discourse.group/\">Octave Discourse</a>: wh-1099255210</p>\n</blockquote>\n<h2 id=\"Project-Description\"><a href=\"#Project-Description\" class=\"headerlink\" title=\"Project Description\"></a>Project Description</h2><p>YAML, as well as JSON, is a very common data format. GNU Octave has built-in support of JSON, but still lacks of YAML support. The goal of the project is to implementing YAML support into Octave with <a href=\"https://github.com/biojppm/rapidyaml\">Rapid YAML</a> (a fast C++ library). During the project, an Octave package containing <code>yamldecode()</code> &amp; <code>yamlencode()</code> functions with proper documentation and unit tests will be created, and the package will be considered to be merged into core Octave. </p>\n<h2 id=\"Related-Work\"><a href=\"#Related-Work\" class=\"headerlink\" title=\"Related Work\"></a>Related Work</h2><p>In GSoC 2020, JSON format has been successfully implemented. The package <a href=\"https://github.com/gnu-octave/pkg-json\">pkg-json</a> is a good example on how to encode and decode this kind of data format. The pkg-json used <a href=\"https://github.com/Tencent/rapidjson\">rapidjson</a> (a JSON parser and generator for C++), while in my project I decide to use <a href=\"https://github.com/biojppm/rapidyaml\">Rapid YAML</a>, which is a C++ library to parse and emit YAML.</p>\n<h2 id=\"My-Project-Plan\"><a href=\"#My-Project-Plan\" class=\"headerlink\" title=\"My Project Plan\"></a>My Project Plan</h2><p>First, I have to get familiar with YAML data format in case to know the proper conversion between YAML data types and Octave data types. I have made a table on this:</p>\n<p>From YAML to Octave data (will be used in <code>yamldecode()</code>)</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>YAML Data Type</th>\n<th>Octave Data Type</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Number</td>\n<td>scalar double</td>\n</tr>\n<tr>\n<td>Boolean</td>\n<td>scalar logical</td>\n</tr>\n<tr>\n<td>String</td>\n<td>character vector</td>\n</tr>\n<tr>\n<td>Dictionary</td>\n<td>scalar struct</td>\n</tr>\n<tr>\n<td>List, of Numbers</td>\n<td>double array</td>\n</tr>\n<tr>\n<td>List, of Booleans</td>\n<td>logical array</td>\n</tr>\n<tr>\n<td>List, of Strings</td>\n<td>string array</td>\n</tr>\n<tr>\n<td>List, of Dictionary - Same field names</td>\n<td>struct array</td>\n</tr>\n<tr>\n<td>List, of Dictionary - Different field names</td>\n<td>cell array of scalar struct</td>\n</tr>\n<tr>\n<td>null, in numeric lists</td>\n<td>NaN</td>\n</tr>\n<tr>\n<td>null, in nonnumeric lists</td>\n<td>Empty double array</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>From Octave data to YAML (will be used in <code>yamlencode()</code>)</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>Octave Data Type</th>\n<th>YAML Data Type</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>scalar logical</td>\n<td>Boolean</td>\n</tr>\n<tr>\n<td>logical vector</td>\n<td>List, of Booleans, reshaped to row vector</td>\n</tr>\n<tr>\n<td>logical array</td>\n<td>List, of Booleans, nested</td>\n</tr>\n<tr>\n<td>scalar numeric</td>\n<td>Number</td>\n</tr>\n<tr>\n<td>numeric vector</td>\n<td>List, of Numbers, reshaped to row vector</td>\n</tr>\n<tr>\n<td>numeric array</td>\n<td>List, of Numbers, nested</td>\n</tr>\n<tr>\n<td>NaN, NA, Inf, -Inf</td>\n<td>null (1) \\  “NaN”, “NaN”, “Infinity”, “-Infinity” (2)</td>\n</tr>\n<tr>\n<td>character vector</td>\n<td>String</td>\n</tr>\n<tr>\n<td>character array</td>\n<td>List, of Strings</td>\n</tr>\n<tr>\n<td>scalar cell</td>\n<td>List</td>\n</tr>\n<tr>\n<td>cell vector</td>\n<td>List, reshaped to row vector</td>\n</tr>\n<tr>\n<td>cell array</td>\n<td>List, flattened to row vector</td>\n</tr>\n<tr>\n<td>scalar struct</td>\n<td>Dictionary</td>\n</tr>\n<tr>\n<td>struct vector</td>\n<td>List, of Dictionaries, reshaped to row vector</td>\n</tr>\n<tr>\n<td>struct array</td>\n<td>List, of Dictionaries, nested</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>(1) <code>ConvertInfAndNan = true</code></p>\n<p>(2) <code>ConvertInfAndNan = false</code></p>\n<p>With tables above, it will be much easier to convert data from one side to the other side. I just need to recursively decompose data, convert them respectively to what they should be, then put them together as an output.</p>\n<h2 id=\"My-Timeline\"><a href=\"#My-Timeline\" class=\"headerlink\" title=\"My Timeline\"></a>My Timeline</h2><p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20220408144000601.png\" alt=\"image-20220408144000601\"></p>\n<h2 id=\"About-Me\"><a href=\"#About-Me\" class=\"headerlink\" title=\"About Me\"></a>About Me</h2><p>I’m currently a junior CS student. I’m good at C/C++ programming, and work with JSON/YAML very often, so I’m familiar with them. I’m used to putting my codes on github and using git to manage them, but I’ve never really been a contributor in open source projects. So this is my first time to be a part of the open source community, and I’m really excited about it.</p>\n<p>During the past few weeks, I have joined the <strong>GNU Octave Developer Community,</strong> and have been a member in <strong>Octave discourse</strong>. Under the guidance of the mentor, I have created the <a href=\"https://github.com/gnu-octave/pkg-yaml\">pkg-yaml</a> repository on github, committed my code to it. The package can now be installed, and has the function to parse simple YAML data to Octave data. I also sent a pull request to <a href=\"https://github.com/biojppm/rapidyaml\">Rapid YAML</a> to help fix their amalgamate script to generate header files, and it <a href=\"https://github.com/biojppm/rapidyaml/commit/a04663f55fa1d5a58c873017e24849309a29c226\">successfully got merged into master branch</a>.</p>\n<p>I believe with my enthusiasm and patience, I will be up to this job.</p>\n"},{"title":"CS:GO中的cfg系统","date":"2020-09-28T09:02:08.000Z","_content":"\n## 导语\n\n本人写文章的时候，**CS:GO**游戏时长已经来到了1800小时。这个游戏不仅仅是竞技的快感让我无法自拔，游戏系统与生俱来的“程序员”风格也是让我无法割舍的（毕竟是v社的游戏）。在遇到CS:GO前，我无法想象一个游戏的**控制台系统**能够强大到让我痴迷。离开了控制台，也就离开了这个游戏的特色。那么谈到控制台，就不得不谈谈**cfg**。本文希望能用较为简单易懂的语言来谈谈CS:GO中的cfg系统，希望能给大家一些帮助。\n\n## 知识补充\n\n### 简单设置\n\n**cfg**其实就是设置(config)的缩写。.cfg也是文件的后缀名，我们在游戏根目录里能找到许多这样的文件。这些文件的作用就是储存我们在游戏里的设置参数。打个比方，我的游戏音量是1.0，那么我就能在.cfg的文件里找到这么一行：\n\n```\nvolume \"1.0\"\n```\n\n前面的volume指的是音量，后面的\"1.0\"是数值，代表音量的值为1.0。这个例子反映了大部分设置的基本格式，基本都是一个**设置项目名**，后面跟着一个**具体的数值**。一对一的关系还是很易懂的。由此，我们了解了调整设置的基本语法为：\n\n```\n[设置项目名] \"[具体数值]\"\n```\n\n\n双引号在这里不是必须的，但有些时候需要，为了规范我们尽量加上(游戏中控制台不需要)。\n\n我们可以将上面这一行代码称为一个指令。\n\n我们再举几个真实文件中的例子：\n\n```\ncl_crosshair_drawoutline \t\t\t\t\t        \"1\"\ncl_crosshair_dynamic_maxdist_splitratio \t\"0.35\"\ncl_crosshair_dynamic_splitalpha_innermod \t\"1\"\ncl_crosshair_dynamic_splitalpha_outermod \t\"0.5\"\ncl_crosshair_dynamic_splitdist \t\t\t\t    \"7\"\ncl_crosshair_friendly_warning \t\t\t\t    \"1\"\ncl_crosshair_outlinethickness \t\t\t\t    \"1.067771\"\ncl_crosshair_sniper_show_normal_inaccuracy\"0\"\ncl_crosshair_sniper_width \t\t\t\t\t      \"1\"\ncl_crosshair_t \t\t\t\t\t\t\t\t            \"0\"\ncl_crosshairalpha \t\t\t\t\t\t\t          \"255.000000\"\ncl_crosshaircolor \t\t\t\t\t\t\t          \"5\"\ncl_crosshaircolor_b\t \t\t\t\t\t\t          \"0.000000\"\ncl_crosshaircolor_g \t\t\t\t\t\t          \"255.000000\"\ncl_crosshaircolor_r \t\t\t\t\t\t          \"255.000000\"\ncl_crosshairdot \t\t\t\t\t\t\t            \"0.000000\"\ncl_crosshairgap \t\t\t\t\t\t\t            \"0.076290\"\ncl_crosshairgap_useweaponvalue \t\t\t\t    \"0\"\ncl_crosshairsize \t\t\t\t\t\t\t            \"2.917408\"\ncl_crosshairstyle \t\t\t\t\t\t\t          \"4.000000\"\ncl_crosshairthickness \t\t\t\t\t\t        \"1.692905\"\ncl_crosshairusealpha \t\t\t\t\t\t          \"1\"\n```\n\n这是控制**游戏准星**相关的代码，由很多行指令组成。crosshair意思是准星，因此我们知道带有crosshair的内容都是和准星有关的，这段代码也是本人在游戏中的所使用的准星设置。我带着大家解读几行：\n\n```\ncl_crosshair_drawoutline \t\t\t\t\t\"1\"\n```\n\n字面意思来看，crosshair_drawoutline显然指的是准星的轮廓，这里后面的数值则应该为1或0，表示有或无轮廓。\n\n```\ncl_crosshairalpha \t\t\t\t\t\t\t\"255.000000\"\n```\n\n字面意思来看，crosshairalpha显然指的是准星的透明度，这里后面的数值我设置的是255，表示透明度为100%。\n\n实际上我们可以发现，cfg中的简单设置内容往往都很易懂，用最少的词表达得很清楚，后面的数值也自然容易理解。这一类的设置我们不再多说，后面会详细介绍几个常用的设置内容。\n\n### cfg中指令的类型\n\n上面提到的**\\[设置项目名\\]+\\[一个具体的数值\\]**只是指令中的一种，cfg中可不止这一种指令。\n\n下面我们介绍的这些指令是**只有名称，没有值**的。这种指令往往是一个动作，比如：清除血迹、重新投掷上一个投掷物、退出服务器等等：\n\n```\nr_cleaardecals;\t\t\t\t\t\t\t\t//清除血迹\nsv_rethrow_last_grenade;\t\t\t\t\t//重新投掷上一个投掷物\ndisconnect;\t\t\t\t\t\t\t\t\t//断开连接\nquit;\t\t\t\t\t\t\t\t\t\t//退出游戏(光速白给)\nretry;\t\t\t\t\t\t\t\t\t\t//重新连接\n```\n\n这些指令和上面介绍的设置指令可以说都是具体的指令，下面我们介绍的一些指令类型，则是属于非具体的指令，我们可以将其理解为一些函数，那么我们开始吧。\n\n### bind的作用\n\ncfg中不只是有简单的设置，按键的绑定也是cfg中不可或缺的。按键绑定的基本代码是：\n\n```\nbind \"[按键]\" \"[指令]\"\n```\n\n我列举一些游戏中基础的绑定代码展示一下：\n\n```\nbind \"w\" \"+forward\"\nbind \"a\" \"+moveleft\"\nbind \"s\" \"+back\"\nbind \"d\" \"+moveright\"\nbind \"b\" \"buymenu\"\nbind \"m\" \"teammenu\"\nbind \"q\" \"lastinv\"\nbind \"r\" \"+reload\"\nbind \"SPACE\" \"+jump\"\nbind \"TAB\" \"+showscores\"\nbind \"SHIFT\" \"+speed\"\nbind \"CTRL\" \"+duck\"\nbind \"MOUSE1\" \"+attack\"\nbind \"MOUSE2\" \"+attack2\"\n```\n\n这些基本是游戏基本按键的绑定了，前四行简单明了，wasd控制前后左右，b键购买菜单(buymenu)，m键选择队伍(teammenu)，q键切回上一个武器(lastinv)，r键换弹，空格(space)跳跃，Tab键看战绩，Shift行走，Ctrl键蹲下，左键(MOUSE1)攻击或者射击，右键(MOUSE2)武器第二操作。这些都是十分易懂的代码行，因此读到这里应该不算困难。\n\n以上所有的代码都是一个按键对应一个指令。那么下面我提出一个问题，如果我想要用一个按键操作两个动作可以实现吗？\n\n答案是肯定的，我们看下面这行：\n\n```\nbind \"f\" \"+lookatweapon;r_cleardecals\"\n```\n\n**f**键本来是检视武器(+lookatweapon)的按钮，这里我们后面加了个分号，又写了一个r_cleardecals，这是什么意思呢？解释一下，后面这个指令是清除血迹和弹孔的指令。这两个指令被双引号括起来，相当于是打了个包，按下f键的时候就会同时执行后面的两个指令。\n\n因此我们将bind指令的用法重新定义一下：\n\n```\nbind \"[按键]\" \"[指令或指令集]\"\n```\n\n指令集里的每个指令之间一定要记得打分号。\n\n又比如我想要在按Shift键的时候同时走路和清除血迹，那么我们应该这样修改代码：\n\n```\nbind \"SHIFT\" \"+speed;r_cleardecals\"\n```\n\n再来一个：\n\n```\nbind \"CAPSLOCK\" \"+jump;-attack;-attack2;-jump\"\n```\n\n这是本人的跳投绑定，这也是最简单的实现跳投的方式。我们可以看到后面的指令集有四个指令。第一个是按下跳跃键，第二第三个是松开攻击键，最后是松开跳跃键，仔细想想是不是很简单呢？\n\n当然，如果你想解绑一个按键，则可以使用unbind指令：\n\n```\nunbind \"[按键]\"\n```\n\n比如我想取消上面绑定的跳投键，则可以这么写：\n\n```\nunbind \"CAPSLOCK\"\n```\n\n### toggle的作用\n\n你是否碰到这种情况：队友全部牺牲，只剩你一人面对残局，你聚精会神，这时候队友却吵吵闹闹，让你完全没法集中注意力——这时的你是否想要一个按钮，让该死的队友赶紧闭嘴？很幸运，我们有一个指令能够做到这一点。\n\n有的人可能会说，这个不是很简单吗，我直接绑定一个按键（比如\"ALT\"键）到静音指令上：\n\n```\nbind \"ALT\" \"voice_scale 0\"\n```\n\n这么做固然能达到效果，但是残局打完以后呢？你保持在了静音队友的状态，想要调回去还得去设置里重新设置，较为繁琐。\n\n这里我向大家介绍一个新的指令——**toggle**，指令用法如下：\n\n```\ntoggle \"[设置项目名] [数值1] [数值2] ...[数值n]\"\n```\n\n这个指令看起来有很多参数，但是用法十分简单。toggle的意思为拨动，这里是switch from的意思。我们大概也能猜到了：toggle就是让指定设置项目的数值在后面的数值1、数值2...中变动。\n\n我们来用toggle实现我们上面的需求：\n\n```\nbind \"ALT\" \"toggle voice_scale 0 1\"\n```\n\n这一行的意思就是，当我按下alt键的时候，voice_scale的值变为0；当我再按一次，值变为1，再按一次，又变为0...这样我们就实现了随时静音和恢复语音的功能。\n\n由我们上面的定义，可知toggle的数值可以不止2个，利用这个我们可以进行一些设置较为细微的调整，比如游戏音量的调整：\n\n```\nbind \"PGUP\" \"toggle volume 0 0.1 0.2 0.3 0.5 0.7 0.9\"\n```\n\n还有准星透明度的调整：\n\n```\nbind \"PGDN\" \"toggle cl_crosshairalpha 0 64 128 192 255\"\n```\n\n这样我们就能够实现一个通过按\"Pgdn\"键逐渐增加准星透明度的效果。\n\n当然，大部分时候我们不会用到这么细的调整，很多时候设置的数值都只需要在0和1之间切换而已。这时候我们有个简单一些的新指令BindToggle：\n\n```\nBindToggle \"[按键]\" \"[指令]\"\n```\n\n我们举个例子，cl_righthand 1指的是人物右手持枪，cl_righthand 0是左手持枪，我们想要用v键切换左右手：\n\n```\nBindToggle \"v\" \"cl_righthand\"\n```\n\n这时候按下v，cl_righthand的值就会在0和1之间切换了。\n\n注意：这个BindToggle后面的指令只能是一个而不能是指令集，也就是说使用这个指令的话你无法用一个键控制几个设置项的变动。\n\n> ## 练习<sup>1</sup>:\n>\n> 如果我要把上面这个切换左右手的这个方法用toggle写，该怎么写呢？如果我要用一个键同时控制cl_righthand和volume这两个值呢？\n\n### 加与减的作用\n\n在步入真正高级的内容之前，我们先回顾一下之前讲的知识点，是否有一些没有提及的细节还未涉及呢？\n\n可能有的人早已有了一个疑问：有些指令中的+和-是什么意思？\n\n回到bind那里，我列举了一些指令：\n\n```\nbind \"w\" \"+forward\"\nbind \"a\" \"+moveleft\"\nbind \"s\" \"+back\"\nbind \"d\" \"+moveright\"\n```\n\n这些指令前面的加号，其实是模拟键盘\"按下\"的这个动作。我们按下w键，一定是在按住的时候人物才会一直向前走，松开以后，这个前进的动作就停止了。\n\n由这个\"+\"的定义，我们也不难猜测\"-\"的意思：\n\n```\nbind \"CAPSLOCK\" \"+jump;-attack;-attack2;-jump\"\n```\n\n我们跳投的时候，往往是先拉引信，对准参照物，然后按下跳投键，这里面的\"-attack;-attack2\"就是模拟松开攻击键的动作；后面的\"-jump\"则是模拟松开跳跃键的动作。\n\n为什么在这里介绍加与减的概念？因为下一个指令中会涉及加与减的运用，也是十分有意思的内容。\n\n### alias的作用\n\ncfg中还有一种高级的操作，alias，这在普通的cfg中几乎用不到，但是如果你想要拥有一个高端的cfg，这个指令就十分地重要了。\n\nalias基本语法：\n\n```\nalias \"名称\" \"指令或指令集\"\n```\n\n这看上去有点像函数，只不过没有传递参数。它的功能就是打包一个指令集。\n\n没有听明白？我们举个例子来解释就行了：\n\n```\nalias \"Jumpthrow\" \"+jump;-attack;-attack2;-jump\"\nbind \"CAPSLOCK\" \"Jumpthrow\"\n```\n\n这段代码也是跳投的代码，和上面只用bind指令的效果是一样的，或者说是等价的。这段代码先将跳投的一系列指令放到一起，命名为\"Jumpthrow\"，然后将\"Jumpthrow\"绑定在CAPSLOCK键上。\n\n理解了上面的代码，我们来介绍灵性的部分：加与减要来了！\n\n上一栏提到的加与减，是模拟按下与松开的操作。那么alias定义的名称，是不是也能加与减呢？我们看下面的代码：\n\n```\nalias \"+Jumpthrow\" \"+jump;-attack;-attack2\"\nalias \"-Jumpthrow\" \"-jump\"\nbind \"CAPSLOCK\" \"+Jumpthrow\"\n```\n\n如果我说这几行代码和之前的跳投是等效的，你能够理解其中加和减的含义吗？\n\n其实，alias的真正内涵在这里有了体现：alias其实就是创造一个新的指令，这个指令和jump、forward、speed等等系统内置的一些指令一样，拥有\"按下和松开\"的状态。\n\n知道这一点我们再去解读上面的代码，意思便是如果我按下\"CAPSLOCK\"这个键，那么就进行\"+Jumpthrow\"里的操作；如果我松开，那么就执行\"-Jumpthrow\"里的操作。这样是不是就明白了？\n\n那么我们再举一个只能用alias实现的例子来帮助理解：大跳指令(理论上可以有其他方式，但是实战测试只有alias能用，如果有人知道原因欢迎与我交流)\n\n大跳其实是一种按键技巧，由于游戏引擎的特性，在跳跃之前半蹲可以跳得更高。但是也有很多人练习多次依然无法百分百成功。这时候我们就可以通过指令帮助我们达到100%的成功率：\n\n```\nalias \"+Highjump\" \"+duck;+jump\"\nalias \"-Highjump\" \"-duck;-jump\"\nbind \"v\" \"+Highjump\"\n```\n\n这个代码也很好理解：按v触发\"+Highjump\"中的内容：按下蹲和跳；松开v触发\"-Highjump\"中的内容：松开蹲和跳。(提示：大跳虽然好，但是不建议绑在默认的跳跃键上，因为不需要大跳的时候先蹲再跳会导致跳跃失误，比如从边缘掉下去等等)\n\nalias还有一个常用的功能是将一些难记的代码简化，比如重新投掷上一次的投掷物的指令是sv_rethrow_last_grenade，这个太难记了，那么我简化一下：\n\n```\nalias \"rethrow\" \"sv_rethrow_last_grenade\"\n```\n\n这样一来以后，在控制台输入rethrow即可执行sv_rethrow_last_grenade；或者我们也可以绑定\"rethrow\"到一个具体的按键上，都是可行的。当然这个简化的例子并不是很好，我们最常用到的简化代码的例子还是在一键买枪中比较多见。\n\n> ## 练习<sup>2</sup>:\n>\n> 跳投的指令能不能这样写？：\n>\n> ```\n> alias \"+Jumpthrow\" \"+jump\"\n> alias \"-Jumpthrow\" \"-attack;-attack2;-jump\"\n> bind \"CAPSLOCK\" \"+Jumpthrow\"\n> ```\n>\n> 如果不能，你是否能指出这样操作的问题所在？\n\n- 后续内容待补充\n\n### 总结\n\n显然这不是CS:GO中独有的，但是CS:GO把cfg文件开放给玩家，让玩家可以任意调整任何设置的值，甚至可以在设置里写函数，可以说是把cfg的运用发挥到了极致。这也是我关于这款游戏最喜欢的内容。","source":"_posts/CSGO 中的 cfg 系统.md","raw":"---\ntitle: CS:GO中的cfg系统\ndate: 2020-09-28 17:02:08\ntags:\n---\n\n## 导语\n\n本人写文章的时候，**CS:GO**游戏时长已经来到了1800小时。这个游戏不仅仅是竞技的快感让我无法自拔，游戏系统与生俱来的“程序员”风格也是让我无法割舍的（毕竟是v社的游戏）。在遇到CS:GO前，我无法想象一个游戏的**控制台系统**能够强大到让我痴迷。离开了控制台，也就离开了这个游戏的特色。那么谈到控制台，就不得不谈谈**cfg**。本文希望能用较为简单易懂的语言来谈谈CS:GO中的cfg系统，希望能给大家一些帮助。\n\n## 知识补充\n\n### 简单设置\n\n**cfg**其实就是设置(config)的缩写。.cfg也是文件的后缀名，我们在游戏根目录里能找到许多这样的文件。这些文件的作用就是储存我们在游戏里的设置参数。打个比方，我的游戏音量是1.0，那么我就能在.cfg的文件里找到这么一行：\n\n```\nvolume \"1.0\"\n```\n\n前面的volume指的是音量，后面的\"1.0\"是数值，代表音量的值为1.0。这个例子反映了大部分设置的基本格式，基本都是一个**设置项目名**，后面跟着一个**具体的数值**。一对一的关系还是很易懂的。由此，我们了解了调整设置的基本语法为：\n\n```\n[设置项目名] \"[具体数值]\"\n```\n\n\n双引号在这里不是必须的，但有些时候需要，为了规范我们尽量加上(游戏中控制台不需要)。\n\n我们可以将上面这一行代码称为一个指令。\n\n我们再举几个真实文件中的例子：\n\n```\ncl_crosshair_drawoutline \t\t\t\t\t        \"1\"\ncl_crosshair_dynamic_maxdist_splitratio \t\"0.35\"\ncl_crosshair_dynamic_splitalpha_innermod \t\"1\"\ncl_crosshair_dynamic_splitalpha_outermod \t\"0.5\"\ncl_crosshair_dynamic_splitdist \t\t\t\t    \"7\"\ncl_crosshair_friendly_warning \t\t\t\t    \"1\"\ncl_crosshair_outlinethickness \t\t\t\t    \"1.067771\"\ncl_crosshair_sniper_show_normal_inaccuracy\"0\"\ncl_crosshair_sniper_width \t\t\t\t\t      \"1\"\ncl_crosshair_t \t\t\t\t\t\t\t\t            \"0\"\ncl_crosshairalpha \t\t\t\t\t\t\t          \"255.000000\"\ncl_crosshaircolor \t\t\t\t\t\t\t          \"5\"\ncl_crosshaircolor_b\t \t\t\t\t\t\t          \"0.000000\"\ncl_crosshaircolor_g \t\t\t\t\t\t          \"255.000000\"\ncl_crosshaircolor_r \t\t\t\t\t\t          \"255.000000\"\ncl_crosshairdot \t\t\t\t\t\t\t            \"0.000000\"\ncl_crosshairgap \t\t\t\t\t\t\t            \"0.076290\"\ncl_crosshairgap_useweaponvalue \t\t\t\t    \"0\"\ncl_crosshairsize \t\t\t\t\t\t\t            \"2.917408\"\ncl_crosshairstyle \t\t\t\t\t\t\t          \"4.000000\"\ncl_crosshairthickness \t\t\t\t\t\t        \"1.692905\"\ncl_crosshairusealpha \t\t\t\t\t\t          \"1\"\n```\n\n这是控制**游戏准星**相关的代码，由很多行指令组成。crosshair意思是准星，因此我们知道带有crosshair的内容都是和准星有关的，这段代码也是本人在游戏中的所使用的准星设置。我带着大家解读几行：\n\n```\ncl_crosshair_drawoutline \t\t\t\t\t\"1\"\n```\n\n字面意思来看，crosshair_drawoutline显然指的是准星的轮廓，这里后面的数值则应该为1或0，表示有或无轮廓。\n\n```\ncl_crosshairalpha \t\t\t\t\t\t\t\"255.000000\"\n```\n\n字面意思来看，crosshairalpha显然指的是准星的透明度，这里后面的数值我设置的是255，表示透明度为100%。\n\n实际上我们可以发现，cfg中的简单设置内容往往都很易懂，用最少的词表达得很清楚，后面的数值也自然容易理解。这一类的设置我们不再多说，后面会详细介绍几个常用的设置内容。\n\n### cfg中指令的类型\n\n上面提到的**\\[设置项目名\\]+\\[一个具体的数值\\]**只是指令中的一种，cfg中可不止这一种指令。\n\n下面我们介绍的这些指令是**只有名称，没有值**的。这种指令往往是一个动作，比如：清除血迹、重新投掷上一个投掷物、退出服务器等等：\n\n```\nr_cleaardecals;\t\t\t\t\t\t\t\t//清除血迹\nsv_rethrow_last_grenade;\t\t\t\t\t//重新投掷上一个投掷物\ndisconnect;\t\t\t\t\t\t\t\t\t//断开连接\nquit;\t\t\t\t\t\t\t\t\t\t//退出游戏(光速白给)\nretry;\t\t\t\t\t\t\t\t\t\t//重新连接\n```\n\n这些指令和上面介绍的设置指令可以说都是具体的指令，下面我们介绍的一些指令类型，则是属于非具体的指令，我们可以将其理解为一些函数，那么我们开始吧。\n\n### bind的作用\n\ncfg中不只是有简单的设置，按键的绑定也是cfg中不可或缺的。按键绑定的基本代码是：\n\n```\nbind \"[按键]\" \"[指令]\"\n```\n\n我列举一些游戏中基础的绑定代码展示一下：\n\n```\nbind \"w\" \"+forward\"\nbind \"a\" \"+moveleft\"\nbind \"s\" \"+back\"\nbind \"d\" \"+moveright\"\nbind \"b\" \"buymenu\"\nbind \"m\" \"teammenu\"\nbind \"q\" \"lastinv\"\nbind \"r\" \"+reload\"\nbind \"SPACE\" \"+jump\"\nbind \"TAB\" \"+showscores\"\nbind \"SHIFT\" \"+speed\"\nbind \"CTRL\" \"+duck\"\nbind \"MOUSE1\" \"+attack\"\nbind \"MOUSE2\" \"+attack2\"\n```\n\n这些基本是游戏基本按键的绑定了，前四行简单明了，wasd控制前后左右，b键购买菜单(buymenu)，m键选择队伍(teammenu)，q键切回上一个武器(lastinv)，r键换弹，空格(space)跳跃，Tab键看战绩，Shift行走，Ctrl键蹲下，左键(MOUSE1)攻击或者射击，右键(MOUSE2)武器第二操作。这些都是十分易懂的代码行，因此读到这里应该不算困难。\n\n以上所有的代码都是一个按键对应一个指令。那么下面我提出一个问题，如果我想要用一个按键操作两个动作可以实现吗？\n\n答案是肯定的，我们看下面这行：\n\n```\nbind \"f\" \"+lookatweapon;r_cleardecals\"\n```\n\n**f**键本来是检视武器(+lookatweapon)的按钮，这里我们后面加了个分号，又写了一个r_cleardecals，这是什么意思呢？解释一下，后面这个指令是清除血迹和弹孔的指令。这两个指令被双引号括起来，相当于是打了个包，按下f键的时候就会同时执行后面的两个指令。\n\n因此我们将bind指令的用法重新定义一下：\n\n```\nbind \"[按键]\" \"[指令或指令集]\"\n```\n\n指令集里的每个指令之间一定要记得打分号。\n\n又比如我想要在按Shift键的时候同时走路和清除血迹，那么我们应该这样修改代码：\n\n```\nbind \"SHIFT\" \"+speed;r_cleardecals\"\n```\n\n再来一个：\n\n```\nbind \"CAPSLOCK\" \"+jump;-attack;-attack2;-jump\"\n```\n\n这是本人的跳投绑定，这也是最简单的实现跳投的方式。我们可以看到后面的指令集有四个指令。第一个是按下跳跃键，第二第三个是松开攻击键，最后是松开跳跃键，仔细想想是不是很简单呢？\n\n当然，如果你想解绑一个按键，则可以使用unbind指令：\n\n```\nunbind \"[按键]\"\n```\n\n比如我想取消上面绑定的跳投键，则可以这么写：\n\n```\nunbind \"CAPSLOCK\"\n```\n\n### toggle的作用\n\n你是否碰到这种情况：队友全部牺牲，只剩你一人面对残局，你聚精会神，这时候队友却吵吵闹闹，让你完全没法集中注意力——这时的你是否想要一个按钮，让该死的队友赶紧闭嘴？很幸运，我们有一个指令能够做到这一点。\n\n有的人可能会说，这个不是很简单吗，我直接绑定一个按键（比如\"ALT\"键）到静音指令上：\n\n```\nbind \"ALT\" \"voice_scale 0\"\n```\n\n这么做固然能达到效果，但是残局打完以后呢？你保持在了静音队友的状态，想要调回去还得去设置里重新设置，较为繁琐。\n\n这里我向大家介绍一个新的指令——**toggle**，指令用法如下：\n\n```\ntoggle \"[设置项目名] [数值1] [数值2] ...[数值n]\"\n```\n\n这个指令看起来有很多参数，但是用法十分简单。toggle的意思为拨动，这里是switch from的意思。我们大概也能猜到了：toggle就是让指定设置项目的数值在后面的数值1、数值2...中变动。\n\n我们来用toggle实现我们上面的需求：\n\n```\nbind \"ALT\" \"toggle voice_scale 0 1\"\n```\n\n这一行的意思就是，当我按下alt键的时候，voice_scale的值变为0；当我再按一次，值变为1，再按一次，又变为0...这样我们就实现了随时静音和恢复语音的功能。\n\n由我们上面的定义，可知toggle的数值可以不止2个，利用这个我们可以进行一些设置较为细微的调整，比如游戏音量的调整：\n\n```\nbind \"PGUP\" \"toggle volume 0 0.1 0.2 0.3 0.5 0.7 0.9\"\n```\n\n还有准星透明度的调整：\n\n```\nbind \"PGDN\" \"toggle cl_crosshairalpha 0 64 128 192 255\"\n```\n\n这样我们就能够实现一个通过按\"Pgdn\"键逐渐增加准星透明度的效果。\n\n当然，大部分时候我们不会用到这么细的调整，很多时候设置的数值都只需要在0和1之间切换而已。这时候我们有个简单一些的新指令BindToggle：\n\n```\nBindToggle \"[按键]\" \"[指令]\"\n```\n\n我们举个例子，cl_righthand 1指的是人物右手持枪，cl_righthand 0是左手持枪，我们想要用v键切换左右手：\n\n```\nBindToggle \"v\" \"cl_righthand\"\n```\n\n这时候按下v，cl_righthand的值就会在0和1之间切换了。\n\n注意：这个BindToggle后面的指令只能是一个而不能是指令集，也就是说使用这个指令的话你无法用一个键控制几个设置项的变动。\n\n> ## 练习<sup>1</sup>:\n>\n> 如果我要把上面这个切换左右手的这个方法用toggle写，该怎么写呢？如果我要用一个键同时控制cl_righthand和volume这两个值呢？\n\n### 加与减的作用\n\n在步入真正高级的内容之前，我们先回顾一下之前讲的知识点，是否有一些没有提及的细节还未涉及呢？\n\n可能有的人早已有了一个疑问：有些指令中的+和-是什么意思？\n\n回到bind那里，我列举了一些指令：\n\n```\nbind \"w\" \"+forward\"\nbind \"a\" \"+moveleft\"\nbind \"s\" \"+back\"\nbind \"d\" \"+moveright\"\n```\n\n这些指令前面的加号，其实是模拟键盘\"按下\"的这个动作。我们按下w键，一定是在按住的时候人物才会一直向前走，松开以后，这个前进的动作就停止了。\n\n由这个\"+\"的定义，我们也不难猜测\"-\"的意思：\n\n```\nbind \"CAPSLOCK\" \"+jump;-attack;-attack2;-jump\"\n```\n\n我们跳投的时候，往往是先拉引信，对准参照物，然后按下跳投键，这里面的\"-attack;-attack2\"就是模拟松开攻击键的动作；后面的\"-jump\"则是模拟松开跳跃键的动作。\n\n为什么在这里介绍加与减的概念？因为下一个指令中会涉及加与减的运用，也是十分有意思的内容。\n\n### alias的作用\n\ncfg中还有一种高级的操作，alias，这在普通的cfg中几乎用不到，但是如果你想要拥有一个高端的cfg，这个指令就十分地重要了。\n\nalias基本语法：\n\n```\nalias \"名称\" \"指令或指令集\"\n```\n\n这看上去有点像函数，只不过没有传递参数。它的功能就是打包一个指令集。\n\n没有听明白？我们举个例子来解释就行了：\n\n```\nalias \"Jumpthrow\" \"+jump;-attack;-attack2;-jump\"\nbind \"CAPSLOCK\" \"Jumpthrow\"\n```\n\n这段代码也是跳投的代码，和上面只用bind指令的效果是一样的，或者说是等价的。这段代码先将跳投的一系列指令放到一起，命名为\"Jumpthrow\"，然后将\"Jumpthrow\"绑定在CAPSLOCK键上。\n\n理解了上面的代码，我们来介绍灵性的部分：加与减要来了！\n\n上一栏提到的加与减，是模拟按下与松开的操作。那么alias定义的名称，是不是也能加与减呢？我们看下面的代码：\n\n```\nalias \"+Jumpthrow\" \"+jump;-attack;-attack2\"\nalias \"-Jumpthrow\" \"-jump\"\nbind \"CAPSLOCK\" \"+Jumpthrow\"\n```\n\n如果我说这几行代码和之前的跳投是等效的，你能够理解其中加和减的含义吗？\n\n其实，alias的真正内涵在这里有了体现：alias其实就是创造一个新的指令，这个指令和jump、forward、speed等等系统内置的一些指令一样，拥有\"按下和松开\"的状态。\n\n知道这一点我们再去解读上面的代码，意思便是如果我按下\"CAPSLOCK\"这个键，那么就进行\"+Jumpthrow\"里的操作；如果我松开，那么就执行\"-Jumpthrow\"里的操作。这样是不是就明白了？\n\n那么我们再举一个只能用alias实现的例子来帮助理解：大跳指令(理论上可以有其他方式，但是实战测试只有alias能用，如果有人知道原因欢迎与我交流)\n\n大跳其实是一种按键技巧，由于游戏引擎的特性，在跳跃之前半蹲可以跳得更高。但是也有很多人练习多次依然无法百分百成功。这时候我们就可以通过指令帮助我们达到100%的成功率：\n\n```\nalias \"+Highjump\" \"+duck;+jump\"\nalias \"-Highjump\" \"-duck;-jump\"\nbind \"v\" \"+Highjump\"\n```\n\n这个代码也很好理解：按v触发\"+Highjump\"中的内容：按下蹲和跳；松开v触发\"-Highjump\"中的内容：松开蹲和跳。(提示：大跳虽然好，但是不建议绑在默认的跳跃键上，因为不需要大跳的时候先蹲再跳会导致跳跃失误，比如从边缘掉下去等等)\n\nalias还有一个常用的功能是将一些难记的代码简化，比如重新投掷上一次的投掷物的指令是sv_rethrow_last_grenade，这个太难记了，那么我简化一下：\n\n```\nalias \"rethrow\" \"sv_rethrow_last_grenade\"\n```\n\n这样一来以后，在控制台输入rethrow即可执行sv_rethrow_last_grenade；或者我们也可以绑定\"rethrow\"到一个具体的按键上，都是可行的。当然这个简化的例子并不是很好，我们最常用到的简化代码的例子还是在一键买枪中比较多见。\n\n> ## 练习<sup>2</sup>:\n>\n> 跳投的指令能不能这样写？：\n>\n> ```\n> alias \"+Jumpthrow\" \"+jump\"\n> alias \"-Jumpthrow\" \"-attack;-attack2;-jump\"\n> bind \"CAPSLOCK\" \"+Jumpthrow\"\n> ```\n>\n> 如果不能，你是否能指出这样操作的问题所在？\n\n- 后续内容待补充\n\n### 总结\n\n显然这不是CS:GO中独有的，但是CS:GO把cfg文件开放给玩家，让玩家可以任意调整任何设置的值，甚至可以在设置里写函数，可以说是把cfg的运用发挥到了极致。这也是我关于这款游戏最喜欢的内容。","slug":"CSGO 中的 cfg 系统","published":1,"updated":"2022-08-25T13:13:32.983Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clgyiarts0005jku7ci7ib58r","content":"<h2 id=\"导语\"><a href=\"#导语\" class=\"headerlink\" title=\"导语\"></a>导语</h2><p>本人写文章的时候，<strong>CS:GO</strong>游戏时长已经来到了1800小时。这个游戏不仅仅是竞技的快感让我无法自拔，游戏系统与生俱来的“程序员”风格也是让我无法割舍的（毕竟是v社的游戏）。在遇到CS:GO前，我无法想象一个游戏的<strong>控制台系统</strong>能够强大到让我痴迷。离开了控制台，也就离开了这个游戏的特色。那么谈到控制台，就不得不谈谈<strong>cfg</strong>。本文希望能用较为简单易懂的语言来谈谈CS:GO中的cfg系统，希望能给大家一些帮助。</p>\n<h2 id=\"知识补充\"><a href=\"#知识补充\" class=\"headerlink\" title=\"知识补充\"></a>知识补充</h2><h3 id=\"简单设置\"><a href=\"#简单设置\" class=\"headerlink\" title=\"简单设置\"></a>简单设置</h3><p><strong>cfg</strong>其实就是设置(config)的缩写。.cfg也是文件的后缀名，我们在游戏根目录里能找到许多这样的文件。这些文件的作用就是储存我们在游戏里的设置参数。打个比方，我的游戏音量是1.0，那么我就能在.cfg的文件里找到这么一行：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">volume &quot;1.0&quot;</span><br></pre></td></tr></table></figure>\n<p>前面的volume指的是音量，后面的”1.0”是数值，代表音量的值为1.0。这个例子反映了大部分设置的基本格式，基本都是一个<strong>设置项目名</strong>，后面跟着一个<strong>具体的数值</strong>。一对一的关系还是很易懂的。由此，我们了解了调整设置的基本语法为：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[设置项目名] &quot;[具体数值]&quot;</span><br></pre></td></tr></table></figure>\n<p>双引号在这里不是必须的，但有些时候需要，为了规范我们尽量加上(游戏中控制台不需要)。</p>\n<p>我们可以将上面这一行代码称为一个指令。</p>\n<p>我们再举几个真实文件中的例子：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cl_crosshair_drawoutline \t\t\t\t\t        &quot;1&quot;</span><br><span class=\"line\">cl_crosshair_dynamic_maxdist_splitratio \t&quot;0.35&quot;</span><br><span class=\"line\">cl_crosshair_dynamic_splitalpha_innermod \t&quot;1&quot;</span><br><span class=\"line\">cl_crosshair_dynamic_splitalpha_outermod \t&quot;0.5&quot;</span><br><span class=\"line\">cl_crosshair_dynamic_splitdist \t\t\t\t    &quot;7&quot;</span><br><span class=\"line\">cl_crosshair_friendly_warning \t\t\t\t    &quot;1&quot;</span><br><span class=\"line\">cl_crosshair_outlinethickness \t\t\t\t    &quot;1.067771&quot;</span><br><span class=\"line\">cl_crosshair_sniper_show_normal_inaccuracy&quot;0&quot;</span><br><span class=\"line\">cl_crosshair_sniper_width \t\t\t\t\t      &quot;1&quot;</span><br><span class=\"line\">cl_crosshair_t \t\t\t\t\t\t\t\t            &quot;0&quot;</span><br><span class=\"line\">cl_crosshairalpha \t\t\t\t\t\t\t          &quot;255.000000&quot;</span><br><span class=\"line\">cl_crosshaircolor \t\t\t\t\t\t\t          &quot;5&quot;</span><br><span class=\"line\">cl_crosshaircolor_b\t \t\t\t\t\t\t          &quot;0.000000&quot;</span><br><span class=\"line\">cl_crosshaircolor_g \t\t\t\t\t\t          &quot;255.000000&quot;</span><br><span class=\"line\">cl_crosshaircolor_r \t\t\t\t\t\t          &quot;255.000000&quot;</span><br><span class=\"line\">cl_crosshairdot \t\t\t\t\t\t\t            &quot;0.000000&quot;</span><br><span class=\"line\">cl_crosshairgap \t\t\t\t\t\t\t            &quot;0.076290&quot;</span><br><span class=\"line\">cl_crosshairgap_useweaponvalue \t\t\t\t    &quot;0&quot;</span><br><span class=\"line\">cl_crosshairsize \t\t\t\t\t\t\t            &quot;2.917408&quot;</span><br><span class=\"line\">cl_crosshairstyle \t\t\t\t\t\t\t          &quot;4.000000&quot;</span><br><span class=\"line\">cl_crosshairthickness \t\t\t\t\t\t        &quot;1.692905&quot;</span><br><span class=\"line\">cl_crosshairusealpha \t\t\t\t\t\t          &quot;1&quot;</span><br></pre></td></tr></table></figure>\n<p>这是控制<strong>游戏准星</strong>相关的代码，由很多行指令组成。crosshair意思是准星，因此我们知道带有crosshair的内容都是和准星有关的，这段代码也是本人在游戏中的所使用的准星设置。我带着大家解读几行：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cl_crosshair_drawoutline \t\t\t\t\t&quot;1&quot;</span><br></pre></td></tr></table></figure>\n<p>字面意思来看，crosshair_drawoutline显然指的是准星的轮廓，这里后面的数值则应该为1或0，表示有或无轮廓。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cl_crosshairalpha \t\t\t\t\t\t\t&quot;255.000000&quot;</span><br></pre></td></tr></table></figure>\n<p>字面意思来看，crosshairalpha显然指的是准星的透明度，这里后面的数值我设置的是255，表示透明度为100%。</p>\n<p>实际上我们可以发现，cfg中的简单设置内容往往都很易懂，用最少的词表达得很清楚，后面的数值也自然容易理解。这一类的设置我们不再多说，后面会详细介绍几个常用的设置内容。</p>\n<h3 id=\"cfg中指令的类型\"><a href=\"#cfg中指令的类型\" class=\"headerlink\" title=\"cfg中指令的类型\"></a>cfg中指令的类型</h3><p>上面提到的<strong>[设置项目名]+[一个具体的数值]</strong>只是指令中的一种，cfg中可不止这一种指令。</p>\n<p>下面我们介绍的这些指令是<strong>只有名称，没有值</strong>的。这种指令往往是一个动作，比如：清除血迹、重新投掷上一个投掷物、退出服务器等等：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">r_cleaardecals;\t\t\t\t\t\t\t\t//清除血迹</span><br><span class=\"line\">sv_rethrow_last_grenade;\t\t\t\t\t//重新投掷上一个投掷物</span><br><span class=\"line\">disconnect;\t\t\t\t\t\t\t\t\t//断开连接</span><br><span class=\"line\">quit;\t\t\t\t\t\t\t\t\t\t//退出游戏(光速白给)</span><br><span class=\"line\">retry;\t\t\t\t\t\t\t\t\t\t//重新连接</span><br></pre></td></tr></table></figure>\n<p>这些指令和上面介绍的设置指令可以说都是具体的指令，下面我们介绍的一些指令类型，则是属于非具体的指令，我们可以将其理解为一些函数，那么我们开始吧。</p>\n<h3 id=\"bind的作用\"><a href=\"#bind的作用\" class=\"headerlink\" title=\"bind的作用\"></a>bind的作用</h3><p>cfg中不只是有简单的设置，按键的绑定也是cfg中不可或缺的。按键绑定的基本代码是：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bind &quot;[按键]&quot; &quot;[指令]&quot;</span><br></pre></td></tr></table></figure>\n<p>我列举一些游戏中基础的绑定代码展示一下：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bind &quot;w&quot; &quot;+forward&quot;</span><br><span class=\"line\">bind &quot;a&quot; &quot;+moveleft&quot;</span><br><span class=\"line\">bind &quot;s&quot; &quot;+back&quot;</span><br><span class=\"line\">bind &quot;d&quot; &quot;+moveright&quot;</span><br><span class=\"line\">bind &quot;b&quot; &quot;buymenu&quot;</span><br><span class=\"line\">bind &quot;m&quot; &quot;teammenu&quot;</span><br><span class=\"line\">bind &quot;q&quot; &quot;lastinv&quot;</span><br><span class=\"line\">bind &quot;r&quot; &quot;+reload&quot;</span><br><span class=\"line\">bind &quot;SPACE&quot; &quot;+jump&quot;</span><br><span class=\"line\">bind &quot;TAB&quot; &quot;+showscores&quot;</span><br><span class=\"line\">bind &quot;SHIFT&quot; &quot;+speed&quot;</span><br><span class=\"line\">bind &quot;CTRL&quot; &quot;+duck&quot;</span><br><span class=\"line\">bind &quot;MOUSE1&quot; &quot;+attack&quot;</span><br><span class=\"line\">bind &quot;MOUSE2&quot; &quot;+attack2&quot;</span><br></pre></td></tr></table></figure>\n<p>这些基本是游戏基本按键的绑定了，前四行简单明了，wasd控制前后左右，b键购买菜单(buymenu)，m键选择队伍(teammenu)，q键切回上一个武器(lastinv)，r键换弹，空格(space)跳跃，Tab键看战绩，Shift行走，Ctrl键蹲下，左键(MOUSE1)攻击或者射击，右键(MOUSE2)武器第二操作。这些都是十分易懂的代码行，因此读到这里应该不算困难。</p>\n<p>以上所有的代码都是一个按键对应一个指令。那么下面我提出一个问题，如果我想要用一个按键操作两个动作可以实现吗？</p>\n<p>答案是肯定的，我们看下面这行：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bind &quot;f&quot; &quot;+lookatweapon;r_cleardecals&quot;</span><br></pre></td></tr></table></figure>\n<p><strong>f</strong>键本来是检视武器(+lookatweapon)的按钮，这里我们后面加了个分号，又写了一个r_cleardecals，这是什么意思呢？解释一下，后面这个指令是清除血迹和弹孔的指令。这两个指令被双引号括起来，相当于是打了个包，按下f键的时候就会同时执行后面的两个指令。</p>\n<p>因此我们将bind指令的用法重新定义一下：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bind &quot;[按键]&quot; &quot;[指令或指令集]&quot;</span><br></pre></td></tr></table></figure>\n<p>指令集里的每个指令之间一定要记得打分号。</p>\n<p>又比如我想要在按Shift键的时候同时走路和清除血迹，那么我们应该这样修改代码：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bind &quot;SHIFT&quot; &quot;+speed;r_cleardecals&quot;</span><br></pre></td></tr></table></figure>\n<p>再来一个：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bind &quot;CAPSLOCK&quot; &quot;+jump;-attack;-attack2;-jump&quot;</span><br></pre></td></tr></table></figure>\n<p>这是本人的跳投绑定，这也是最简单的实现跳投的方式。我们可以看到后面的指令集有四个指令。第一个是按下跳跃键，第二第三个是松开攻击键，最后是松开跳跃键，仔细想想是不是很简单呢？</p>\n<p>当然，如果你想解绑一个按键，则可以使用unbind指令：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">unbind &quot;[按键]&quot;</span><br></pre></td></tr></table></figure>\n<p>比如我想取消上面绑定的跳投键，则可以这么写：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">unbind &quot;CAPSLOCK&quot;</span><br></pre></td></tr></table></figure>\n<h3 id=\"toggle的作用\"><a href=\"#toggle的作用\" class=\"headerlink\" title=\"toggle的作用\"></a>toggle的作用</h3><p>你是否碰到这种情况：队友全部牺牲，只剩你一人面对残局，你聚精会神，这时候队友却吵吵闹闹，让你完全没法集中注意力——这时的你是否想要一个按钮，让该死的队友赶紧闭嘴？很幸运，我们有一个指令能够做到这一点。</p>\n<p>有的人可能会说，这个不是很简单吗，我直接绑定一个按键（比如”ALT”键）到静音指令上：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bind &quot;ALT&quot; &quot;voice_scale 0&quot;</span><br></pre></td></tr></table></figure>\n<p>这么做固然能达到效果，但是残局打完以后呢？你保持在了静音队友的状态，想要调回去还得去设置里重新设置，较为繁琐。</p>\n<p>这里我向大家介绍一个新的指令——<strong>toggle</strong>，指令用法如下：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">toggle &quot;[设置项目名] [数值1] [数值2] ...[数值n]&quot;</span><br></pre></td></tr></table></figure>\n<p>这个指令看起来有很多参数，但是用法十分简单。toggle的意思为拨动，这里是switch from的意思。我们大概也能猜到了：toggle就是让指定设置项目的数值在后面的数值1、数值2…中变动。</p>\n<p>我们来用toggle实现我们上面的需求：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bind &quot;ALT&quot; &quot;toggle voice_scale 0 1&quot;</span><br></pre></td></tr></table></figure>\n<p>这一行的意思就是，当我按下alt键的时候，voice_scale的值变为0；当我再按一次，值变为1，再按一次，又变为0…这样我们就实现了随时静音和恢复语音的功能。</p>\n<p>由我们上面的定义，可知toggle的数值可以不止2个，利用这个我们可以进行一些设置较为细微的调整，比如游戏音量的调整：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bind &quot;PGUP&quot; &quot;toggle volume 0 0.1 0.2 0.3 0.5 0.7 0.9&quot;</span><br></pre></td></tr></table></figure>\n<p>还有准星透明度的调整：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bind &quot;PGDN&quot; &quot;toggle cl_crosshairalpha 0 64 128 192 255&quot;</span><br></pre></td></tr></table></figure>\n<p>这样我们就能够实现一个通过按”Pgdn”键逐渐增加准星透明度的效果。</p>\n<p>当然，大部分时候我们不会用到这么细的调整，很多时候设置的数值都只需要在0和1之间切换而已。这时候我们有个简单一些的新指令BindToggle：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">BindToggle &quot;[按键]&quot; &quot;[指令]&quot;</span><br></pre></td></tr></table></figure>\n<p>我们举个例子，cl_righthand 1指的是人物右手持枪，cl_righthand 0是左手持枪，我们想要用v键切换左右手：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">BindToggle &quot;v&quot; &quot;cl_righthand&quot;</span><br></pre></td></tr></table></figure>\n<p>这时候按下v，cl_righthand的值就会在0和1之间切换了。</p>\n<p>注意：这个BindToggle后面的指令只能是一个而不能是指令集，也就是说使用这个指令的话你无法用一个键控制几个设置项的变动。</p>\n<blockquote>\n<h2 id=\"练习1\"><a href=\"#练习1\" class=\"headerlink\" title=\"练习1:\"></a>练习<sup>1</sup>:</h2><p>如果我要把上面这个切换左右手的这个方法用toggle写，该怎么写呢？如果我要用一个键同时控制cl_righthand和volume这两个值呢？</p>\n</blockquote>\n<h3 id=\"加与减的作用\"><a href=\"#加与减的作用\" class=\"headerlink\" title=\"加与减的作用\"></a>加与减的作用</h3><p>在步入真正高级的内容之前，我们先回顾一下之前讲的知识点，是否有一些没有提及的细节还未涉及呢？</p>\n<p>可能有的人早已有了一个疑问：有些指令中的+和-是什么意思？</p>\n<p>回到bind那里，我列举了一些指令：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bind &quot;w&quot; &quot;+forward&quot;</span><br><span class=\"line\">bind &quot;a&quot; &quot;+moveleft&quot;</span><br><span class=\"line\">bind &quot;s&quot; &quot;+back&quot;</span><br><span class=\"line\">bind &quot;d&quot; &quot;+moveright&quot;</span><br></pre></td></tr></table></figure>\n<p>这些指令前面的加号，其实是模拟键盘”按下”的这个动作。我们按下w键，一定是在按住的时候人物才会一直向前走，松开以后，这个前进的动作就停止了。</p>\n<p>由这个”+”的定义，我们也不难猜测”-“的意思：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bind &quot;CAPSLOCK&quot; &quot;+jump;-attack;-attack2;-jump&quot;</span><br></pre></td></tr></table></figure>\n<p>我们跳投的时候，往往是先拉引信，对准参照物，然后按下跳投键，这里面的”-attack;-attack2”就是模拟松开攻击键的动作；后面的”-jump”则是模拟松开跳跃键的动作。</p>\n<p>为什么在这里介绍加与减的概念？因为下一个指令中会涉及加与减的运用，也是十分有意思的内容。</p>\n<h3 id=\"alias的作用\"><a href=\"#alias的作用\" class=\"headerlink\" title=\"alias的作用\"></a>alias的作用</h3><p>cfg中还有一种高级的操作，alias，这在普通的cfg中几乎用不到，但是如果你想要拥有一个高端的cfg，这个指令就十分地重要了。</p>\n<p>alias基本语法：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">alias &quot;名称&quot; &quot;指令或指令集&quot;</span><br></pre></td></tr></table></figure>\n<p>这看上去有点像函数，只不过没有传递参数。它的功能就是打包一个指令集。</p>\n<p>没有听明白？我们举个例子来解释就行了：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">alias &quot;Jumpthrow&quot; &quot;+jump;-attack;-attack2;-jump&quot;</span><br><span class=\"line\">bind &quot;CAPSLOCK&quot; &quot;Jumpthrow&quot;</span><br></pre></td></tr></table></figure>\n<p>这段代码也是跳投的代码，和上面只用bind指令的效果是一样的，或者说是等价的。这段代码先将跳投的一系列指令放到一起，命名为”Jumpthrow”，然后将”Jumpthrow”绑定在CAPSLOCK键上。</p>\n<p>理解了上面的代码，我们来介绍灵性的部分：加与减要来了！</p>\n<p>上一栏提到的加与减，是模拟按下与松开的操作。那么alias定义的名称，是不是也能加与减呢？我们看下面的代码：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">alias &quot;+Jumpthrow&quot; &quot;+jump;-attack;-attack2&quot;</span><br><span class=\"line\">alias &quot;-Jumpthrow&quot; &quot;-jump&quot;</span><br><span class=\"line\">bind &quot;CAPSLOCK&quot; &quot;+Jumpthrow&quot;</span><br></pre></td></tr></table></figure>\n<p>如果我说这几行代码和之前的跳投是等效的，你能够理解其中加和减的含义吗？</p>\n<p>其实，alias的真正内涵在这里有了体现：alias其实就是创造一个新的指令，这个指令和jump、forward、speed等等系统内置的一些指令一样，拥有”按下和松开”的状态。</p>\n<p>知道这一点我们再去解读上面的代码，意思便是如果我按下”CAPSLOCK”这个键，那么就进行”+Jumpthrow”里的操作；如果我松开，那么就执行”-Jumpthrow”里的操作。这样是不是就明白了？</p>\n<p>那么我们再举一个只能用alias实现的例子来帮助理解：大跳指令(理论上可以有其他方式，但是实战测试只有alias能用，如果有人知道原因欢迎与我交流)</p>\n<p>大跳其实是一种按键技巧，由于游戏引擎的特性，在跳跃之前半蹲可以跳得更高。但是也有很多人练习多次依然无法百分百成功。这时候我们就可以通过指令帮助我们达到100%的成功率：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">alias &quot;+Highjump&quot; &quot;+duck;+jump&quot;</span><br><span class=\"line\">alias &quot;-Highjump&quot; &quot;-duck;-jump&quot;</span><br><span class=\"line\">bind &quot;v&quot; &quot;+Highjump&quot;</span><br></pre></td></tr></table></figure>\n<p>这个代码也很好理解：按v触发”+Highjump”中的内容：按下蹲和跳；松开v触发”-Highjump”中的内容：松开蹲和跳。(提示：大跳虽然好，但是不建议绑在默认的跳跃键上，因为不需要大跳的时候先蹲再跳会导致跳跃失误，比如从边缘掉下去等等)</p>\n<p>alias还有一个常用的功能是将一些难记的代码简化，比如重新投掷上一次的投掷物的指令是sv_rethrow_last_grenade，这个太难记了，那么我简化一下：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">alias &quot;rethrow&quot; &quot;sv_rethrow_last_grenade&quot;</span><br></pre></td></tr></table></figure>\n<p>这样一来以后，在控制台输入rethrow即可执行sv_rethrow_last_grenade；或者我们也可以绑定”rethrow”到一个具体的按键上，都是可行的。当然这个简化的例子并不是很好，我们最常用到的简化代码的例子还是在一键买枪中比较多见。</p>\n<blockquote>\n<h2 id=\"练习2\"><a href=\"#练习2\" class=\"headerlink\" title=\"练习2:\"></a>练习<sup>2</sup>:</h2><p>跳投的指令能不能这样写？：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">alias &quot;+Jumpthrow&quot; &quot;+jump&quot;</span><br><span class=\"line\">alias &quot;-Jumpthrow&quot; &quot;-attack;-attack2;-jump&quot;</span><br><span class=\"line\">bind &quot;CAPSLOCK&quot; &quot;+Jumpthrow&quot;</span><br></pre></td></tr></table></figure>\n<p>如果不能，你是否能指出这样操作的问题所在？</p>\n</blockquote>\n<ul>\n<li>后续内容待补充</li>\n</ul>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>显然这不是CS:GO中独有的，但是CS:GO把cfg文件开放给玩家，让玩家可以任意调整任何设置的值，甚至可以在设置里写函数，可以说是把cfg的运用发挥到了极致。这也是我关于这款游戏最喜欢的内容。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"导语\"><a href=\"#导语\" class=\"headerlink\" title=\"导语\"></a>导语</h2><p>本人写文章的时候，<strong>CS:GO</strong>游戏时长已经来到了1800小时。这个游戏不仅仅是竞技的快感让我无法自拔，游戏系统与生俱来的“程序员”风格也是让我无法割舍的（毕竟是v社的游戏）。在遇到CS:GO前，我无法想象一个游戏的<strong>控制台系统</strong>能够强大到让我痴迷。离开了控制台，也就离开了这个游戏的特色。那么谈到控制台，就不得不谈谈<strong>cfg</strong>。本文希望能用较为简单易懂的语言来谈谈CS:GO中的cfg系统，希望能给大家一些帮助。</p>\n<h2 id=\"知识补充\"><a href=\"#知识补充\" class=\"headerlink\" title=\"知识补充\"></a>知识补充</h2><h3 id=\"简单设置\"><a href=\"#简单设置\" class=\"headerlink\" title=\"简单设置\"></a>简单设置</h3><p><strong>cfg</strong>其实就是设置(config)的缩写。.cfg也是文件的后缀名，我们在游戏根目录里能找到许多这样的文件。这些文件的作用就是储存我们在游戏里的设置参数。打个比方，我的游戏音量是1.0，那么我就能在.cfg的文件里找到这么一行：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">volume &quot;1.0&quot;</span><br></pre></td></tr></table></figure>\n<p>前面的volume指的是音量，后面的”1.0”是数值，代表音量的值为1.0。这个例子反映了大部分设置的基本格式，基本都是一个<strong>设置项目名</strong>，后面跟着一个<strong>具体的数值</strong>。一对一的关系还是很易懂的。由此，我们了解了调整设置的基本语法为：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[设置项目名] &quot;[具体数值]&quot;</span><br></pre></td></tr></table></figure>\n<p>双引号在这里不是必须的，但有些时候需要，为了规范我们尽量加上(游戏中控制台不需要)。</p>\n<p>我们可以将上面这一行代码称为一个指令。</p>\n<p>我们再举几个真实文件中的例子：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cl_crosshair_drawoutline \t\t\t\t\t        &quot;1&quot;</span><br><span class=\"line\">cl_crosshair_dynamic_maxdist_splitratio \t&quot;0.35&quot;</span><br><span class=\"line\">cl_crosshair_dynamic_splitalpha_innermod \t&quot;1&quot;</span><br><span class=\"line\">cl_crosshair_dynamic_splitalpha_outermod \t&quot;0.5&quot;</span><br><span class=\"line\">cl_crosshair_dynamic_splitdist \t\t\t\t    &quot;7&quot;</span><br><span class=\"line\">cl_crosshair_friendly_warning \t\t\t\t    &quot;1&quot;</span><br><span class=\"line\">cl_crosshair_outlinethickness \t\t\t\t    &quot;1.067771&quot;</span><br><span class=\"line\">cl_crosshair_sniper_show_normal_inaccuracy&quot;0&quot;</span><br><span class=\"line\">cl_crosshair_sniper_width \t\t\t\t\t      &quot;1&quot;</span><br><span class=\"line\">cl_crosshair_t \t\t\t\t\t\t\t\t            &quot;0&quot;</span><br><span class=\"line\">cl_crosshairalpha \t\t\t\t\t\t\t          &quot;255.000000&quot;</span><br><span class=\"line\">cl_crosshaircolor \t\t\t\t\t\t\t          &quot;5&quot;</span><br><span class=\"line\">cl_crosshaircolor_b\t \t\t\t\t\t\t          &quot;0.000000&quot;</span><br><span class=\"line\">cl_crosshaircolor_g \t\t\t\t\t\t          &quot;255.000000&quot;</span><br><span class=\"line\">cl_crosshaircolor_r \t\t\t\t\t\t          &quot;255.000000&quot;</span><br><span class=\"line\">cl_crosshairdot \t\t\t\t\t\t\t            &quot;0.000000&quot;</span><br><span class=\"line\">cl_crosshairgap \t\t\t\t\t\t\t            &quot;0.076290&quot;</span><br><span class=\"line\">cl_crosshairgap_useweaponvalue \t\t\t\t    &quot;0&quot;</span><br><span class=\"line\">cl_crosshairsize \t\t\t\t\t\t\t            &quot;2.917408&quot;</span><br><span class=\"line\">cl_crosshairstyle \t\t\t\t\t\t\t          &quot;4.000000&quot;</span><br><span class=\"line\">cl_crosshairthickness \t\t\t\t\t\t        &quot;1.692905&quot;</span><br><span class=\"line\">cl_crosshairusealpha \t\t\t\t\t\t          &quot;1&quot;</span><br></pre></td></tr></table></figure>\n<p>这是控制<strong>游戏准星</strong>相关的代码，由很多行指令组成。crosshair意思是准星，因此我们知道带有crosshair的内容都是和准星有关的，这段代码也是本人在游戏中的所使用的准星设置。我带着大家解读几行：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cl_crosshair_drawoutline \t\t\t\t\t&quot;1&quot;</span><br></pre></td></tr></table></figure>\n<p>字面意思来看，crosshair_drawoutline显然指的是准星的轮廓，这里后面的数值则应该为1或0，表示有或无轮廓。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cl_crosshairalpha \t\t\t\t\t\t\t&quot;255.000000&quot;</span><br></pre></td></tr></table></figure>\n<p>字面意思来看，crosshairalpha显然指的是准星的透明度，这里后面的数值我设置的是255，表示透明度为100%。</p>\n<p>实际上我们可以发现，cfg中的简单设置内容往往都很易懂，用最少的词表达得很清楚，后面的数值也自然容易理解。这一类的设置我们不再多说，后面会详细介绍几个常用的设置内容。</p>\n<h3 id=\"cfg中指令的类型\"><a href=\"#cfg中指令的类型\" class=\"headerlink\" title=\"cfg中指令的类型\"></a>cfg中指令的类型</h3><p>上面提到的<strong>[设置项目名]+[一个具体的数值]</strong>只是指令中的一种，cfg中可不止这一种指令。</p>\n<p>下面我们介绍的这些指令是<strong>只有名称，没有值</strong>的。这种指令往往是一个动作，比如：清除血迹、重新投掷上一个投掷物、退出服务器等等：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">r_cleaardecals;\t\t\t\t\t\t\t\t//清除血迹</span><br><span class=\"line\">sv_rethrow_last_grenade;\t\t\t\t\t//重新投掷上一个投掷物</span><br><span class=\"line\">disconnect;\t\t\t\t\t\t\t\t\t//断开连接</span><br><span class=\"line\">quit;\t\t\t\t\t\t\t\t\t\t//退出游戏(光速白给)</span><br><span class=\"line\">retry;\t\t\t\t\t\t\t\t\t\t//重新连接</span><br></pre></td></tr></table></figure>\n<p>这些指令和上面介绍的设置指令可以说都是具体的指令，下面我们介绍的一些指令类型，则是属于非具体的指令，我们可以将其理解为一些函数，那么我们开始吧。</p>\n<h3 id=\"bind的作用\"><a href=\"#bind的作用\" class=\"headerlink\" title=\"bind的作用\"></a>bind的作用</h3><p>cfg中不只是有简单的设置，按键的绑定也是cfg中不可或缺的。按键绑定的基本代码是：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bind &quot;[按键]&quot; &quot;[指令]&quot;</span><br></pre></td></tr></table></figure>\n<p>我列举一些游戏中基础的绑定代码展示一下：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bind &quot;w&quot; &quot;+forward&quot;</span><br><span class=\"line\">bind &quot;a&quot; &quot;+moveleft&quot;</span><br><span class=\"line\">bind &quot;s&quot; &quot;+back&quot;</span><br><span class=\"line\">bind &quot;d&quot; &quot;+moveright&quot;</span><br><span class=\"line\">bind &quot;b&quot; &quot;buymenu&quot;</span><br><span class=\"line\">bind &quot;m&quot; &quot;teammenu&quot;</span><br><span class=\"line\">bind &quot;q&quot; &quot;lastinv&quot;</span><br><span class=\"line\">bind &quot;r&quot; &quot;+reload&quot;</span><br><span class=\"line\">bind &quot;SPACE&quot; &quot;+jump&quot;</span><br><span class=\"line\">bind &quot;TAB&quot; &quot;+showscores&quot;</span><br><span class=\"line\">bind &quot;SHIFT&quot; &quot;+speed&quot;</span><br><span class=\"line\">bind &quot;CTRL&quot; &quot;+duck&quot;</span><br><span class=\"line\">bind &quot;MOUSE1&quot; &quot;+attack&quot;</span><br><span class=\"line\">bind &quot;MOUSE2&quot; &quot;+attack2&quot;</span><br></pre></td></tr></table></figure>\n<p>这些基本是游戏基本按键的绑定了，前四行简单明了，wasd控制前后左右，b键购买菜单(buymenu)，m键选择队伍(teammenu)，q键切回上一个武器(lastinv)，r键换弹，空格(space)跳跃，Tab键看战绩，Shift行走，Ctrl键蹲下，左键(MOUSE1)攻击或者射击，右键(MOUSE2)武器第二操作。这些都是十分易懂的代码行，因此读到这里应该不算困难。</p>\n<p>以上所有的代码都是一个按键对应一个指令。那么下面我提出一个问题，如果我想要用一个按键操作两个动作可以实现吗？</p>\n<p>答案是肯定的，我们看下面这行：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bind &quot;f&quot; &quot;+lookatweapon;r_cleardecals&quot;</span><br></pre></td></tr></table></figure>\n<p><strong>f</strong>键本来是检视武器(+lookatweapon)的按钮，这里我们后面加了个分号，又写了一个r_cleardecals，这是什么意思呢？解释一下，后面这个指令是清除血迹和弹孔的指令。这两个指令被双引号括起来，相当于是打了个包，按下f键的时候就会同时执行后面的两个指令。</p>\n<p>因此我们将bind指令的用法重新定义一下：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bind &quot;[按键]&quot; &quot;[指令或指令集]&quot;</span><br></pre></td></tr></table></figure>\n<p>指令集里的每个指令之间一定要记得打分号。</p>\n<p>又比如我想要在按Shift键的时候同时走路和清除血迹，那么我们应该这样修改代码：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bind &quot;SHIFT&quot; &quot;+speed;r_cleardecals&quot;</span><br></pre></td></tr></table></figure>\n<p>再来一个：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bind &quot;CAPSLOCK&quot; &quot;+jump;-attack;-attack2;-jump&quot;</span><br></pre></td></tr></table></figure>\n<p>这是本人的跳投绑定，这也是最简单的实现跳投的方式。我们可以看到后面的指令集有四个指令。第一个是按下跳跃键，第二第三个是松开攻击键，最后是松开跳跃键，仔细想想是不是很简单呢？</p>\n<p>当然，如果你想解绑一个按键，则可以使用unbind指令：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">unbind &quot;[按键]&quot;</span><br></pre></td></tr></table></figure>\n<p>比如我想取消上面绑定的跳投键，则可以这么写：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">unbind &quot;CAPSLOCK&quot;</span><br></pre></td></tr></table></figure>\n<h3 id=\"toggle的作用\"><a href=\"#toggle的作用\" class=\"headerlink\" title=\"toggle的作用\"></a>toggle的作用</h3><p>你是否碰到这种情况：队友全部牺牲，只剩你一人面对残局，你聚精会神，这时候队友却吵吵闹闹，让你完全没法集中注意力——这时的你是否想要一个按钮，让该死的队友赶紧闭嘴？很幸运，我们有一个指令能够做到这一点。</p>\n<p>有的人可能会说，这个不是很简单吗，我直接绑定一个按键（比如”ALT”键）到静音指令上：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bind &quot;ALT&quot; &quot;voice_scale 0&quot;</span><br></pre></td></tr></table></figure>\n<p>这么做固然能达到效果，但是残局打完以后呢？你保持在了静音队友的状态，想要调回去还得去设置里重新设置，较为繁琐。</p>\n<p>这里我向大家介绍一个新的指令——<strong>toggle</strong>，指令用法如下：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">toggle &quot;[设置项目名] [数值1] [数值2] ...[数值n]&quot;</span><br></pre></td></tr></table></figure>\n<p>这个指令看起来有很多参数，但是用法十分简单。toggle的意思为拨动，这里是switch from的意思。我们大概也能猜到了：toggle就是让指定设置项目的数值在后面的数值1、数值2…中变动。</p>\n<p>我们来用toggle实现我们上面的需求：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bind &quot;ALT&quot; &quot;toggle voice_scale 0 1&quot;</span><br></pre></td></tr></table></figure>\n<p>这一行的意思就是，当我按下alt键的时候，voice_scale的值变为0；当我再按一次，值变为1，再按一次，又变为0…这样我们就实现了随时静音和恢复语音的功能。</p>\n<p>由我们上面的定义，可知toggle的数值可以不止2个，利用这个我们可以进行一些设置较为细微的调整，比如游戏音量的调整：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bind &quot;PGUP&quot; &quot;toggle volume 0 0.1 0.2 0.3 0.5 0.7 0.9&quot;</span><br></pre></td></tr></table></figure>\n<p>还有准星透明度的调整：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bind &quot;PGDN&quot; &quot;toggle cl_crosshairalpha 0 64 128 192 255&quot;</span><br></pre></td></tr></table></figure>\n<p>这样我们就能够实现一个通过按”Pgdn”键逐渐增加准星透明度的效果。</p>\n<p>当然，大部分时候我们不会用到这么细的调整，很多时候设置的数值都只需要在0和1之间切换而已。这时候我们有个简单一些的新指令BindToggle：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">BindToggle &quot;[按键]&quot; &quot;[指令]&quot;</span><br></pre></td></tr></table></figure>\n<p>我们举个例子，cl_righthand 1指的是人物右手持枪，cl_righthand 0是左手持枪，我们想要用v键切换左右手：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">BindToggle &quot;v&quot; &quot;cl_righthand&quot;</span><br></pre></td></tr></table></figure>\n<p>这时候按下v，cl_righthand的值就会在0和1之间切换了。</p>\n<p>注意：这个BindToggle后面的指令只能是一个而不能是指令集，也就是说使用这个指令的话你无法用一个键控制几个设置项的变动。</p>\n<blockquote>\n<h2 id=\"练习1\"><a href=\"#练习1\" class=\"headerlink\" title=\"练习1:\"></a>练习<sup>1</sup>:</h2><p>如果我要把上面这个切换左右手的这个方法用toggle写，该怎么写呢？如果我要用一个键同时控制cl_righthand和volume这两个值呢？</p>\n</blockquote>\n<h3 id=\"加与减的作用\"><a href=\"#加与减的作用\" class=\"headerlink\" title=\"加与减的作用\"></a>加与减的作用</h3><p>在步入真正高级的内容之前，我们先回顾一下之前讲的知识点，是否有一些没有提及的细节还未涉及呢？</p>\n<p>可能有的人早已有了一个疑问：有些指令中的+和-是什么意思？</p>\n<p>回到bind那里，我列举了一些指令：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bind &quot;w&quot; &quot;+forward&quot;</span><br><span class=\"line\">bind &quot;a&quot; &quot;+moveleft&quot;</span><br><span class=\"line\">bind &quot;s&quot; &quot;+back&quot;</span><br><span class=\"line\">bind &quot;d&quot; &quot;+moveright&quot;</span><br></pre></td></tr></table></figure>\n<p>这些指令前面的加号，其实是模拟键盘”按下”的这个动作。我们按下w键，一定是在按住的时候人物才会一直向前走，松开以后，这个前进的动作就停止了。</p>\n<p>由这个”+”的定义，我们也不难猜测”-“的意思：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bind &quot;CAPSLOCK&quot; &quot;+jump;-attack;-attack2;-jump&quot;</span><br></pre></td></tr></table></figure>\n<p>我们跳投的时候，往往是先拉引信，对准参照物，然后按下跳投键，这里面的”-attack;-attack2”就是模拟松开攻击键的动作；后面的”-jump”则是模拟松开跳跃键的动作。</p>\n<p>为什么在这里介绍加与减的概念？因为下一个指令中会涉及加与减的运用，也是十分有意思的内容。</p>\n<h3 id=\"alias的作用\"><a href=\"#alias的作用\" class=\"headerlink\" title=\"alias的作用\"></a>alias的作用</h3><p>cfg中还有一种高级的操作，alias，这在普通的cfg中几乎用不到，但是如果你想要拥有一个高端的cfg，这个指令就十分地重要了。</p>\n<p>alias基本语法：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">alias &quot;名称&quot; &quot;指令或指令集&quot;</span><br></pre></td></tr></table></figure>\n<p>这看上去有点像函数，只不过没有传递参数。它的功能就是打包一个指令集。</p>\n<p>没有听明白？我们举个例子来解释就行了：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">alias &quot;Jumpthrow&quot; &quot;+jump;-attack;-attack2;-jump&quot;</span><br><span class=\"line\">bind &quot;CAPSLOCK&quot; &quot;Jumpthrow&quot;</span><br></pre></td></tr></table></figure>\n<p>这段代码也是跳投的代码，和上面只用bind指令的效果是一样的，或者说是等价的。这段代码先将跳投的一系列指令放到一起，命名为”Jumpthrow”，然后将”Jumpthrow”绑定在CAPSLOCK键上。</p>\n<p>理解了上面的代码，我们来介绍灵性的部分：加与减要来了！</p>\n<p>上一栏提到的加与减，是模拟按下与松开的操作。那么alias定义的名称，是不是也能加与减呢？我们看下面的代码：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">alias &quot;+Jumpthrow&quot; &quot;+jump;-attack;-attack2&quot;</span><br><span class=\"line\">alias &quot;-Jumpthrow&quot; &quot;-jump&quot;</span><br><span class=\"line\">bind &quot;CAPSLOCK&quot; &quot;+Jumpthrow&quot;</span><br></pre></td></tr></table></figure>\n<p>如果我说这几行代码和之前的跳投是等效的，你能够理解其中加和减的含义吗？</p>\n<p>其实，alias的真正内涵在这里有了体现：alias其实就是创造一个新的指令，这个指令和jump、forward、speed等等系统内置的一些指令一样，拥有”按下和松开”的状态。</p>\n<p>知道这一点我们再去解读上面的代码，意思便是如果我按下”CAPSLOCK”这个键，那么就进行”+Jumpthrow”里的操作；如果我松开，那么就执行”-Jumpthrow”里的操作。这样是不是就明白了？</p>\n<p>那么我们再举一个只能用alias实现的例子来帮助理解：大跳指令(理论上可以有其他方式，但是实战测试只有alias能用，如果有人知道原因欢迎与我交流)</p>\n<p>大跳其实是一种按键技巧，由于游戏引擎的特性，在跳跃之前半蹲可以跳得更高。但是也有很多人练习多次依然无法百分百成功。这时候我们就可以通过指令帮助我们达到100%的成功率：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">alias &quot;+Highjump&quot; &quot;+duck;+jump&quot;</span><br><span class=\"line\">alias &quot;-Highjump&quot; &quot;-duck;-jump&quot;</span><br><span class=\"line\">bind &quot;v&quot; &quot;+Highjump&quot;</span><br></pre></td></tr></table></figure>\n<p>这个代码也很好理解：按v触发”+Highjump”中的内容：按下蹲和跳；松开v触发”-Highjump”中的内容：松开蹲和跳。(提示：大跳虽然好，但是不建议绑在默认的跳跃键上，因为不需要大跳的时候先蹲再跳会导致跳跃失误，比如从边缘掉下去等等)</p>\n<p>alias还有一个常用的功能是将一些难记的代码简化，比如重新投掷上一次的投掷物的指令是sv_rethrow_last_grenade，这个太难记了，那么我简化一下：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">alias &quot;rethrow&quot; &quot;sv_rethrow_last_grenade&quot;</span><br></pre></td></tr></table></figure>\n<p>这样一来以后，在控制台输入rethrow即可执行sv_rethrow_last_grenade；或者我们也可以绑定”rethrow”到一个具体的按键上，都是可行的。当然这个简化的例子并不是很好，我们最常用到的简化代码的例子还是在一键买枪中比较多见。</p>\n<blockquote>\n<h2 id=\"练习2\"><a href=\"#练习2\" class=\"headerlink\" title=\"练习2:\"></a>练习<sup>2</sup>:</h2><p>跳投的指令能不能这样写？：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">alias &quot;+Jumpthrow&quot; &quot;+jump&quot;</span><br><span class=\"line\">alias &quot;-Jumpthrow&quot; &quot;-attack;-attack2;-jump&quot;</span><br><span class=\"line\">bind &quot;CAPSLOCK&quot; &quot;+Jumpthrow&quot;</span><br></pre></td></tr></table></figure>\n<p>如果不能，你是否能指出这样操作的问题所在？</p>\n</blockquote>\n<ul>\n<li>后续内容待补充</li>\n</ul>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>显然这不是CS:GO中独有的，但是CS:GO把cfg文件开放给玩家，让玩家可以任意调整任何设置的值，甚至可以在设置里写函数，可以说是把cfg的运用发挥到了极致。这也是我关于这款游戏最喜欢的内容。</p>\n"},{"title":"Hugging Face 学习(1)","date":"2022-07-12T04:00:06.000Z","_content":"\n# Hugging Face 学习(1)\n\nHugging Face是一个非常流行的 NLP 库，Hugging Face 的 [官网](https://huggingface.co/) 上也有很多开源的模型。\n\n官网上的有些模型已经提供了简单的使用方法。比如我最先接触到的这个 `dalle-mini` 模型，提供了一个生成的接口(虽然好像还是定向到了[其它网站](https://www.craiyon.com)，据说是因为访问量太大了)，可以输入一段文本，生成一些图片。这里随便输入一个句子 A Rubic's Cube swimming in the pool (水池里游泳的魔方)：\n\n![image-20220720084135775](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220720084135775.png)\n\n\n\n关于 `dalle` 这个模型，最近看到一篇文章比较有意思(https://blog.csdn.net/fengdu78/article/details/125109103)\n\n---\n\n如果是在本地使用 Hugging Face 的一些 NLP 模型，可以安装他们提供的 `transformers` 的库。\n\n```shell\npip install transformers\n```\n\n这里使用一个 [t5-small](https://huggingface.co/t5-small) 的模型测试一下。\n\nt5 (Text-To-Text Transfer Transformer) 是一个统一框架，将所有 NLP 任务都转化成 Text-to-Text （文本到文本）任务。\n\n![](https://1.bp.blogspot.com/-o4oiOExxq1s/Xk26XPC3haI/AAAAAAAAFU8/NBlvOWB84L0PTYy9TzZBaLf6fwPGJTR0QCLcBGAsYHQ/s640/image3.gif)\n\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n\ntokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n\nmodel = AutoModelWithLMHead.from_pretrained(\"t5-small\")\n```\n\n只需要三行代码，就可以直接加载模型开始使用：\n\n```python\ninput_ids = tokenizer(\"translate English to Germany: What a nice day.\", return_tensors=\"pt\").input_ids\n\noutputs = model.generate(input_ids)\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\n得到输出：\n\n```\nWas für ein nettes Tag.\n```\n\n","source":"_posts/Hugging Face 学习(1).md","raw":"---\ntitle: Hugging Face 学习(1)\ndate: 2022-07-12 12:00:06\ntags:\n---\n\n# Hugging Face 学习(1)\n\nHugging Face是一个非常流行的 NLP 库，Hugging Face 的 [官网](https://huggingface.co/) 上也有很多开源的模型。\n\n官网上的有些模型已经提供了简单的使用方法。比如我最先接触到的这个 `dalle-mini` 模型，提供了一个生成的接口(虽然好像还是定向到了[其它网站](https://www.craiyon.com)，据说是因为访问量太大了)，可以输入一段文本，生成一些图片。这里随便输入一个句子 A Rubic's Cube swimming in the pool (水池里游泳的魔方)：\n\n![image-20220720084135775](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220720084135775.png)\n\n\n\n关于 `dalle` 这个模型，最近看到一篇文章比较有意思(https://blog.csdn.net/fengdu78/article/details/125109103)\n\n---\n\n如果是在本地使用 Hugging Face 的一些 NLP 模型，可以安装他们提供的 `transformers` 的库。\n\n```shell\npip install transformers\n```\n\n这里使用一个 [t5-small](https://huggingface.co/t5-small) 的模型测试一下。\n\nt5 (Text-To-Text Transfer Transformer) 是一个统一框架，将所有 NLP 任务都转化成 Text-to-Text （文本到文本）任务。\n\n![](https://1.bp.blogspot.com/-o4oiOExxq1s/Xk26XPC3haI/AAAAAAAAFU8/NBlvOWB84L0PTYy9TzZBaLf6fwPGJTR0QCLcBGAsYHQ/s640/image3.gif)\n\n```python\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\n\ntokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n\nmodel = AutoModelWithLMHead.from_pretrained(\"t5-small\")\n```\n\n只需要三行代码，就可以直接加载模型开始使用：\n\n```python\ninput_ids = tokenizer(\"translate English to Germany: What a nice day.\", return_tensors=\"pt\").input_ids\n\noutputs = model.generate(input_ids)\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\n得到输出：\n\n```\nWas für ein nettes Tag.\n```\n\n","slug":"Hugging Face 学习(1)","published":1,"updated":"2022-08-25T13:13:32.984Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clgyiartt0006jku7asodhkb1","content":"<h1 id=\"Hugging-Face-学习-1\"><a href=\"#Hugging-Face-学习-1\" class=\"headerlink\" title=\"Hugging Face 学习(1)\"></a>Hugging Face 学习(1)</h1><p>Hugging Face是一个非常流行的 NLP 库，Hugging Face 的 <a href=\"https://huggingface.co/\">官网</a> 上也有很多开源的模型。</p>\n<p>官网上的有些模型已经提供了简单的使用方法。比如我最先接触到的这个 <code>dalle-mini</code> 模型，提供了一个生成的接口(虽然好像还是定向到了<a href=\"https://www.craiyon.com\">其它网站</a>，据说是因为访问量太大了)，可以输入一段文本，生成一些图片。这里随便输入一个句子 A Rubic’s Cube swimming in the pool (水池里游泳的魔方)：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220720084135775.png\" alt=\"image-20220720084135775\"></p>\n<p>关于 <code>dalle</code> 这个模型，最近看到一篇文章比较有意思(<a href=\"https://blog.csdn.net/fengdu78/article/details/125109103\">https://blog.csdn.net/fengdu78/article/details/125109103</a>)</p>\n<hr>\n<p>如果是在本地使用 Hugging Face 的一些 NLP 模型，可以安装他们提供的 <code>transformers</code> 的库。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install transformers</span><br></pre></td></tr></table></figure>\n<p>这里使用一个 <a href=\"https://huggingface.co/t5-small\">t5-small</a> 的模型测试一下。</p>\n<p>t5 (Text-To-Text Transfer Transformer) 是一个统一框架，将所有 NLP 任务都转化成 Text-to-Text （文本到文本）任务。</p>\n<p><img src=\"https://1.bp.blogspot.com/-o4oiOExxq1s/Xk26XPC3haI/AAAAAAAAFU8/NBlvOWB84L0PTYy9TzZBaLf6fwPGJTR0QCLcBGAsYHQ/s640/image3.gif\" alt=\"\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> transformers <span class=\"keyword\">import</span> AutoTokenizer, AutoModelWithLMHead</span><br><span class=\"line\"></span><br><span class=\"line\">tokenizer = AutoTokenizer.from_pretrained(<span class=\"string\">&quot;t5-small&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">model = AutoModelWithLMHead.from_pretrained(<span class=\"string\">&quot;t5-small&quot;</span>)</span><br></pre></td></tr></table></figure>\n<p>只需要三行代码，就可以直接加载模型开始使用：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">input_ids = tokenizer(<span class=\"string\">&quot;translate English to Germany: What a nice day.&quot;</span>, return_tensors=<span class=\"string\">&quot;pt&quot;</span>).input_ids</span><br><span class=\"line\"></span><br><span class=\"line\">outputs = model.generate(input_ids)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(tokenizer.decode(outputs[<span class=\"number\">0</span>], skip_special_tokens=<span class=\"literal\">True</span>))</span><br></pre></td></tr></table></figure>\n<p>得到输出：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Was für ein nettes Tag.</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Hugging-Face-学习-1\"><a href=\"#Hugging-Face-学习-1\" class=\"headerlink\" title=\"Hugging Face 学习(1)\"></a>Hugging Face 学习(1)</h1><p>Hugging Face是一个非常流行的 NLP 库，Hugging Face 的 <a href=\"https://huggingface.co/\">官网</a> 上也有很多开源的模型。</p>\n<p>官网上的有些模型已经提供了简单的使用方法。比如我最先接触到的这个 <code>dalle-mini</code> 模型，提供了一个生成的接口(虽然好像还是定向到了<a href=\"https://www.craiyon.com\">其它网站</a>，据说是因为访问量太大了)，可以输入一段文本，生成一些图片。这里随便输入一个句子 A Rubic’s Cube swimming in the pool (水池里游泳的魔方)：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220720084135775.png\" alt=\"image-20220720084135775\"></p>\n<p>关于 <code>dalle</code> 这个模型，最近看到一篇文章比较有意思(<a href=\"https://blog.csdn.net/fengdu78/article/details/125109103\">https://blog.csdn.net/fengdu78/article/details/125109103</a>)</p>\n<hr>\n<p>如果是在本地使用 Hugging Face 的一些 NLP 模型，可以安装他们提供的 <code>transformers</code> 的库。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install transformers</span><br></pre></td></tr></table></figure>\n<p>这里使用一个 <a href=\"https://huggingface.co/t5-small\">t5-small</a> 的模型测试一下。</p>\n<p>t5 (Text-To-Text Transfer Transformer) 是一个统一框架，将所有 NLP 任务都转化成 Text-to-Text （文本到文本）任务。</p>\n<p><img src=\"https://1.bp.blogspot.com/-o4oiOExxq1s/Xk26XPC3haI/AAAAAAAAFU8/NBlvOWB84L0PTYy9TzZBaLf6fwPGJTR0QCLcBGAsYHQ/s640/image3.gif\" alt=\"\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> transformers <span class=\"keyword\">import</span> AutoTokenizer, AutoModelWithLMHead</span><br><span class=\"line\"></span><br><span class=\"line\">tokenizer = AutoTokenizer.from_pretrained(<span class=\"string\">&quot;t5-small&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">model = AutoModelWithLMHead.from_pretrained(<span class=\"string\">&quot;t5-small&quot;</span>)</span><br></pre></td></tr></table></figure>\n<p>只需要三行代码，就可以直接加载模型开始使用：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">input_ids = tokenizer(<span class=\"string\">&quot;translate English to Germany: What a nice day.&quot;</span>, return_tensors=<span class=\"string\">&quot;pt&quot;</span>).input_ids</span><br><span class=\"line\"></span><br><span class=\"line\">outputs = model.generate(input_ids)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(tokenizer.decode(outputs[<span class=\"number\">0</span>], skip_special_tokens=<span class=\"literal\">True</span>))</span><br></pre></td></tr></table></figure>\n<p>得到输出：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Was für ein nettes Tag.</span><br></pre></td></tr></table></figure>\n"},{"title":"Transformer 学习笔记","date":"2022-07-17T04:39:08.000Z","mathjax":true,"_content":"\n# Transformer 笔记\n\n> 输入 -> 编码 -> 解码 -> 输出\n\n## Transformer 结构\n\n注意点：\n\n每个小 Encoder 结构都是一样的，但是参数都不同，分别训练\n\n每个小 Decoder 结构都是一样的，但是参数都不同，分别训练\n\nEncoder 和 Decoder 是不一样的\n\n![image-20220804104944103](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220804104944103.png)\n\n这里的N在文章中为6\n\n## Encoder 和 Decoder\n\n### Encoder\n\n结构 : 输入、注意力机制、前馈神经网络\n\n六个大的模块之间是串行的\n\n每一个模块注意力层和前馈神经层自身可以并行计算，不同单词之间没有依赖关系；注意力层和前馈神经层又是串行关系。\n\n#### 输入\n\n包括：\n\n- Embedding\n\n- 位置编码\n\n位置编码记录了词序信息。\n\n关于位置编码，对于所有的 timeline 上的位置共享一套参数\n\nRNN 网络输入有天然的时序关系；Transformer 的 multihead 机制是同时处理所有输入的，忽略了时序关系，因此需要位置编码。\n\n位置编码的公式：\n\n$PE_{(pos,2i)}=\\sin(pos/10000^{2i/d_{model}})$\n\n$PE_{(pos,2i+1)}=\\cos(pos/10000^{2i/d_{model}})$\n\n其中 $i$ 是位置编码维度（如 0-511）\n\n位置编码（512d）+Embedding（512d）得到 transformer 的输入\n\n位置嵌入有效的原因：在绝对位置信息中包含相对位置信息，$pos+k$ 位置的位置向量中的某一维 $2i$ 或 $2i+1$ 能够被 $pos$ 位置和 $k$ 位置的位置向量中的 $2i$ 和 $2i+1$ 维线性组合。但这种相对位置信息会在注意力机制那里消失。\n\n推理公式：\n\n$PE_{(pos+k,2i)}=PE_{(pos,2i)}\\times PE_{(k,2i+1)}+PE_{(pos,2i+1)}\\times PE_{(k,2i)}$\n\n$PE_{(pos+k,2i+1)}=PE_{(pos,2i+1)}\\times PE_{(k,2i+1)}-PE_{(pos,2i)}\\times PE_{(k,2i)}$\n\n## 注意力机制\n\n$Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V$\n\n$Q$ 是输入，点乘 $K$（表示区域），得到相似度向量，softmax 归一化后乘 $V$ 得到 Attention Value\n\n在只有单词向量的情况下，将单词向量与 $QKV$ 矩阵相乘得到 $QKV$ 向量。 \n\n作归一化的原因：随着$d_k$的增大，$QK^T$的结果也随之增大，而如果点乘后的值过大，会导致softmax函数趋近于边缘，梯度较小。为了抵消影响，需要将点积缩放。\n\n## Normalization\n\n![diif_norm](https://raw.githubusercontent.com/1099255210/blogimgrepo/main/img/diff_norms.webp)\n\n**CV 使用 Batch Norm**\n\nBN 是对于每一个特征在 batch_size 上作特征缩放。\n\nBN的：\n\n- 优点：可以解决内部协变量偏移；缓解了梯度饱和问题，加快收敛\n- 缺点：batch_size 较小的时候效果差；在 RNN 中效果比较差，因为输入是动态的，不能有效地得到整个batch_size 中的均值和方差。\n\n**NLP 使用 Layer Norm**\n\nLN是针对每一个样本进行特征缩放。\n\n**为什么 CV 任务用 BN 而 NLP 任务使用 LN ?**\n\n词向量是学习出来的用来表示语义的参数，不是真实存在的；而图像的像素是真实存在，包含图像特征的信息。","source":"_posts/Transformer 学习笔记.md","raw":"---\ntitle: Transformer 学习笔记\ndate: 2022-07-17 12:39:08\ntags:\nmathjax: true\n---\n\n# Transformer 笔记\n\n> 输入 -> 编码 -> 解码 -> 输出\n\n## Transformer 结构\n\n注意点：\n\n每个小 Encoder 结构都是一样的，但是参数都不同，分别训练\n\n每个小 Decoder 结构都是一样的，但是参数都不同，分别训练\n\nEncoder 和 Decoder 是不一样的\n\n![image-20220804104944103](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220804104944103.png)\n\n这里的N在文章中为6\n\n## Encoder 和 Decoder\n\n### Encoder\n\n结构 : 输入、注意力机制、前馈神经网络\n\n六个大的模块之间是串行的\n\n每一个模块注意力层和前馈神经层自身可以并行计算，不同单词之间没有依赖关系；注意力层和前馈神经层又是串行关系。\n\n#### 输入\n\n包括：\n\n- Embedding\n\n- 位置编码\n\n位置编码记录了词序信息。\n\n关于位置编码，对于所有的 timeline 上的位置共享一套参数\n\nRNN 网络输入有天然的时序关系；Transformer 的 multihead 机制是同时处理所有输入的，忽略了时序关系，因此需要位置编码。\n\n位置编码的公式：\n\n$PE_{(pos,2i)}=\\sin(pos/10000^{2i/d_{model}})$\n\n$PE_{(pos,2i+1)}=\\cos(pos/10000^{2i/d_{model}})$\n\n其中 $i$ 是位置编码维度（如 0-511）\n\n位置编码（512d）+Embedding（512d）得到 transformer 的输入\n\n位置嵌入有效的原因：在绝对位置信息中包含相对位置信息，$pos+k$ 位置的位置向量中的某一维 $2i$ 或 $2i+1$ 能够被 $pos$ 位置和 $k$ 位置的位置向量中的 $2i$ 和 $2i+1$ 维线性组合。但这种相对位置信息会在注意力机制那里消失。\n\n推理公式：\n\n$PE_{(pos+k,2i)}=PE_{(pos,2i)}\\times PE_{(k,2i+1)}+PE_{(pos,2i+1)}\\times PE_{(k,2i)}$\n\n$PE_{(pos+k,2i+1)}=PE_{(pos,2i+1)}\\times PE_{(k,2i+1)}-PE_{(pos,2i)}\\times PE_{(k,2i)}$\n\n## 注意力机制\n\n$Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V$\n\n$Q$ 是输入，点乘 $K$（表示区域），得到相似度向量，softmax 归一化后乘 $V$ 得到 Attention Value\n\n在只有单词向量的情况下，将单词向量与 $QKV$ 矩阵相乘得到 $QKV$ 向量。 \n\n作归一化的原因：随着$d_k$的增大，$QK^T$的结果也随之增大，而如果点乘后的值过大，会导致softmax函数趋近于边缘，梯度较小。为了抵消影响，需要将点积缩放。\n\n## Normalization\n\n![diif_norm](https://raw.githubusercontent.com/1099255210/blogimgrepo/main/img/diff_norms.webp)\n\n**CV 使用 Batch Norm**\n\nBN 是对于每一个特征在 batch_size 上作特征缩放。\n\nBN的：\n\n- 优点：可以解决内部协变量偏移；缓解了梯度饱和问题，加快收敛\n- 缺点：batch_size 较小的时候效果差；在 RNN 中效果比较差，因为输入是动态的，不能有效地得到整个batch_size 中的均值和方差。\n\n**NLP 使用 Layer Norm**\n\nLN是针对每一个样本进行特征缩放。\n\n**为什么 CV 任务用 BN 而 NLP 任务使用 LN ?**\n\n词向量是学习出来的用来表示语义的参数，不是真实存在的；而图像的像素是真实存在，包含图像特征的信息。","slug":"Transformer 学习笔记","published":1,"updated":"2022-08-25T13:13:32.984Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clgyiartu0007jku78pj1bdqh","content":"<h1 id=\"Transformer-笔记\"><a href=\"#Transformer-笔记\" class=\"headerlink\" title=\"Transformer 笔记\"></a>Transformer 笔记</h1><blockquote>\n<p>输入 -&gt; 编码 -&gt; 解码 -&gt; 输出</p>\n</blockquote>\n<h2 id=\"Transformer-结构\"><a href=\"#Transformer-结构\" class=\"headerlink\" title=\"Transformer 结构\"></a>Transformer 结构</h2><p>注意点：</p>\n<p>每个小 Encoder 结构都是一样的，但是参数都不同，分别训练</p>\n<p>每个小 Decoder 结构都是一样的，但是参数都不同，分别训练</p>\n<p>Encoder 和 Decoder 是不一样的</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220804104944103.png\" alt=\"image-20220804104944103\"></p>\n<p>这里的N在文章中为6</p>\n<h2 id=\"Encoder-和-Decoder\"><a href=\"#Encoder-和-Decoder\" class=\"headerlink\" title=\"Encoder 和 Decoder\"></a>Encoder 和 Decoder</h2><h3 id=\"Encoder\"><a href=\"#Encoder\" class=\"headerlink\" title=\"Encoder\"></a>Encoder</h3><p>结构 : 输入、注意力机制、前馈神经网络</p>\n<p>六个大的模块之间是串行的</p>\n<p>每一个模块注意力层和前馈神经层自身可以并行计算，不同单词之间没有依赖关系；注意力层和前馈神经层又是串行关系。</p>\n<h4 id=\"输入\"><a href=\"#输入\" class=\"headerlink\" title=\"输入\"></a>输入</h4><p>包括：</p>\n<ul>\n<li><p>Embedding</p>\n</li>\n<li><p>位置编码</p>\n</li>\n</ul>\n<p>位置编码记录了词序信息。</p>\n<p>关于位置编码，对于所有的 timeline 上的位置共享一套参数</p>\n<p>RNN 网络输入有天然的时序关系；Transformer 的 multihead 机制是同时处理所有输入的，忽略了时序关系，因此需要位置编码。</p>\n<p>位置编码的公式：</p>\n<p>$PE_{(pos,2i)}=\\sin(pos/10000^{2i/d_{model}})$</p>\n<p>$PE_{(pos,2i+1)}=\\cos(pos/10000^{2i/d_{model}})$</p>\n<p>其中 $i$ 是位置编码维度（如 0-511）</p>\n<p>位置编码（512d）+Embedding（512d）得到 transformer 的输入</p>\n<p>位置嵌入有效的原因：在绝对位置信息中包含相对位置信息，$pos+k$ 位置的位置向量中的某一维 $2i$ 或 $2i+1$ 能够被 $pos$ 位置和 $k$ 位置的位置向量中的 $2i$ 和 $2i+1$ 维线性组合。但这种相对位置信息会在注意力机制那里消失。</p>\n<p>推理公式：</p>\n<p>$PE_{(pos+k,2i)}=PE_{(pos,2i)}\\times PE_{(k,2i+1)}+PE_{(pos,2i+1)}\\times PE_{(k,2i)}$</p>\n<p>$PE_{(pos+k,2i+1)}=PE_{(pos,2i+1)}\\times PE_{(k,2i+1)}-PE_{(pos,2i)}\\times PE_{(k,2i)}$</p>\n<h2 id=\"注意力机制\"><a href=\"#注意力机制\" class=\"headerlink\" title=\"注意力机制\"></a>注意力机制</h2><p>$Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V$</p>\n<p>$Q$ 是输入，点乘 $K$（表示区域），得到相似度向量，softmax 归一化后乘 $V$ 得到 Attention Value</p>\n<p>在只有单词向量的情况下，将单词向量与 $QKV$ 矩阵相乘得到 $QKV$ 向量。 </p>\n<p>作归一化的原因：随着$d_k$的增大，$QK^T$的结果也随之增大，而如果点乘后的值过大，会导致softmax函数趋近于边缘，梯度较小。为了抵消影响，需要将点积缩放。</p>\n<h2 id=\"Normalization\"><a href=\"#Normalization\" class=\"headerlink\" title=\"Normalization\"></a>Normalization</h2><p><img src=\"https://raw.githubusercontent.com/1099255210/blogimgrepo/main/img/diff_norms.webp\" alt=\"diif_norm\"></p>\n<p><strong>CV 使用 Batch Norm</strong></p>\n<p>BN 是对于每一个特征在 batch_size 上作特征缩放。</p>\n<p>BN的：</p>\n<ul>\n<li>优点：可以解决内部协变量偏移；缓解了梯度饱和问题，加快收敛</li>\n<li>缺点：batch_size 较小的时候效果差；在 RNN 中效果比较差，因为输入是动态的，不能有效地得到整个batch_size 中的均值和方差。</li>\n</ul>\n<p><strong>NLP 使用 Layer Norm</strong></p>\n<p>LN是针对每一个样本进行特征缩放。</p>\n<p><strong>为什么 CV 任务用 BN 而 NLP 任务使用 LN ?</strong></p>\n<p>词向量是学习出来的用来表示语义的参数，不是真实存在的；而图像的像素是真实存在，包含图像特征的信息。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Transformer-笔记\"><a href=\"#Transformer-笔记\" class=\"headerlink\" title=\"Transformer 笔记\"></a>Transformer 笔记</h1><blockquote>\n<p>输入 -&gt; 编码 -&gt; 解码 -&gt; 输出</p>\n</blockquote>\n<h2 id=\"Transformer-结构\"><a href=\"#Transformer-结构\" class=\"headerlink\" title=\"Transformer 结构\"></a>Transformer 结构</h2><p>注意点：</p>\n<p>每个小 Encoder 结构都是一样的，但是参数都不同，分别训练</p>\n<p>每个小 Decoder 结构都是一样的，但是参数都不同，分别训练</p>\n<p>Encoder 和 Decoder 是不一样的</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220804104944103.png\" alt=\"image-20220804104944103\"></p>\n<p>这里的N在文章中为6</p>\n<h2 id=\"Encoder-和-Decoder\"><a href=\"#Encoder-和-Decoder\" class=\"headerlink\" title=\"Encoder 和 Decoder\"></a>Encoder 和 Decoder</h2><h3 id=\"Encoder\"><a href=\"#Encoder\" class=\"headerlink\" title=\"Encoder\"></a>Encoder</h3><p>结构 : 输入、注意力机制、前馈神经网络</p>\n<p>六个大的模块之间是串行的</p>\n<p>每一个模块注意力层和前馈神经层自身可以并行计算，不同单词之间没有依赖关系；注意力层和前馈神经层又是串行关系。</p>\n<h4 id=\"输入\"><a href=\"#输入\" class=\"headerlink\" title=\"输入\"></a>输入</h4><p>包括：</p>\n<ul>\n<li><p>Embedding</p>\n</li>\n<li><p>位置编码</p>\n</li>\n</ul>\n<p>位置编码记录了词序信息。</p>\n<p>关于位置编码，对于所有的 timeline 上的位置共享一套参数</p>\n<p>RNN 网络输入有天然的时序关系；Transformer 的 multihead 机制是同时处理所有输入的，忽略了时序关系，因此需要位置编码。</p>\n<p>位置编码的公式：</p>\n<p>$PE_{(pos,2i)}=\\sin(pos/10000^{2i/d_{model}})$</p>\n<p>$PE_{(pos,2i+1)}=\\cos(pos/10000^{2i/d_{model}})$</p>\n<p>其中 $i$ 是位置编码维度（如 0-511）</p>\n<p>位置编码（512d）+Embedding（512d）得到 transformer 的输入</p>\n<p>位置嵌入有效的原因：在绝对位置信息中包含相对位置信息，$pos+k$ 位置的位置向量中的某一维 $2i$ 或 $2i+1$ 能够被 $pos$ 位置和 $k$ 位置的位置向量中的 $2i$ 和 $2i+1$ 维线性组合。但这种相对位置信息会在注意力机制那里消失。</p>\n<p>推理公式：</p>\n<p>$PE_{(pos+k,2i)}=PE_{(pos,2i)}\\times PE_{(k,2i+1)}+PE_{(pos,2i+1)}\\times PE_{(k,2i)}$</p>\n<p>$PE_{(pos+k,2i+1)}=PE_{(pos,2i+1)}\\times PE_{(k,2i+1)}-PE_{(pos,2i)}\\times PE_{(k,2i)}$</p>\n<h2 id=\"注意力机制\"><a href=\"#注意力机制\" class=\"headerlink\" title=\"注意力机制\"></a>注意力机制</h2><p>$Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V$</p>\n<p>$Q$ 是输入，点乘 $K$（表示区域），得到相似度向量，softmax 归一化后乘 $V$ 得到 Attention Value</p>\n<p>在只有单词向量的情况下，将单词向量与 $QKV$ 矩阵相乘得到 $QKV$ 向量。 </p>\n<p>作归一化的原因：随着$d_k$的增大，$QK^T$的结果也随之增大，而如果点乘后的值过大，会导致softmax函数趋近于边缘，梯度较小。为了抵消影响，需要将点积缩放。</p>\n<h2 id=\"Normalization\"><a href=\"#Normalization\" class=\"headerlink\" title=\"Normalization\"></a>Normalization</h2><p><img src=\"https://raw.githubusercontent.com/1099255210/blogimgrepo/main/img/diff_norms.webp\" alt=\"diif_norm\"></p>\n<p><strong>CV 使用 Batch Norm</strong></p>\n<p>BN 是对于每一个特征在 batch_size 上作特征缩放。</p>\n<p>BN的：</p>\n<ul>\n<li>优点：可以解决内部协变量偏移；缓解了梯度饱和问题，加快收敛</li>\n<li>缺点：batch_size 较小的时候效果差；在 RNN 中效果比较差，因为输入是动态的，不能有效地得到整个batch_size 中的均值和方差。</li>\n</ul>\n<p><strong>NLP 使用 Layer Norm</strong></p>\n<p>LN是针对每一个样本进行特征缩放。</p>\n<p><strong>为什么 CV 任务用 BN 而 NLP 任务使用 LN ?</strong></p>\n<p>词向量是学习出来的用来表示语义的参数，不是真实存在的；而图像的像素是真实存在，包含图像特征的信息。</p>\n"},{"title":"Ubuntu 换源","date":"2020-10-01T04:39:08.000Z","_content":"\n来到阿里云的镜像网站，查找对应系统需要替换的代码\n\nhttps://developer.aliyun.com/mirror/\n\n修改 `/etc/apt/sources.list` 中的内容，将原本的内容替换为阿里云镜像站上的的代码\n\n然后控制台输入：\n\n```shell\nsudo apt update\nsudo apt upgrade\n```","source":"_posts/Ubuntu 换源.md","raw":"---\ntitle: Ubuntu 换源\ndate: 2020-09-31 12:39:08\ntags:\n---\n\n来到阿里云的镜像网站，查找对应系统需要替换的代码\n\nhttps://developer.aliyun.com/mirror/\n\n修改 `/etc/apt/sources.list` 中的内容，将原本的内容替换为阿里云镜像站上的的代码\n\n然后控制台输入：\n\n```shell\nsudo apt update\nsudo apt upgrade\n```","slug":"Ubuntu 换源","published":1,"updated":"2022-08-27T02:10:45.083Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clgyiartu0008jku7f7ss2i7t","content":"<p>来到阿里云的镜像网站，查找对应系统需要替换的代码</p>\n<p><a href=\"https://developer.aliyun.com/mirror/\">https://developer.aliyun.com/mirror/</a></p>\n<p>修改 <code>/etc/apt/sources.list</code> 中的内容，将原本的内容替换为阿里云镜像站上的的代码</p>\n<p>然后控制台输入：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt update</span><br><span class=\"line\">sudo apt upgrade</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<p>来到阿里云的镜像网站，查找对应系统需要替换的代码</p>\n<p><a href=\"https://developer.aliyun.com/mirror/\">https://developer.aliyun.com/mirror/</a></p>\n<p>修改 <code>/etc/apt/sources.list</code> 中的内容，将原本的内容替换为阿里云镜像站上的的代码</p>\n<p>然后控制台输入：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt update</span><br><span class=\"line\">sudo apt upgrade</span><br></pre></td></tr></table></figure>"},{"title":"gradio 本地部署页面空白的解决方案","date":"2022-09-06T06:53:08.000Z","_content":"\n最近在学习如何用简单的 python 代码部署应用，于是接触到了 gradio 这个神器，看着就很好用，而且 huggingface 上的很多模型都在用它作为展示的 demo。\n\n但是在我想在本地部署它的时候，出现了问题：运行之后控制台输出了本地部署的链接，点开之后竟然是一个空白的页面。我的代码并不复杂，内容只有官方文档中的 quick start 的 helloworld 部分：\n\n```python\nimport gradio as gr\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\ndemo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\ndemo.launch()\n```\n\n（我的本地环境是 windows11，python 3.13，gradio 3.2）\n\n出了这个问题之后我看了一下浏览器控制台的报错信息，显示了一个 js 的引入错误：\n\n```\nFailed to load module script: Expected a JavaScript module script but the server responded with a MIME type of \"text/plain\". Strict MIME type checking is enforced for module scripts per HTML spec.\n```\n\n应该就是这个错误导致了无法正确地显示页面，遂以报错信息搜索，找到了一个 [issue](https://github.com/gradio-app/gradio/issues/1203)\n\n这里最终提出了解决的方案：\n\n在 `routes.py` 文件开头加入下面几行代码：\n\n```python\nimport mimetypes\nmimetypes.init()\n\nmimetypes.add_type('application/javascript', '.js')\n```\n\n\n\n![image-20220906150703488](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220906150703488.png)\n\n\n\n同时也了解到这个问题似乎在 windows 上发生得比较多，而且会在未来的版本里修复（如果没有副作用的话，但是这个问题是7.11解决的，现在已经9月了依然没有能在大版本修复），总之还是记录一下吧。","source":"_posts/gradio 本地部署页面空白的解决方案.md","raw":"---\ntitle: gradio 本地部署页面空白的解决方案\ndate: 2022-09-06 14:53:08\ntags:\n---\n\n最近在学习如何用简单的 python 代码部署应用，于是接触到了 gradio 这个神器，看着就很好用，而且 huggingface 上的很多模型都在用它作为展示的 demo。\n\n但是在我想在本地部署它的时候，出现了问题：运行之后控制台输出了本地部署的链接，点开之后竟然是一个空白的页面。我的代码并不复杂，内容只有官方文档中的 quick start 的 helloworld 部分：\n\n```python\nimport gradio as gr\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\ndemo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\ndemo.launch()\n```\n\n（我的本地环境是 windows11，python 3.13，gradio 3.2）\n\n出了这个问题之后我看了一下浏览器控制台的报错信息，显示了一个 js 的引入错误：\n\n```\nFailed to load module script: Expected a JavaScript module script but the server responded with a MIME type of \"text/plain\". Strict MIME type checking is enforced for module scripts per HTML spec.\n```\n\n应该就是这个错误导致了无法正确地显示页面，遂以报错信息搜索，找到了一个 [issue](https://github.com/gradio-app/gradio/issues/1203)\n\n这里最终提出了解决的方案：\n\n在 `routes.py` 文件开头加入下面几行代码：\n\n```python\nimport mimetypes\nmimetypes.init()\n\nmimetypes.add_type('application/javascript', '.js')\n```\n\n\n\n![image-20220906150703488](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220906150703488.png)\n\n\n\n同时也了解到这个问题似乎在 windows 上发生得比较多，而且会在未来的版本里修复（如果没有副作用的话，但是这个问题是7.11解决的，现在已经9月了依然没有能在大版本修复），总之还是记录一下吧。","slug":"gradio 本地部署页面空白的解决方案","published":1,"updated":"2022-09-09T04:34:56.840Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clgyiartv0009jku74plqhyql","content":"<p>最近在学习如何用简单的 python 代码部署应用，于是接触到了 gradio 这个神器，看着就很好用，而且 huggingface 上的很多模型都在用它作为展示的 demo。</p>\n<p>但是在我想在本地部署它的时候，出现了问题：运行之后控制台输出了本地部署的链接，点开之后竟然是一个空白的页面。我的代码并不复杂，内容只有官方文档中的 quick start 的 helloworld 部分：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> gradio <span class=\"keyword\">as</span> gr</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">greet</span>(<span class=\"params\">name</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"string\">&quot;Hello &quot;</span> + name + <span class=\"string\">&quot;!&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">demo = gr.Interface(fn=greet, inputs=<span class=\"string\">&quot;text&quot;</span>, outputs=<span class=\"string\">&quot;text&quot;</span>)</span><br><span class=\"line\">demo.launch()</span><br></pre></td></tr></table></figure>\n<p>（我的本地环境是 windows11，python 3.13，gradio 3.2）</p>\n<p>出了这个问题之后我看了一下浏览器控制台的报错信息，显示了一个 js 的引入错误：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Failed to load module script: Expected a JavaScript module script but the server responded with a MIME type of &quot;text/plain&quot;. Strict MIME type checking is enforced for module scripts per HTML spec.</span><br></pre></td></tr></table></figure>\n<p>应该就是这个错误导致了无法正确地显示页面，遂以报错信息搜索，找到了一个 <a href=\"https://github.com/gradio-app/gradio/issues/1203\">issue</a></p>\n<p>这里最终提出了解决的方案：</p>\n<p>在 <code>routes.py</code> 文件开头加入下面几行代码：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> mimetypes</span><br><span class=\"line\">mimetypes.init()</span><br><span class=\"line\"></span><br><span class=\"line\">mimetypes.add_type(<span class=\"string\">&#x27;application/javascript&#x27;</span>, <span class=\"string\">&#x27;.js&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220906150703488.png\" alt=\"image-20220906150703488\"></p>\n<p>同时也了解到这个问题似乎在 windows 上发生得比较多，而且会在未来的版本里修复（如果没有副作用的话，但是这个问题是7.11解决的，现在已经9月了依然没有能在大版本修复），总之还是记录一下吧。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>最近在学习如何用简单的 python 代码部署应用，于是接触到了 gradio 这个神器，看着就很好用，而且 huggingface 上的很多模型都在用它作为展示的 demo。</p>\n<p>但是在我想在本地部署它的时候，出现了问题：运行之后控制台输出了本地部署的链接，点开之后竟然是一个空白的页面。我的代码并不复杂，内容只有官方文档中的 quick start 的 helloworld 部分：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> gradio <span class=\"keyword\">as</span> gr</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">greet</span>(<span class=\"params\">name</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"string\">&quot;Hello &quot;</span> + name + <span class=\"string\">&quot;!&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">demo = gr.Interface(fn=greet, inputs=<span class=\"string\">&quot;text&quot;</span>, outputs=<span class=\"string\">&quot;text&quot;</span>)</span><br><span class=\"line\">demo.launch()</span><br></pre></td></tr></table></figure>\n<p>（我的本地环境是 windows11，python 3.13，gradio 3.2）</p>\n<p>出了这个问题之后我看了一下浏览器控制台的报错信息，显示了一个 js 的引入错误：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Failed to load module script: Expected a JavaScript module script but the server responded with a MIME type of &quot;text/plain&quot;. Strict MIME type checking is enforced for module scripts per HTML spec.</span><br></pre></td></tr></table></figure>\n<p>应该就是这个错误导致了无法正确地显示页面，遂以报错信息搜索，找到了一个 <a href=\"https://github.com/gradio-app/gradio/issues/1203\">issue</a></p>\n<p>这里最终提出了解决的方案：</p>\n<p>在 <code>routes.py</code> 文件开头加入下面几行代码：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> mimetypes</span><br><span class=\"line\">mimetypes.init()</span><br><span class=\"line\"></span><br><span class=\"line\">mimetypes.add_type(<span class=\"string\">&#x27;application/javascript&#x27;</span>, <span class=\"string\">&#x27;.js&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220906150703488.png\" alt=\"image-20220906150703488\"></p>\n<p>同时也了解到这个问题似乎在 windows 上发生得比较多，而且会在未来的版本里修复（如果没有副作用的话，但是这个问题是7.11解决的，现在已经9月了依然没有能在大版本修复），总之还是记录一下吧。</p>\n"},{"title":"Ubuntu 20.04 与 win10 双系统安装笔记","date":"2020-09-30T04:39:08.000Z","_content":"\n# Ubuntu 20.04 与 win10 双系统安装笔记\n\n## 安装前的认知准备\n\n在安装之前先了解一下基本的安装知识以及需求，以下为我认知中所需要作的准备：\n\n- 一个 8G 以上的u盘(作为启动盘)\n- Ubuntu 20.04 系统镜像(从[Ubuntu官网](https://ubuntu.com/download/desktop)下载)\n- 50G 及以上的磁盘空间\n\n本人电脑型号是 ThinkPad T460s，原操作系统为 win10 2004 x64，BIOS 模式为 UEFI 引导，电脑仅有一块 256G 固态硬盘，无独立显卡，因此应该不会出现N卡驱动缺失的情况。\n\n以上了解之后开始操作。\n\n## 准备与安装全过程\n\n首先，为系统分出足够空间。右键 **此电脑**，找到 **管理**>**存储**>**磁盘管理** ，右键磁盘，点击压缩卷。在这里我分出了 100G 的空间，应该足够轻度的使用。\n\n然后开始制作启动盘。下载制作启动盘的软件 [rufus](http://rufus.ie/) 并打开，插入u盘，在软件中选择正确的u盘与 ubuntu 镜像， 其余项目并不需要修改，直接开始，提示信息可忽略。耐心等待10-20分钟即可制作完成。\n\n接下来是 BIOS 的设置。重启电脑前，找到电脑电源设置，关闭快速启动；然后重启电脑，找到本电脑型号进入 BIOS 的快捷键，我的电脑是 DELETE；进入 BIOS 之后，将 Secure Boot 设置为 Disabled，再确认一下 Legacy Boot 为 UEFI，然后插入u盘，将u盘的BOOT优先级提到最高，保存设置并启动。这时候电脑启动时就会进入选择的界面，我们直接选择 ubuntu，这样就进入系统安装的界面了。\n\n系统安装过程中，没有什么要特别注意的。更新和其他软件那里我选择的是正常安装；安装类型里，网上有教程称需要选择其他选项，在这里我选择了与 Windows Boot Manager 共存，目前并未出现太大问题。等待一阵子，系统应当就准备就绪了。\n\n## 对于系统的配置\n\n### 替换国内源\n\n在 **Software & Updates** 中选择国内的镜像，我选择的是阿里云(mirrors.aliyun.com)的镜像\n\n选择之后 Ctrl+Alt+T 打开终端，输入以下指令更新：\n\n```shell\nsudo apt update\nsudo apt upgrade\n```\n\n### 中文输入法\n\n```shell\nsudo apt install ibus-libpinyin\nsudo apt install ibus-clutter\n```\n\n然后重启，找到输入法选项，在 Input Sources 中新增 Chinese(Intelligent Pinyin) 就可以了。\n\n### 双系统时间不统一\n\n这个是系统看待硬件时间方式不一造成的，终端输入：\n\n```shell\ntimedatectl set-local-rtc 1 --adjust-system-clock\n```\n\n禁用 Ubuntu 中的 UTC 即可。\n\n### 取消 sudo 密码（有风险）\n\n```shell\nsudo cp /etc/sudoers\nsudo visudo\n```\n\n找到 `%sudo ALL=(ALL:ALL) ALL` 修改为 `%sudo ALL=(ALL:ALL) NOPASSWD:ALL` 然后 Ctrl+s, Ctrl+x, 重启即可。\n\n### 安装与配置 CMake\n\n在 CMake 官网下载源码，本次安装使用的版本是 3.18.3 的版本。下载完成后打开终端：\n\n```shell\ntar -xzvf cmake-3.18.3.tar.gz\ncd cmake-3.18.3 \n./bootstrap\n```\n\n出现报错：\n\n```shell\nCannot find a C++ compiler that supports both C++11 and the specified C++ flags.\n......\n```\n\n通过指令更新 g++：\n\n```shell\nsudo apt-get install g++\n```\n\n继续报错：\n\n```shell\nCannot find appropriate Makefile processor on this system.\nPlease specify one using environment variable MAKE.\n```\n\n 输入指令：\n\n```shell\nsudo apt-get install build-essential\n```\n\n`./bootstrap` 运行到最后再次出错：\n\n```shell\nCould not find OpenSSL. Install an OpenSSL development package or configure CMake with -DCMAIKE_USE_OPENSSL=OFF to build without OpenSSL.\n```\n\n安装 openssl 的编译依赖：\n\n```shell\nsudo apt-get install libssl-dev\n```\n\n成功运行，输入`make` ，耐心等待，这一步时间较长。完成后，输入\n\n```shell\nsudo make install\n```\n\n完成，查看版本 `cmake --version` 看到 `cmake version 3.18.3` 安装成功\n\n### 配置 boost\n\n这里选用的版本是 boost 1.69.0 (其实我也不知道该装哪一个版本)\n\n首先官网下载，解包，进入文件夹：\n\n```Shell\n./bootstrap.sh\n```\n\n没有问题，下一步\n\n```shell\n./b2\n```\n\n这一步也比较长，完成之后：\n\n```\nsudo ./b2 install\n```\n\n配置完成。\n\n### 配置 OpenCV\n\n到现在再配置 OpenCV 是考虑到 OpenCV 需要使用 CMake 以及个人感觉配置可能需要一些库的安装。\n\n同样首先到官网下载源码，`unzip` 解压，进入目录：\n\n```shell\nmkdir build\ncd build\ncmake ..\n```\n\n如果没有问题，下一步：\n\n```shell\nmake\nmake install\n```\n\n配置完成\n\n### 配置 git\n\n```shell\nsudo apt-get install git\ngit --version\n```\n\n控制台输出 `git version 2.25.1` 安装完成\n\n## 使用过程中的其他经验在之后进行补充\n\n总体来说，由于很早之前有安装的经验，这次的安装并没有碰到太大的问题，还算顺利。\n\nCSDN链接：https://blog.csdn.net/wwh010802/article/details/108857863\n\n","source":"_posts/ubuntu 20.04 安装.md","raw":"---\ntitle: Ubuntu 20.04 与 win10 双系统安装笔记\ndate: 2020-09-30 12:39:08\ntags:\n---\n\n# Ubuntu 20.04 与 win10 双系统安装笔记\n\n## 安装前的认知准备\n\n在安装之前先了解一下基本的安装知识以及需求，以下为我认知中所需要作的准备：\n\n- 一个 8G 以上的u盘(作为启动盘)\n- Ubuntu 20.04 系统镜像(从[Ubuntu官网](https://ubuntu.com/download/desktop)下载)\n- 50G 及以上的磁盘空间\n\n本人电脑型号是 ThinkPad T460s，原操作系统为 win10 2004 x64，BIOS 模式为 UEFI 引导，电脑仅有一块 256G 固态硬盘，无独立显卡，因此应该不会出现N卡驱动缺失的情况。\n\n以上了解之后开始操作。\n\n## 准备与安装全过程\n\n首先，为系统分出足够空间。右键 **此电脑**，找到 **管理**>**存储**>**磁盘管理** ，右键磁盘，点击压缩卷。在这里我分出了 100G 的空间，应该足够轻度的使用。\n\n然后开始制作启动盘。下载制作启动盘的软件 [rufus](http://rufus.ie/) 并打开，插入u盘，在软件中选择正确的u盘与 ubuntu 镜像， 其余项目并不需要修改，直接开始，提示信息可忽略。耐心等待10-20分钟即可制作完成。\n\n接下来是 BIOS 的设置。重启电脑前，找到电脑电源设置，关闭快速启动；然后重启电脑，找到本电脑型号进入 BIOS 的快捷键，我的电脑是 DELETE；进入 BIOS 之后，将 Secure Boot 设置为 Disabled，再确认一下 Legacy Boot 为 UEFI，然后插入u盘，将u盘的BOOT优先级提到最高，保存设置并启动。这时候电脑启动时就会进入选择的界面，我们直接选择 ubuntu，这样就进入系统安装的界面了。\n\n系统安装过程中，没有什么要特别注意的。更新和其他软件那里我选择的是正常安装；安装类型里，网上有教程称需要选择其他选项，在这里我选择了与 Windows Boot Manager 共存，目前并未出现太大问题。等待一阵子，系统应当就准备就绪了。\n\n## 对于系统的配置\n\n### 替换国内源\n\n在 **Software & Updates** 中选择国内的镜像，我选择的是阿里云(mirrors.aliyun.com)的镜像\n\n选择之后 Ctrl+Alt+T 打开终端，输入以下指令更新：\n\n```shell\nsudo apt update\nsudo apt upgrade\n```\n\n### 中文输入法\n\n```shell\nsudo apt install ibus-libpinyin\nsudo apt install ibus-clutter\n```\n\n然后重启，找到输入法选项，在 Input Sources 中新增 Chinese(Intelligent Pinyin) 就可以了。\n\n### 双系统时间不统一\n\n这个是系统看待硬件时间方式不一造成的，终端输入：\n\n```shell\ntimedatectl set-local-rtc 1 --adjust-system-clock\n```\n\n禁用 Ubuntu 中的 UTC 即可。\n\n### 取消 sudo 密码（有风险）\n\n```shell\nsudo cp /etc/sudoers\nsudo visudo\n```\n\n找到 `%sudo ALL=(ALL:ALL) ALL` 修改为 `%sudo ALL=(ALL:ALL) NOPASSWD:ALL` 然后 Ctrl+s, Ctrl+x, 重启即可。\n\n### 安装与配置 CMake\n\n在 CMake 官网下载源码，本次安装使用的版本是 3.18.3 的版本。下载完成后打开终端：\n\n```shell\ntar -xzvf cmake-3.18.3.tar.gz\ncd cmake-3.18.3 \n./bootstrap\n```\n\n出现报错：\n\n```shell\nCannot find a C++ compiler that supports both C++11 and the specified C++ flags.\n......\n```\n\n通过指令更新 g++：\n\n```shell\nsudo apt-get install g++\n```\n\n继续报错：\n\n```shell\nCannot find appropriate Makefile processor on this system.\nPlease specify one using environment variable MAKE.\n```\n\n 输入指令：\n\n```shell\nsudo apt-get install build-essential\n```\n\n`./bootstrap` 运行到最后再次出错：\n\n```shell\nCould not find OpenSSL. Install an OpenSSL development package or configure CMake with -DCMAIKE_USE_OPENSSL=OFF to build without OpenSSL.\n```\n\n安装 openssl 的编译依赖：\n\n```shell\nsudo apt-get install libssl-dev\n```\n\n成功运行，输入`make` ，耐心等待，这一步时间较长。完成后，输入\n\n```shell\nsudo make install\n```\n\n完成，查看版本 `cmake --version` 看到 `cmake version 3.18.3` 安装成功\n\n### 配置 boost\n\n这里选用的版本是 boost 1.69.0 (其实我也不知道该装哪一个版本)\n\n首先官网下载，解包，进入文件夹：\n\n```Shell\n./bootstrap.sh\n```\n\n没有问题，下一步\n\n```shell\n./b2\n```\n\n这一步也比较长，完成之后：\n\n```\nsudo ./b2 install\n```\n\n配置完成。\n\n### 配置 OpenCV\n\n到现在再配置 OpenCV 是考虑到 OpenCV 需要使用 CMake 以及个人感觉配置可能需要一些库的安装。\n\n同样首先到官网下载源码，`unzip` 解压，进入目录：\n\n```shell\nmkdir build\ncd build\ncmake ..\n```\n\n如果没有问题，下一步：\n\n```shell\nmake\nmake install\n```\n\n配置完成\n\n### 配置 git\n\n```shell\nsudo apt-get install git\ngit --version\n```\n\n控制台输出 `git version 2.25.1` 安装完成\n\n## 使用过程中的其他经验在之后进行补充\n\n总体来说，由于很早之前有安装的经验，这次的安装并没有碰到太大的问题，还算顺利。\n\nCSDN链接：https://blog.csdn.net/wwh010802/article/details/108857863\n\n","slug":"ubuntu 20.04 安装","published":1,"updated":"2022-08-25T13:13:32.985Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clgyiartv000ajku7gfx47ad8","content":"<h1 id=\"Ubuntu-20-04-与-win10-双系统安装笔记\"><a href=\"#Ubuntu-20-04-与-win10-双系统安装笔记\" class=\"headerlink\" title=\"Ubuntu 20.04 与 win10 双系统安装笔记\"></a>Ubuntu 20.04 与 win10 双系统安装笔记</h1><h2 id=\"安装前的认知准备\"><a href=\"#安装前的认知准备\" class=\"headerlink\" title=\"安装前的认知准备\"></a>安装前的认知准备</h2><p>在安装之前先了解一下基本的安装知识以及需求，以下为我认知中所需要作的准备：</p>\n<ul>\n<li>一个 8G 以上的u盘(作为启动盘)</li>\n<li>Ubuntu 20.04 系统镜像(从<a href=\"https://ubuntu.com/download/desktop\">Ubuntu官网</a>下载)</li>\n<li>50G 及以上的磁盘空间</li>\n</ul>\n<p>本人电脑型号是 ThinkPad T460s，原操作系统为 win10 2004 x64，BIOS 模式为 UEFI 引导，电脑仅有一块 256G 固态硬盘，无独立显卡，因此应该不会出现N卡驱动缺失的情况。</p>\n<p>以上了解之后开始操作。</p>\n<h2 id=\"准备与安装全过程\"><a href=\"#准备与安装全过程\" class=\"headerlink\" title=\"准备与安装全过程\"></a>准备与安装全过程</h2><p>首先，为系统分出足够空间。右键 <strong>此电脑</strong>，找到 <strong>管理</strong>&gt;<strong>存储</strong>&gt;<strong>磁盘管理</strong> ，右键磁盘，点击压缩卷。在这里我分出了 100G 的空间，应该足够轻度的使用。</p>\n<p>然后开始制作启动盘。下载制作启动盘的软件 <a href=\"http://rufus.ie/\">rufus</a> 并打开，插入u盘，在软件中选择正确的u盘与 ubuntu 镜像， 其余项目并不需要修改，直接开始，提示信息可忽略。耐心等待10-20分钟即可制作完成。</p>\n<p>接下来是 BIOS 的设置。重启电脑前，找到电脑电源设置，关闭快速启动；然后重启电脑，找到本电脑型号进入 BIOS 的快捷键，我的电脑是 DELETE；进入 BIOS 之后，将 Secure Boot 设置为 Disabled，再确认一下 Legacy Boot 为 UEFI，然后插入u盘，将u盘的BOOT优先级提到最高，保存设置并启动。这时候电脑启动时就会进入选择的界面，我们直接选择 ubuntu，这样就进入系统安装的界面了。</p>\n<p>系统安装过程中，没有什么要特别注意的。更新和其他软件那里我选择的是正常安装；安装类型里，网上有教程称需要选择其他选项，在这里我选择了与 Windows Boot Manager 共存，目前并未出现太大问题。等待一阵子，系统应当就准备就绪了。</p>\n<h2 id=\"对于系统的配置\"><a href=\"#对于系统的配置\" class=\"headerlink\" title=\"对于系统的配置\"></a>对于系统的配置</h2><h3 id=\"替换国内源\"><a href=\"#替换国内源\" class=\"headerlink\" title=\"替换国内源\"></a>替换国内源</h3><p>在 <strong>Software &amp; Updates</strong> 中选择国内的镜像，我选择的是阿里云(mirrors.aliyun.com)的镜像</p>\n<p>选择之后 Ctrl+Alt+T 打开终端，输入以下指令更新：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt update</span><br><span class=\"line\">sudo apt upgrade</span><br></pre></td></tr></table></figure>\n<h3 id=\"中文输入法\"><a href=\"#中文输入法\" class=\"headerlink\" title=\"中文输入法\"></a>中文输入法</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt install ibus-libpinyin</span><br><span class=\"line\">sudo apt install ibus-clutter</span><br></pre></td></tr></table></figure>\n<p>然后重启，找到输入法选项，在 Input Sources 中新增 Chinese(Intelligent Pinyin) 就可以了。</p>\n<h3 id=\"双系统时间不统一\"><a href=\"#双系统时间不统一\" class=\"headerlink\" title=\"双系统时间不统一\"></a>双系统时间不统一</h3><p>这个是系统看待硬件时间方式不一造成的，终端输入：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">timedatectl set-local-rtc 1 --adjust-system-clock</span><br></pre></td></tr></table></figure>\n<p>禁用 Ubuntu 中的 UTC 即可。</p>\n<h3 id=\"取消-sudo-密码（有风险）\"><a href=\"#取消-sudo-密码（有风险）\" class=\"headerlink\" title=\"取消 sudo 密码（有风险）\"></a>取消 sudo 密码（有风险）</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo cp /etc/sudoers</span><br><span class=\"line\">sudo visudo</span><br></pre></td></tr></table></figure>\n<p>找到 <code>%sudo ALL=(ALL:ALL) ALL</code> 修改为 <code>%sudo ALL=(ALL:ALL) NOPASSWD:ALL</code> 然后 Ctrl+s, Ctrl+x, 重启即可。</p>\n<h3 id=\"安装与配置-CMake\"><a href=\"#安装与配置-CMake\" class=\"headerlink\" title=\"安装与配置 CMake\"></a>安装与配置 CMake</h3><p>在 CMake 官网下载源码，本次安装使用的版本是 3.18.3 的版本。下载完成后打开终端：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tar -xzvf cmake-3.18.3.tar.gz</span><br><span class=\"line\">cd cmake-3.18.3 </span><br><span class=\"line\">./bootstrap</span><br></pre></td></tr></table></figure>\n<p>出现报错：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Cannot find a C++ compiler that supports both C++11 and the specified C++ flags.</span><br><span class=\"line\">......</span><br></pre></td></tr></table></figure>\n<p>通过指令更新 g++：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install g++</span><br></pre></td></tr></table></figure>\n<p>继续报错：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Cannot find appropriate Makefile processor on this system.</span><br><span class=\"line\">Please specify one using environment variable MAKE.</span><br></pre></td></tr></table></figure>\n<p> 输入指令：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install build-essential</span><br></pre></td></tr></table></figure>\n<p><code>./bootstrap</code> 运行到最后再次出错：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Could not find OpenSSL. Install an OpenSSL development package or configure CMake with -DCMAIKE_USE_OPENSSL=OFF to build without OpenSSL.</span><br></pre></td></tr></table></figure>\n<p>安装 openssl 的编译依赖：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install libssl-dev</span><br></pre></td></tr></table></figure>\n<p>成功运行，输入<code>make</code> ，耐心等待，这一步时间较长。完成后，输入</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo make install</span><br></pre></td></tr></table></figure>\n<p>完成，查看版本 <code>cmake --version</code> 看到 <code>cmake version 3.18.3</code> 安装成功</p>\n<h3 id=\"配置-boost\"><a href=\"#配置-boost\" class=\"headerlink\" title=\"配置 boost\"></a>配置 boost</h3><p>这里选用的版本是 boost 1.69.0 (其实我也不知道该装哪一个版本)</p>\n<p>首先官网下载，解包，进入文件夹：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./bootstrap.sh</span><br></pre></td></tr></table></figure>\n<p>没有问题，下一步</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./b2</span><br></pre></td></tr></table></figure>\n<p>这一步也比较长，完成之后：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo ./b2 install</span><br></pre></td></tr></table></figure>\n<p>配置完成。</p>\n<h3 id=\"配置-OpenCV\"><a href=\"#配置-OpenCV\" class=\"headerlink\" title=\"配置 OpenCV\"></a>配置 OpenCV</h3><p>到现在再配置 OpenCV 是考虑到 OpenCV 需要使用 CMake 以及个人感觉配置可能需要一些库的安装。</p>\n<p>同样首先到官网下载源码，<code>unzip</code> 解压，进入目录：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir build</span><br><span class=\"line\">cd build</span><br><span class=\"line\">cmake ..</span><br></pre></td></tr></table></figure>\n<p>如果没有问题，下一步：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">make</span><br><span class=\"line\">make install</span><br></pre></td></tr></table></figure>\n<p>配置完成</p>\n<h3 id=\"配置-git\"><a href=\"#配置-git\" class=\"headerlink\" title=\"配置 git\"></a>配置 git</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install git</span><br><span class=\"line\">git --version</span><br></pre></td></tr></table></figure>\n<p>控制台输出 <code>git version 2.25.1</code> 安装完成</p>\n<h2 id=\"使用过程中的其他经验在之后进行补充\"><a href=\"#使用过程中的其他经验在之后进行补充\" class=\"headerlink\" title=\"使用过程中的其他经验在之后进行补充\"></a>使用过程中的其他经验在之后进行补充</h2><p>总体来说，由于很早之前有安装的经验，这次的安装并没有碰到太大的问题，还算顺利。</p>\n<p>CSDN链接：<a href=\"https://blog.csdn.net/wwh010802/article/details/108857863\">https://blog.csdn.net/wwh010802/article/details/108857863</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Ubuntu-20-04-与-win10-双系统安装笔记\"><a href=\"#Ubuntu-20-04-与-win10-双系统安装笔记\" class=\"headerlink\" title=\"Ubuntu 20.04 与 win10 双系统安装笔记\"></a>Ubuntu 20.04 与 win10 双系统安装笔记</h1><h2 id=\"安装前的认知准备\"><a href=\"#安装前的认知准备\" class=\"headerlink\" title=\"安装前的认知准备\"></a>安装前的认知准备</h2><p>在安装之前先了解一下基本的安装知识以及需求，以下为我认知中所需要作的准备：</p>\n<ul>\n<li>一个 8G 以上的u盘(作为启动盘)</li>\n<li>Ubuntu 20.04 系统镜像(从<a href=\"https://ubuntu.com/download/desktop\">Ubuntu官网</a>下载)</li>\n<li>50G 及以上的磁盘空间</li>\n</ul>\n<p>本人电脑型号是 ThinkPad T460s，原操作系统为 win10 2004 x64，BIOS 模式为 UEFI 引导，电脑仅有一块 256G 固态硬盘，无独立显卡，因此应该不会出现N卡驱动缺失的情况。</p>\n<p>以上了解之后开始操作。</p>\n<h2 id=\"准备与安装全过程\"><a href=\"#准备与安装全过程\" class=\"headerlink\" title=\"准备与安装全过程\"></a>准备与安装全过程</h2><p>首先，为系统分出足够空间。右键 <strong>此电脑</strong>，找到 <strong>管理</strong>&gt;<strong>存储</strong>&gt;<strong>磁盘管理</strong> ，右键磁盘，点击压缩卷。在这里我分出了 100G 的空间，应该足够轻度的使用。</p>\n<p>然后开始制作启动盘。下载制作启动盘的软件 <a href=\"http://rufus.ie/\">rufus</a> 并打开，插入u盘，在软件中选择正确的u盘与 ubuntu 镜像， 其余项目并不需要修改，直接开始，提示信息可忽略。耐心等待10-20分钟即可制作完成。</p>\n<p>接下来是 BIOS 的设置。重启电脑前，找到电脑电源设置，关闭快速启动；然后重启电脑，找到本电脑型号进入 BIOS 的快捷键，我的电脑是 DELETE；进入 BIOS 之后，将 Secure Boot 设置为 Disabled，再确认一下 Legacy Boot 为 UEFI，然后插入u盘，将u盘的BOOT优先级提到最高，保存设置并启动。这时候电脑启动时就会进入选择的界面，我们直接选择 ubuntu，这样就进入系统安装的界面了。</p>\n<p>系统安装过程中，没有什么要特别注意的。更新和其他软件那里我选择的是正常安装；安装类型里，网上有教程称需要选择其他选项，在这里我选择了与 Windows Boot Manager 共存，目前并未出现太大问题。等待一阵子，系统应当就准备就绪了。</p>\n<h2 id=\"对于系统的配置\"><a href=\"#对于系统的配置\" class=\"headerlink\" title=\"对于系统的配置\"></a>对于系统的配置</h2><h3 id=\"替换国内源\"><a href=\"#替换国内源\" class=\"headerlink\" title=\"替换国内源\"></a>替换国内源</h3><p>在 <strong>Software &amp; Updates</strong> 中选择国内的镜像，我选择的是阿里云(mirrors.aliyun.com)的镜像</p>\n<p>选择之后 Ctrl+Alt+T 打开终端，输入以下指令更新：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt update</span><br><span class=\"line\">sudo apt upgrade</span><br></pre></td></tr></table></figure>\n<h3 id=\"中文输入法\"><a href=\"#中文输入法\" class=\"headerlink\" title=\"中文输入法\"></a>中文输入法</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt install ibus-libpinyin</span><br><span class=\"line\">sudo apt install ibus-clutter</span><br></pre></td></tr></table></figure>\n<p>然后重启，找到输入法选项，在 Input Sources 中新增 Chinese(Intelligent Pinyin) 就可以了。</p>\n<h3 id=\"双系统时间不统一\"><a href=\"#双系统时间不统一\" class=\"headerlink\" title=\"双系统时间不统一\"></a>双系统时间不统一</h3><p>这个是系统看待硬件时间方式不一造成的，终端输入：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">timedatectl set-local-rtc 1 --adjust-system-clock</span><br></pre></td></tr></table></figure>\n<p>禁用 Ubuntu 中的 UTC 即可。</p>\n<h3 id=\"取消-sudo-密码（有风险）\"><a href=\"#取消-sudo-密码（有风险）\" class=\"headerlink\" title=\"取消 sudo 密码（有风险）\"></a>取消 sudo 密码（有风险）</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo cp /etc/sudoers</span><br><span class=\"line\">sudo visudo</span><br></pre></td></tr></table></figure>\n<p>找到 <code>%sudo ALL=(ALL:ALL) ALL</code> 修改为 <code>%sudo ALL=(ALL:ALL) NOPASSWD:ALL</code> 然后 Ctrl+s, Ctrl+x, 重启即可。</p>\n<h3 id=\"安装与配置-CMake\"><a href=\"#安装与配置-CMake\" class=\"headerlink\" title=\"安装与配置 CMake\"></a>安装与配置 CMake</h3><p>在 CMake 官网下载源码，本次安装使用的版本是 3.18.3 的版本。下载完成后打开终端：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tar -xzvf cmake-3.18.3.tar.gz</span><br><span class=\"line\">cd cmake-3.18.3 </span><br><span class=\"line\">./bootstrap</span><br></pre></td></tr></table></figure>\n<p>出现报错：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Cannot find a C++ compiler that supports both C++11 and the specified C++ flags.</span><br><span class=\"line\">......</span><br></pre></td></tr></table></figure>\n<p>通过指令更新 g++：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install g++</span><br></pre></td></tr></table></figure>\n<p>继续报错：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Cannot find appropriate Makefile processor on this system.</span><br><span class=\"line\">Please specify one using environment variable MAKE.</span><br></pre></td></tr></table></figure>\n<p> 输入指令：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install build-essential</span><br></pre></td></tr></table></figure>\n<p><code>./bootstrap</code> 运行到最后再次出错：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Could not find OpenSSL. Install an OpenSSL development package or configure CMake with -DCMAIKE_USE_OPENSSL=OFF to build without OpenSSL.</span><br></pre></td></tr></table></figure>\n<p>安装 openssl 的编译依赖：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install libssl-dev</span><br></pre></td></tr></table></figure>\n<p>成功运行，输入<code>make</code> ，耐心等待，这一步时间较长。完成后，输入</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo make install</span><br></pre></td></tr></table></figure>\n<p>完成，查看版本 <code>cmake --version</code> 看到 <code>cmake version 3.18.3</code> 安装成功</p>\n<h3 id=\"配置-boost\"><a href=\"#配置-boost\" class=\"headerlink\" title=\"配置 boost\"></a>配置 boost</h3><p>这里选用的版本是 boost 1.69.0 (其实我也不知道该装哪一个版本)</p>\n<p>首先官网下载，解包，进入文件夹：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./bootstrap.sh</span><br></pre></td></tr></table></figure>\n<p>没有问题，下一步</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./b2</span><br></pre></td></tr></table></figure>\n<p>这一步也比较长，完成之后：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo ./b2 install</span><br></pre></td></tr></table></figure>\n<p>配置完成。</p>\n<h3 id=\"配置-OpenCV\"><a href=\"#配置-OpenCV\" class=\"headerlink\" title=\"配置 OpenCV\"></a>配置 OpenCV</h3><p>到现在再配置 OpenCV 是考虑到 OpenCV 需要使用 CMake 以及个人感觉配置可能需要一些库的安装。</p>\n<p>同样首先到官网下载源码，<code>unzip</code> 解压，进入目录：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir build</span><br><span class=\"line\">cd build</span><br><span class=\"line\">cmake ..</span><br></pre></td></tr></table></figure>\n<p>如果没有问题，下一步：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">make</span><br><span class=\"line\">make install</span><br></pre></td></tr></table></figure>\n<p>配置完成</p>\n<h3 id=\"配置-git\"><a href=\"#配置-git\" class=\"headerlink\" title=\"配置 git\"></a>配置 git</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install git</span><br><span class=\"line\">git --version</span><br></pre></td></tr></table></figure>\n<p>控制台输出 <code>git version 2.25.1</code> 安装完成</p>\n<h2 id=\"使用过程中的其他经验在之后进行补充\"><a href=\"#使用过程中的其他经验在之后进行补充\" class=\"headerlink\" title=\"使用过程中的其他经验在之后进行补充\"></a>使用过程中的其他经验在之后进行补充</h2><p>总体来说，由于很早之前有安装的经验，这次的安装并没有碰到太大的问题，还算顺利。</p>\n<p>CSDN链接：<a href=\"https://blog.csdn.net/wwh010802/article/details/108857863\">https://blog.csdn.net/wwh010802/article/details/108857863</a></p>\n"},{"title":"Windows 如何正确安装 cuda 与 cudnn","date":"2022-08-26T09:02:08.000Z","_content":"\n# Windows 如何正确安装 cuda 与 cudnn\n\n首先要了解到的信息是，你的 Nvidia 显卡到底支持多高版本的 CUDA? 在系统状态栏中找到 `NVIDIA 控制面板`，查看 `[帮助]-[系统信息]-[组件]-[NVCUDA64.DLL]` 查看后面的版本，你需要安装的 CUDA 不得高于此版本。 \n\n\n![image-20220826191251461](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220826191251461.png)\n\n\n但是这里要注意的是，并不是你的显卡型号决定了你支持的 CUDA 版本，而是你的显卡和显卡驱动共同决定的。想要更新你的显卡驱动，可以尝试使用 NVIDIA GeForce Experience 进行更新，之后再在 `NVIDIA 控制面板` 查看支持的 CUDA 版本。\n\n\n确定要下载的版本后，在此网站 [CUDA Toolkit Archive](https://developer.nvidia.cn/cuda-toolkit-archive) 找到你需要的版本，我这里选择了 CUDA 11.1.1 的版本，并且按照下图选择了对应我系统的安装包：\n\n\n\n![image-20220826192316090](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220826192316090.png)\n\n\n\n打开安装包，安装过程中选择自定义安装，然后查看下图中的项目，如果当前版本大于新版本则将此项取消勾选，反之则可以勾选。\n\n\n\n![image-20220826192856077](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220826192856077.png)\n\n\n\n安装完成后打开 Powershell 输入 `nvcc -V` 查看是否成功安装：\n\n![image-20220826210836346](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220826210836346.png)\n\n\n\n然后是 cuDNN(针对于神经网络应用加速的库)，进入网站 [cudnn-download](https://developer.nvidia.cn/rdp/cudnn-download) ，这里需要先注册一个 NVIDIA 账号，然后选择对应的版本进行下载。\n\n将下载的压缩包解压，将里面三个文件夹直接放入之前安装的 CUDA 目录中。\n\n\n\n![image-20220826211547617](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220826211547617.png)\n\n\n\n验证一下是否安装成功，打开 Powershell 输入 `nvidia-smi`\n\n\n\n![image-20220826211925113](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220826211925113.png)\n\n\n可以看到安装是成功的","source":"_posts/Windows 如何正确安装 cuda 与 cudnn.md","raw":"---\ntitle: Windows 如何正确安装 cuda 与 cudnn\ndate: 2022-08-26 17:02:08\ntags:\n---\n\n# Windows 如何正确安装 cuda 与 cudnn\n\n首先要了解到的信息是，你的 Nvidia 显卡到底支持多高版本的 CUDA? 在系统状态栏中找到 `NVIDIA 控制面板`，查看 `[帮助]-[系统信息]-[组件]-[NVCUDA64.DLL]` 查看后面的版本，你需要安装的 CUDA 不得高于此版本。 \n\n\n![image-20220826191251461](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220826191251461.png)\n\n\n但是这里要注意的是，并不是你的显卡型号决定了你支持的 CUDA 版本，而是你的显卡和显卡驱动共同决定的。想要更新你的显卡驱动，可以尝试使用 NVIDIA GeForce Experience 进行更新，之后再在 `NVIDIA 控制面板` 查看支持的 CUDA 版本。\n\n\n确定要下载的版本后，在此网站 [CUDA Toolkit Archive](https://developer.nvidia.cn/cuda-toolkit-archive) 找到你需要的版本，我这里选择了 CUDA 11.1.1 的版本，并且按照下图选择了对应我系统的安装包：\n\n\n\n![image-20220826192316090](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220826192316090.png)\n\n\n\n打开安装包，安装过程中选择自定义安装，然后查看下图中的项目，如果当前版本大于新版本则将此项取消勾选，反之则可以勾选。\n\n\n\n![image-20220826192856077](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220826192856077.png)\n\n\n\n安装完成后打开 Powershell 输入 `nvcc -V` 查看是否成功安装：\n\n![image-20220826210836346](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220826210836346.png)\n\n\n\n然后是 cuDNN(针对于神经网络应用加速的库)，进入网站 [cudnn-download](https://developer.nvidia.cn/rdp/cudnn-download) ，这里需要先注册一个 NVIDIA 账号，然后选择对应的版本进行下载。\n\n将下载的压缩包解压，将里面三个文件夹直接放入之前安装的 CUDA 目录中。\n\n\n\n![image-20220826211547617](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220826211547617.png)\n\n\n\n验证一下是否安装成功，打开 Powershell 输入 `nvidia-smi`\n\n\n\n![image-20220826211925113](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220826211925113.png)\n\n\n可以看到安装是成功的","slug":"Windows 如何正确安装 cuda 与 cudnn","published":1,"updated":"2022-08-27T12:44:02.226Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clgyiartx000bjku77uuh8ac2","content":"<h1 id=\"Windows-如何正确安装-cuda-与-cudnn\"><a href=\"#Windows-如何正确安装-cuda-与-cudnn\" class=\"headerlink\" title=\"Windows 如何正确安装 cuda 与 cudnn\"></a>Windows 如何正确安装 cuda 与 cudnn</h1><p>首先要了解到的信息是，你的 Nvidia 显卡到底支持多高版本的 CUDA? 在系统状态栏中找到 <code>NVIDIA 控制面板</code>，查看 <code>[帮助]-[系统信息]-[组件]-[NVCUDA64.DLL]</code> 查看后面的版本，你需要安装的 CUDA 不得高于此版本。 </p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220826191251461.png\" alt=\"image-20220826191251461\"></p>\n<p>但是这里要注意的是，并不是你的显卡型号决定了你支持的 CUDA 版本，而是你的显卡和显卡驱动共同决定的。想要更新你的显卡驱动，可以尝试使用 NVIDIA GeForce Experience 进行更新，之后再在 <code>NVIDIA 控制面板</code> 查看支持的 CUDA 版本。</p>\n<p>确定要下载的版本后，在此网站 <a href=\"https://developer.nvidia.cn/cuda-toolkit-archive\">CUDA Toolkit Archive</a> 找到你需要的版本，我这里选择了 CUDA 11.1.1 的版本，并且按照下图选择了对应我系统的安装包：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220826192316090.png\" alt=\"image-20220826192316090\"></p>\n<p>打开安装包，安装过程中选择自定义安装，然后查看下图中的项目，如果当前版本大于新版本则将此项取消勾选，反之则可以勾选。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220826192856077.png\" alt=\"image-20220826192856077\"></p>\n<p>安装完成后打开 Powershell 输入 <code>nvcc -V</code> 查看是否成功安装：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220826210836346.png\" alt=\"image-20220826210836346\"></p>\n<p>然后是 cuDNN(针对于神经网络应用加速的库)，进入网站 <a href=\"https://developer.nvidia.cn/rdp/cudnn-download\">cudnn-download</a> ，这里需要先注册一个 NVIDIA 账号，然后选择对应的版本进行下载。</p>\n<p>将下载的压缩包解压，将里面三个文件夹直接放入之前安装的 CUDA 目录中。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220826211547617.png\" alt=\"image-20220826211547617\"></p>\n<p>验证一下是否安装成功，打开 Powershell 输入 <code>nvidia-smi</code></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220826211925113.png\" alt=\"image-20220826211925113\"></p>\n<p>可以看到安装是成功的</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Windows-如何正确安装-cuda-与-cudnn\"><a href=\"#Windows-如何正确安装-cuda-与-cudnn\" class=\"headerlink\" title=\"Windows 如何正确安装 cuda 与 cudnn\"></a>Windows 如何正确安装 cuda 与 cudnn</h1><p>首先要了解到的信息是，你的 Nvidia 显卡到底支持多高版本的 CUDA? 在系统状态栏中找到 <code>NVIDIA 控制面板</code>，查看 <code>[帮助]-[系统信息]-[组件]-[NVCUDA64.DLL]</code> 查看后面的版本，你需要安装的 CUDA 不得高于此版本。 </p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220826191251461.png\" alt=\"image-20220826191251461\"></p>\n<p>但是这里要注意的是，并不是你的显卡型号决定了你支持的 CUDA 版本，而是你的显卡和显卡驱动共同决定的。想要更新你的显卡驱动，可以尝试使用 NVIDIA GeForce Experience 进行更新，之后再在 <code>NVIDIA 控制面板</code> 查看支持的 CUDA 版本。</p>\n<p>确定要下载的版本后，在此网站 <a href=\"https://developer.nvidia.cn/cuda-toolkit-archive\">CUDA Toolkit Archive</a> 找到你需要的版本，我这里选择了 CUDA 11.1.1 的版本，并且按照下图选择了对应我系统的安装包：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220826192316090.png\" alt=\"image-20220826192316090\"></p>\n<p>打开安装包，安装过程中选择自定义安装，然后查看下图中的项目，如果当前版本大于新版本则将此项取消勾选，反之则可以勾选。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220826192856077.png\" alt=\"image-20220826192856077\"></p>\n<p>安装完成后打开 Powershell 输入 <code>nvcc -V</code> 查看是否成功安装：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220826210836346.png\" alt=\"image-20220826210836346\"></p>\n<p>然后是 cuDNN(针对于神经网络应用加速的库)，进入网站 <a href=\"https://developer.nvidia.cn/rdp/cudnn-download\">cudnn-download</a> ，这里需要先注册一个 NVIDIA 账号，然后选择对应的版本进行下载。</p>\n<p>将下载的压缩包解压，将里面三个文件夹直接放入之前安装的 CUDA 目录中。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220826211547617.png\" alt=\"image-20220826211547617\"></p>\n<p>验证一下是否安装成功，打开 Powershell 输入 <code>nvidia-smi</code></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220826211925113.png\" alt=\"image-20220826211925113\"></p>\n<p>可以看到安装是成功的</p>\n"},{"title":"Windows 无法修改代理设置的解决方法","date":"2022-08-25T13:25:08.000Z","_content":"\n最近碰到很头疼的问题：我的系统代理打不开了。\n\n问题出现在几周前，我出去旅游了一趟，回来的时候突然代理就无法使用了，当时我以为只是订阅的网站忘记续费了，打开之后发现还有余额。那么我又以为是游戏加速器导致的问题，于是重启了电脑，但还是无法使用。奇怪的是，在 clash 软件里面显示的节点都是可以 ping 通的。这我就很纳闷了：明明节点没有任何问题，为什么没法走代理？而且我不管打开什么网页，软件里都没有显示有任何一点的流量通过。\n\n此时补充一下配置：我使用的是 win11 的系统，平时浏览器多使用火狐，系统里 edge 和 chrome 也都有。这次出问题时我使用的是火狐浏览器。此时，我立马想到的是先用其他浏览器试试，当然，其它的浏览器也没法连接到外网。\n\n这时候，我首先怀疑的问题就是系统代理没有正常打开。于是我就来到 windows 的 [设置]-[网络和Internet]-[代理] 处，查看了一下代理的情况。原来代理状态是关闭的，那我就手动打开，填入地址 `127.0.0.1` 和端口号 `7890`，并且保存，然后尝试连接，结果没有任何变化，依然无法连接外网。这时我瞟了一眼设置的代理，欸，为什么又关闭了，明明我是刚才打开并保存的。于是我再次打开，然而我只要一切换界面，回去一看，又自动关闭了，这也太奇怪了。\n\n我从来没有见过这种情况，便上网搜索一番，在 github 上找到了一个 clash for windows 项目的 [issue](https://github.com/Fndroid/clash_for_windows_pkg/issues/312) 被很多人提到，讲的就是系统代理自动被关闭的情况。按照该 issue 下方各路大神提供的方法，我使用 `Process Monitor` 观察修改 `HKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\\` 项目的应用。但令人头疼的是，我始终只能在该软件里看到我自己修改的动作，却没有观察到任何其他的软件对该注册表进行修改。我百思不得其解：既然我修改了，又被改回去了，为什么找不到是什么东西修改了它呢？当时已经是比较晚的时候了，我出于无奈，只能先将火狐浏览器的代理手动设置成 `127.0.0.1:7890`，能凑合用，把系统代理的问题留到后面解决。\n\n本来我想着可能我也不需要在浏览器之外的软件里使用代理，然而几天后我有安装 python 库的需求，当时我像往常一样输入 `pip install ...`，结果报错 `ValueError: check_hostname requires server_hostname`。这是挺常见的，一般来说是系统设置了代理的原因。我习惯性地准备关闭 clash，可是我突然一想，不对啊，我的系统代理不是本来就没有打开吗？一时半会我还真没想明白，那我就把 vscode 设置从代理走试试？结果又报了另外的错误，似乎也是连接上的错误。\n\n于是，我想在 powershell 里用命令查看一下我的网络代理情况，使用指令 `netsh winhttp show proxy` 结果是 `直接连接（无代理）` 那我的代理确实没有打开呀。如果我想要用指令打开呢？我尝试了指令 `netsh winhttp set proxy '127.0.0.1:7890'`，再次查看状态，依然显示直连，完全没有变化，看来这时候系统代理无法修改的问题依然存在。我只能又去搜搜看遇到 pip 无法安装的时候大伙都是怎么解决的，搜出来一个修改注册表项的方法：将 `HKEY_CURRENT_USER\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Internet Settings` 中的 `ProxyEnable` 项值修改为 0。这样做居然临时解决了问题。\n\n当然，临时解决了问题，却没有完全修复问题。我的系统代理目前是处于完全没有办法配置的情况，这势必在后面给我带来各种各样的麻烦，我肯定要找方法完全解决。于是又是各种各样地找解决方案，终于有一条修改注册表项的方法：修改 `HKEY_LOCAL_MACHINE\\SOFTWARE\\Policies\\Microsoft\\Windows\\CurrentVersion\\Internet Settings` 中的 `ProxySettingsPerUser` 为 1，这样我的代理设置就神奇地可以修改了，直接用 clash 软件打开系统代理即可。当然这时候要把之前上一步的注册表项 `ProxyEnable` 恢复为 1 才行。\n\n踩了不少坑，系统才恢复正常。其实这个问题很让我意外的地方是，解决问题的中途我因为系统还残留一些其他的问题，将系统重装为 win10，然后正好还换了一块全新的主板，结果重装之后过一阵子居然重现了这个问题，这就让我感到很不理解了，理论上重装不是能解决 100% 的问题吗？虽然我最终也没有想出来问题发生的根源在何处，但是我猜测是我常用的某款软件修改了最后的这个注册表项，毕竟我重装了系统，常用的软件也不能落下，那么可能就有一款软件导致了这样的问题发生。在此记录下解决问题的全过程，供之后参考。","source":"_posts/windows 无法修改代理设置的解决方法.md","raw":"---\ntitle: Windows 无法修改代理设置的解决方法\ndate: 2022-08-25 21:25:08\ntags:\n---\n\n最近碰到很头疼的问题：我的系统代理打不开了。\n\n问题出现在几周前，我出去旅游了一趟，回来的时候突然代理就无法使用了，当时我以为只是订阅的网站忘记续费了，打开之后发现还有余额。那么我又以为是游戏加速器导致的问题，于是重启了电脑，但还是无法使用。奇怪的是，在 clash 软件里面显示的节点都是可以 ping 通的。这我就很纳闷了：明明节点没有任何问题，为什么没法走代理？而且我不管打开什么网页，软件里都没有显示有任何一点的流量通过。\n\n此时补充一下配置：我使用的是 win11 的系统，平时浏览器多使用火狐，系统里 edge 和 chrome 也都有。这次出问题时我使用的是火狐浏览器。此时，我立马想到的是先用其他浏览器试试，当然，其它的浏览器也没法连接到外网。\n\n这时候，我首先怀疑的问题就是系统代理没有正常打开。于是我就来到 windows 的 [设置]-[网络和Internet]-[代理] 处，查看了一下代理的情况。原来代理状态是关闭的，那我就手动打开，填入地址 `127.0.0.1` 和端口号 `7890`，并且保存，然后尝试连接，结果没有任何变化，依然无法连接外网。这时我瞟了一眼设置的代理，欸，为什么又关闭了，明明我是刚才打开并保存的。于是我再次打开，然而我只要一切换界面，回去一看，又自动关闭了，这也太奇怪了。\n\n我从来没有见过这种情况，便上网搜索一番，在 github 上找到了一个 clash for windows 项目的 [issue](https://github.com/Fndroid/clash_for_windows_pkg/issues/312) 被很多人提到，讲的就是系统代理自动被关闭的情况。按照该 issue 下方各路大神提供的方法，我使用 `Process Monitor` 观察修改 `HKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\\` 项目的应用。但令人头疼的是，我始终只能在该软件里看到我自己修改的动作，却没有观察到任何其他的软件对该注册表进行修改。我百思不得其解：既然我修改了，又被改回去了，为什么找不到是什么东西修改了它呢？当时已经是比较晚的时候了，我出于无奈，只能先将火狐浏览器的代理手动设置成 `127.0.0.1:7890`，能凑合用，把系统代理的问题留到后面解决。\n\n本来我想着可能我也不需要在浏览器之外的软件里使用代理，然而几天后我有安装 python 库的需求，当时我像往常一样输入 `pip install ...`，结果报错 `ValueError: check_hostname requires server_hostname`。这是挺常见的，一般来说是系统设置了代理的原因。我习惯性地准备关闭 clash，可是我突然一想，不对啊，我的系统代理不是本来就没有打开吗？一时半会我还真没想明白，那我就把 vscode 设置从代理走试试？结果又报了另外的错误，似乎也是连接上的错误。\n\n于是，我想在 powershell 里用命令查看一下我的网络代理情况，使用指令 `netsh winhttp show proxy` 结果是 `直接连接（无代理）` 那我的代理确实没有打开呀。如果我想要用指令打开呢？我尝试了指令 `netsh winhttp set proxy '127.0.0.1:7890'`，再次查看状态，依然显示直连，完全没有变化，看来这时候系统代理无法修改的问题依然存在。我只能又去搜搜看遇到 pip 无法安装的时候大伙都是怎么解决的，搜出来一个修改注册表项的方法：将 `HKEY_CURRENT_USER\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Internet Settings` 中的 `ProxyEnable` 项值修改为 0。这样做居然临时解决了问题。\n\n当然，临时解决了问题，却没有完全修复问题。我的系统代理目前是处于完全没有办法配置的情况，这势必在后面给我带来各种各样的麻烦，我肯定要找方法完全解决。于是又是各种各样地找解决方案，终于有一条修改注册表项的方法：修改 `HKEY_LOCAL_MACHINE\\SOFTWARE\\Policies\\Microsoft\\Windows\\CurrentVersion\\Internet Settings` 中的 `ProxySettingsPerUser` 为 1，这样我的代理设置就神奇地可以修改了，直接用 clash 软件打开系统代理即可。当然这时候要把之前上一步的注册表项 `ProxyEnable` 恢复为 1 才行。\n\n踩了不少坑，系统才恢复正常。其实这个问题很让我意外的地方是，解决问题的中途我因为系统还残留一些其他的问题，将系统重装为 win10，然后正好还换了一块全新的主板，结果重装之后过一阵子居然重现了这个问题，这就让我感到很不理解了，理论上重装不是能解决 100% 的问题吗？虽然我最终也没有想出来问题发生的根源在何处，但是我猜测是我常用的某款软件修改了最后的这个注册表项，毕竟我重装了系统，常用的软件也不能落下，那么可能就有一款软件导致了这样的问题发生。在此记录下解决问题的全过程，供之后参考。","slug":"windows 无法修改代理设置的解决方法","published":1,"updated":"2022-08-25T22:19:23.133Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clgyiartx000cjku7cdx3c5ge","content":"<p>最近碰到很头疼的问题：我的系统代理打不开了。</p>\n<p>问题出现在几周前，我出去旅游了一趟，回来的时候突然代理就无法使用了，当时我以为只是订阅的网站忘记续费了，打开之后发现还有余额。那么我又以为是游戏加速器导致的问题，于是重启了电脑，但还是无法使用。奇怪的是，在 clash 软件里面显示的节点都是可以 ping 通的。这我就很纳闷了：明明节点没有任何问题，为什么没法走代理？而且我不管打开什么网页，软件里都没有显示有任何一点的流量通过。</p>\n<p>此时补充一下配置：我使用的是 win11 的系统，平时浏览器多使用火狐，系统里 edge 和 chrome 也都有。这次出问题时我使用的是火狐浏览器。此时，我立马想到的是先用其他浏览器试试，当然，其它的浏览器也没法连接到外网。</p>\n<p>这时候，我首先怀疑的问题就是系统代理没有正常打开。于是我就来到 windows 的 [设置]-[网络和Internet]-[代理] 处，查看了一下代理的情况。原来代理状态是关闭的，那我就手动打开，填入地址 <code>127.0.0.1</code> 和端口号 <code>7890</code>，并且保存，然后尝试连接，结果没有任何变化，依然无法连接外网。这时我瞟了一眼设置的代理，欸，为什么又关闭了，明明我是刚才打开并保存的。于是我再次打开，然而我只要一切换界面，回去一看，又自动关闭了，这也太奇怪了。</p>\n<p>我从来没有见过这种情况，便上网搜索一番，在 github 上找到了一个 clash for windows 项目的 <a href=\"https://github.com/Fndroid/clash_for_windows_pkg/issues/312\">issue</a> 被很多人提到，讲的就是系统代理自动被关闭的情况。按照该 issue 下方各路大神提供的方法，我使用 <code>Process Monitor</code> 观察修改 <code>HKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\\</code> 项目的应用。但令人头疼的是，我始终只能在该软件里看到我自己修改的动作，却没有观察到任何其他的软件对该注册表进行修改。我百思不得其解：既然我修改了，又被改回去了，为什么找不到是什么东西修改了它呢？当时已经是比较晚的时候了，我出于无奈，只能先将火狐浏览器的代理手动设置成 <code>127.0.0.1:7890</code>，能凑合用，把系统代理的问题留到后面解决。</p>\n<p>本来我想着可能我也不需要在浏览器之外的软件里使用代理，然而几天后我有安装 python 库的需求，当时我像往常一样输入 <code>pip install ...</code>，结果报错 <code>ValueError: check_hostname requires server_hostname</code>。这是挺常见的，一般来说是系统设置了代理的原因。我习惯性地准备关闭 clash，可是我突然一想，不对啊，我的系统代理不是本来就没有打开吗？一时半会我还真没想明白，那我就把 vscode 设置从代理走试试？结果又报了另外的错误，似乎也是连接上的错误。</p>\n<p>于是，我想在 powershell 里用命令查看一下我的网络代理情况，使用指令 <code>netsh winhttp show proxy</code> 结果是 <code>直接连接（无代理）</code> 那我的代理确实没有打开呀。如果我想要用指令打开呢？我尝试了指令 <code>netsh winhttp set proxy &#39;127.0.0.1:7890&#39;</code>，再次查看状态，依然显示直连，完全没有变化，看来这时候系统代理无法修改的问题依然存在。我只能又去搜搜看遇到 pip 无法安装的时候大伙都是怎么解决的，搜出来一个修改注册表项的方法：将 <code>HKEY_CURRENT_USER\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Internet Settings</code> 中的 <code>ProxyEnable</code> 项值修改为 0。这样做居然临时解决了问题。</p>\n<p>当然，临时解决了问题，却没有完全修复问题。我的系统代理目前是处于完全没有办法配置的情况，这势必在后面给我带来各种各样的麻烦，我肯定要找方法完全解决。于是又是各种各样地找解决方案，终于有一条修改注册表项的方法：修改 <code>HKEY_LOCAL_MACHINE\\SOFTWARE\\Policies\\Microsoft\\Windows\\CurrentVersion\\Internet Settings</code> 中的 <code>ProxySettingsPerUser</code> 为 1，这样我的代理设置就神奇地可以修改了，直接用 clash 软件打开系统代理即可。当然这时候要把之前上一步的注册表项 <code>ProxyEnable</code> 恢复为 1 才行。</p>\n<p>踩了不少坑，系统才恢复正常。其实这个问题很让我意外的地方是，解决问题的中途我因为系统还残留一些其他的问题，将系统重装为 win10，然后正好还换了一块全新的主板，结果重装之后过一阵子居然重现了这个问题，这就让我感到很不理解了，理论上重装不是能解决 100% 的问题吗？虽然我最终也没有想出来问题发生的根源在何处，但是我猜测是我常用的某款软件修改了最后的这个注册表项，毕竟我重装了系统，常用的软件也不能落下，那么可能就有一款软件导致了这样的问题发生。在此记录下解决问题的全过程，供之后参考。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>最近碰到很头疼的问题：我的系统代理打不开了。</p>\n<p>问题出现在几周前，我出去旅游了一趟，回来的时候突然代理就无法使用了，当时我以为只是订阅的网站忘记续费了，打开之后发现还有余额。那么我又以为是游戏加速器导致的问题，于是重启了电脑，但还是无法使用。奇怪的是，在 clash 软件里面显示的节点都是可以 ping 通的。这我就很纳闷了：明明节点没有任何问题，为什么没法走代理？而且我不管打开什么网页，软件里都没有显示有任何一点的流量通过。</p>\n<p>此时补充一下配置：我使用的是 win11 的系统，平时浏览器多使用火狐，系统里 edge 和 chrome 也都有。这次出问题时我使用的是火狐浏览器。此时，我立马想到的是先用其他浏览器试试，当然，其它的浏览器也没法连接到外网。</p>\n<p>这时候，我首先怀疑的问题就是系统代理没有正常打开。于是我就来到 windows 的 [设置]-[网络和Internet]-[代理] 处，查看了一下代理的情况。原来代理状态是关闭的，那我就手动打开，填入地址 <code>127.0.0.1</code> 和端口号 <code>7890</code>，并且保存，然后尝试连接，结果没有任何变化，依然无法连接外网。这时我瞟了一眼设置的代理，欸，为什么又关闭了，明明我是刚才打开并保存的。于是我再次打开，然而我只要一切换界面，回去一看，又自动关闭了，这也太奇怪了。</p>\n<p>我从来没有见过这种情况，便上网搜索一番，在 github 上找到了一个 clash for windows 项目的 <a href=\"https://github.com/Fndroid/clash_for_windows_pkg/issues/312\">issue</a> 被很多人提到，讲的就是系统代理自动被关闭的情况。按照该 issue 下方各路大神提供的方法，我使用 <code>Process Monitor</code> 观察修改 <code>HKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\\</code> 项目的应用。但令人头疼的是，我始终只能在该软件里看到我自己修改的动作，却没有观察到任何其他的软件对该注册表进行修改。我百思不得其解：既然我修改了，又被改回去了，为什么找不到是什么东西修改了它呢？当时已经是比较晚的时候了，我出于无奈，只能先将火狐浏览器的代理手动设置成 <code>127.0.0.1:7890</code>，能凑合用，把系统代理的问题留到后面解决。</p>\n<p>本来我想着可能我也不需要在浏览器之外的软件里使用代理，然而几天后我有安装 python 库的需求，当时我像往常一样输入 <code>pip install ...</code>，结果报错 <code>ValueError: check_hostname requires server_hostname</code>。这是挺常见的，一般来说是系统设置了代理的原因。我习惯性地准备关闭 clash，可是我突然一想，不对啊，我的系统代理不是本来就没有打开吗？一时半会我还真没想明白，那我就把 vscode 设置从代理走试试？结果又报了另外的错误，似乎也是连接上的错误。</p>\n<p>于是，我想在 powershell 里用命令查看一下我的网络代理情况，使用指令 <code>netsh winhttp show proxy</code> 结果是 <code>直接连接（无代理）</code> 那我的代理确实没有打开呀。如果我想要用指令打开呢？我尝试了指令 <code>netsh winhttp set proxy &#39;127.0.0.1:7890&#39;</code>，再次查看状态，依然显示直连，完全没有变化，看来这时候系统代理无法修改的问题依然存在。我只能又去搜搜看遇到 pip 无法安装的时候大伙都是怎么解决的，搜出来一个修改注册表项的方法：将 <code>HKEY_CURRENT_USER\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Internet Settings</code> 中的 <code>ProxyEnable</code> 项值修改为 0。这样做居然临时解决了问题。</p>\n<p>当然，临时解决了问题，却没有完全修复问题。我的系统代理目前是处于完全没有办法配置的情况，这势必在后面给我带来各种各样的麻烦，我肯定要找方法完全解决。于是又是各种各样地找解决方案，终于有一条修改注册表项的方法：修改 <code>HKEY_LOCAL_MACHINE\\SOFTWARE\\Policies\\Microsoft\\Windows\\CurrentVersion\\Internet Settings</code> 中的 <code>ProxySettingsPerUser</code> 为 1，这样我的代理设置就神奇地可以修改了，直接用 clash 软件打开系统代理即可。当然这时候要把之前上一步的注册表项 <code>ProxyEnable</code> 恢复为 1 才行。</p>\n<p>踩了不少坑，系统才恢复正常。其实这个问题很让我意外的地方是，解决问题的中途我因为系统还残留一些其他的问题，将系统重装为 win10，然后正好还换了一块全新的主板，结果重装之后过一阵子居然重现了这个问题，这就让我感到很不理解了，理论上重装不是能解决 100% 的问题吗？虽然我最终也没有想出来问题发生的根源在何处，但是我猜测是我常用的某款软件修改了最后的这个注册表项，毕竟我重装了系统，常用的软件也不能落下，那么可能就有一款软件导致了这样的问题发生。在此记录下解决问题的全过程，供之后参考。</p>\n"},{"title":"在树莓派上从源码编译安装 Paddlepaddle","date":"2023-03-17T15:09:08.000Z","mathjax":false,"_content":"\n在树莓派上没有办法直接 `pip install paddlepaddle`，需要从源码编译安装，这里踩了很多的坑，于是记录于此。\n\n(这篇文章是从我的 github 上搬下来的，仓库[在此](https://github.com/1099255210/Paddle-raspberry-pi-64-bit))\n\n## 快速上手（从本人编译好的.whl安装）\n\n安装要求：\n\n硬件与系统： `Raspberry Pi 4B 64-bit(aarch64/armv8)`\n\nPython版本： `Python=3.9`\n\n下载 `.whl` 安装包 [Paddle2.4-Raspberry-pi-64bit](https://github.com/1099255210/Paddle-raspberry-pi-64-bit/releases/download/2.4/paddlepaddle-0.0.0-cp39-cp39-linux_aarch64.whl)\n\n```shell\npip install paddlepaddle-0.0.0-cp39-cp39-linux_aarch64.whl\n```\n\n安装完成后检查是否安装成功：\n\n```shell\npython\n\n>>> import paddle\n>>> paddle.utils.run_check()\n```\n\n如果看到 `PaddlePaddle is installed successfully!`， 说明安装成功。\n\n## 详细信息\n\n编译这个安装包的系统如下：\n\n```shell\naarch64\nPRETTY_NAME=\"Debian GNU/Linux 11 (bullseye)\"\nNAME=\"Debian GNU/Linux\"\nVERSION_ID=\"11\"\nVERSION=\"11 (bullseye)\"\nVERSION_CODENAME=bullseye\nID=debian\nHOME_URL=\"https://www.debian.org/\"\nSUPPORT_URL=\"https://www.debian.org/support\"\nBUG_REPORT_URL=\"https://bugs.debian.org/\"\n```\n\n用下面这行命令查看你的操作系统：\n\n```shell\nuname -m && cat /etc/*release\n```\n\n查看你的 Python 版本：\n\n```shell\npython -c \"import sys; print(sys.version)\"\n```\n\n本安装包需要 Python=3.9\n\n## 编译指南\n\n树莓派 4B 64位 系统镜像：[下载链接](https://downloads.raspberrypi.org/raspios_arm64/images/raspios_arm64-2023-02-22/2023-02-21-raspios-bullseye-arm64.img.xz)\n\nPaddlePaddle 编译流程基本参考 [飞桨官方源码编译指南](https://www.paddlepaddle.org.cn/install/quick?docurl=/documentation/docs/zh/install/compile/linux-compile.html)\n\n对于 树莓派4B 64bit 而言，下面是可以参考的指南：\n\n安装 CMAKE, protobuf, patchelf:\n\n```shell\nsudo apt install cmake patchelf\npip install protobuf\n```\n\n克隆 `PaddlePaddle` 仓库：\n\n```shell\ngit clone https://github.com/PaddlePaddle/Paddle.git && cd Paddle \n```\n\n切换到 `develop` 分支, 新建 `build` 目录：\n\n```shell\ngit checkout develop && mkdir build && cd build\n```\n\n开始编译：\n\n```shell\ncmake .. -DPY_VERSION=3 -DPYTHON_EXECUTABLE=`which python3` -DWITH_ARM=ON -DWITH_GPU=OFF -DON_INFER=ON -DWITH_XBYAK=OFF\nulimit -n 8192\nmake TARGET=ARMV8 -j$(nproc)\n```\n\n这里有一些需要注意的点：\n\n- make 过程中可能会需要克隆一些仓库，如果速度特别慢可以考虑更换 git 代理。\n- 多线程编译会导致一些问题，中途会报错，此时请切换到单线程（`make TARGET=ARMV8`）继续。\n- 单线程编译到约 90% 以上时会报错，此时再切换到多线程编译，直到出错后再切换回单线程，可以进行至编译成功。\n- 整个编译过程可能长达一整天，请留足时间。\n\n如果在编译中遇到了其它问题，请参考以下流程：\n\n1. 在 google 必应等平台搜索（适用于普遍问题）\n2. 在 paddlepaddle 的[官方仓库的issues](https://github.com/PaddlePaddle/Paddle/issues)中寻找关键字。(可参考的关键词：`aarch64/armv8`)\n3. 以上都无法寻找到答案，请在本仓库提出 `issue`，如果有我们碰到的问题可以帮助解答。","source":"_posts/在树莓派上从源码编译安装Paddlepaddle.md","raw":"---\ntitle: 在树莓派上从源码编译安装 Paddlepaddle\ndate: 2023-03-17 23:09:08\ntags:\nmathjax: false\n---\n\n在树莓派上没有办法直接 `pip install paddlepaddle`，需要从源码编译安装，这里踩了很多的坑，于是记录于此。\n\n(这篇文章是从我的 github 上搬下来的，仓库[在此](https://github.com/1099255210/Paddle-raspberry-pi-64-bit))\n\n## 快速上手（从本人编译好的.whl安装）\n\n安装要求：\n\n硬件与系统： `Raspberry Pi 4B 64-bit(aarch64/armv8)`\n\nPython版本： `Python=3.9`\n\n下载 `.whl` 安装包 [Paddle2.4-Raspberry-pi-64bit](https://github.com/1099255210/Paddle-raspberry-pi-64-bit/releases/download/2.4/paddlepaddle-0.0.0-cp39-cp39-linux_aarch64.whl)\n\n```shell\npip install paddlepaddle-0.0.0-cp39-cp39-linux_aarch64.whl\n```\n\n安装完成后检查是否安装成功：\n\n```shell\npython\n\n>>> import paddle\n>>> paddle.utils.run_check()\n```\n\n如果看到 `PaddlePaddle is installed successfully!`， 说明安装成功。\n\n## 详细信息\n\n编译这个安装包的系统如下：\n\n```shell\naarch64\nPRETTY_NAME=\"Debian GNU/Linux 11 (bullseye)\"\nNAME=\"Debian GNU/Linux\"\nVERSION_ID=\"11\"\nVERSION=\"11 (bullseye)\"\nVERSION_CODENAME=bullseye\nID=debian\nHOME_URL=\"https://www.debian.org/\"\nSUPPORT_URL=\"https://www.debian.org/support\"\nBUG_REPORT_URL=\"https://bugs.debian.org/\"\n```\n\n用下面这行命令查看你的操作系统：\n\n```shell\nuname -m && cat /etc/*release\n```\n\n查看你的 Python 版本：\n\n```shell\npython -c \"import sys; print(sys.version)\"\n```\n\n本安装包需要 Python=3.9\n\n## 编译指南\n\n树莓派 4B 64位 系统镜像：[下载链接](https://downloads.raspberrypi.org/raspios_arm64/images/raspios_arm64-2023-02-22/2023-02-21-raspios-bullseye-arm64.img.xz)\n\nPaddlePaddle 编译流程基本参考 [飞桨官方源码编译指南](https://www.paddlepaddle.org.cn/install/quick?docurl=/documentation/docs/zh/install/compile/linux-compile.html)\n\n对于 树莓派4B 64bit 而言，下面是可以参考的指南：\n\n安装 CMAKE, protobuf, patchelf:\n\n```shell\nsudo apt install cmake patchelf\npip install protobuf\n```\n\n克隆 `PaddlePaddle` 仓库：\n\n```shell\ngit clone https://github.com/PaddlePaddle/Paddle.git && cd Paddle \n```\n\n切换到 `develop` 分支, 新建 `build` 目录：\n\n```shell\ngit checkout develop && mkdir build && cd build\n```\n\n开始编译：\n\n```shell\ncmake .. -DPY_VERSION=3 -DPYTHON_EXECUTABLE=`which python3` -DWITH_ARM=ON -DWITH_GPU=OFF -DON_INFER=ON -DWITH_XBYAK=OFF\nulimit -n 8192\nmake TARGET=ARMV8 -j$(nproc)\n```\n\n这里有一些需要注意的点：\n\n- make 过程中可能会需要克隆一些仓库，如果速度特别慢可以考虑更换 git 代理。\n- 多线程编译会导致一些问题，中途会报错，此时请切换到单线程（`make TARGET=ARMV8`）继续。\n- 单线程编译到约 90% 以上时会报错，此时再切换到多线程编译，直到出错后再切换回单线程，可以进行至编译成功。\n- 整个编译过程可能长达一整天，请留足时间。\n\n如果在编译中遇到了其它问题，请参考以下流程：\n\n1. 在 google 必应等平台搜索（适用于普遍问题）\n2. 在 paddlepaddle 的[官方仓库的issues](https://github.com/PaddlePaddle/Paddle/issues)中寻找关键字。(可参考的关键词：`aarch64/armv8`)\n3. 以上都无法寻找到答案，请在本仓库提出 `issue`，如果有我们碰到的问题可以帮助解答。","slug":"在树莓派上从源码编译安装Paddlepaddle","published":1,"updated":"2023-04-27T02:24:09.272Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clgyiarty000djku7d2an0mah","content":"<p>在树莓派上没有办法直接 <code>pip install paddlepaddle</code>，需要从源码编译安装，这里踩了很多的坑，于是记录于此。</p>\n<p>(这篇文章是从我的 github 上搬下来的，仓库<a href=\"https://github.com/1099255210/Paddle-raspberry-pi-64-bit\">在此</a>)</p>\n<h2 id=\"快速上手（从本人编译好的-whl安装）\"><a href=\"#快速上手（从本人编译好的-whl安装）\" class=\"headerlink\" title=\"快速上手（从本人编译好的.whl安装）\"></a>快速上手（从本人编译好的.whl安装）</h2><p>安装要求：</p>\n<p>硬件与系统： <code>Raspberry Pi 4B 64-bit(aarch64/armv8)</code></p>\n<p>Python版本： <code>Python=3.9</code></p>\n<p>下载 <code>.whl</code> 安装包 <a href=\"https://github.com/1099255210/Paddle-raspberry-pi-64-bit/releases/download/2.4/paddlepaddle-0.0.0-cp39-cp39-linux_aarch64.whl\">Paddle2.4-Raspberry-pi-64bit</a></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install paddlepaddle-0.0.0-cp39-cp39-linux_aarch64.whl</span><br></pre></td></tr></table></figure>\n<p>安装完成后检查是否安装成功：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; import paddle</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; paddle.utils.run_check()</span></span><br></pre></td></tr></table></figure>\n<p>如果看到 <code>PaddlePaddle is installed successfully!</code>， 说明安装成功。</p>\n<h2 id=\"详细信息\"><a href=\"#详细信息\" class=\"headerlink\" title=\"详细信息\"></a>详细信息</h2><p>编译这个安装包的系统如下：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">aarch64</span><br><span class=\"line\">PRETTY_NAME=&quot;Debian GNU/Linux 11 (bullseye)&quot;</span><br><span class=\"line\">NAME=&quot;Debian GNU/Linux&quot;</span><br><span class=\"line\">VERSION_ID=&quot;11&quot;</span><br><span class=\"line\">VERSION=&quot;11 (bullseye)&quot;</span><br><span class=\"line\">VERSION_CODENAME=bullseye</span><br><span class=\"line\">ID=debian</span><br><span class=\"line\">HOME_URL=&quot;https://www.debian.org/&quot;</span><br><span class=\"line\">SUPPORT_URL=&quot;https://www.debian.org/support&quot;</span><br><span class=\"line\">BUG_REPORT_URL=&quot;https://bugs.debian.org/&quot;</span><br></pre></td></tr></table></figure>\n<p>用下面这行命令查看你的操作系统：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">uname -m &amp;&amp; cat /etc/*release</span><br></pre></td></tr></table></figure>\n<p>查看你的 Python 版本：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python -c &quot;import sys; print(sys.version)&quot;</span><br></pre></td></tr></table></figure>\n<p>本安装包需要 Python=3.9</p>\n<h2 id=\"编译指南\"><a href=\"#编译指南\" class=\"headerlink\" title=\"编译指南\"></a>编译指南</h2><p>树莓派 4B 64位 系统镜像：<a href=\"https://downloads.raspberrypi.org/raspios_arm64/images/raspios_arm64-2023-02-22/2023-02-21-raspios-bullseye-arm64.img.xz\">下载链接</a></p>\n<p>PaddlePaddle 编译流程基本参考 <a href=\"https://www.paddlepaddle.org.cn/install/quick?docurl=/documentation/docs/zh/install/compile/linux-compile.html\">飞桨官方源码编译指南</a></p>\n<p>对于 树莓派4B 64bit 而言，下面是可以参考的指南：</p>\n<p>安装 CMAKE, protobuf, patchelf:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt install cmake patchelf</span><br><span class=\"line\">pip install protobuf</span><br></pre></td></tr></table></figure>\n<p>克隆 <code>PaddlePaddle</code> 仓库：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone https://github.com/PaddlePaddle/Paddle.git &amp;&amp; cd Paddle </span><br></pre></td></tr></table></figure>\n<p>切换到 <code>develop</code> 分支, 新建 <code>build</code> 目录：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git checkout develop &amp;&amp; mkdir build &amp;&amp; cd build</span><br></pre></td></tr></table></figure>\n<p>开始编译：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cmake .. -DPY_VERSION=3 -DPYTHON_EXECUTABLE=`which python3` -DWITH_ARM=ON -DWITH_GPU=OFF -DON_INFER=ON -DWITH_XBYAK=OFF</span><br><span class=\"line\">ulimit -n 8192</span><br><span class=\"line\">make TARGET=ARMV8 -j$(nproc)</span><br></pre></td></tr></table></figure>\n<p>这里有一些需要注意的点：</p>\n<ul>\n<li>make 过程中可能会需要克隆一些仓库，如果速度特别慢可以考虑更换 git 代理。</li>\n<li>多线程编译会导致一些问题，中途会报错，此时请切换到单线程（<code>make TARGET=ARMV8</code>）继续。</li>\n<li>单线程编译到约 90% 以上时会报错，此时再切换到多线程编译，直到出错后再切换回单线程，可以进行至编译成功。</li>\n<li>整个编译过程可能长达一整天，请留足时间。</li>\n</ul>\n<p>如果在编译中遇到了其它问题，请参考以下流程：</p>\n<ol>\n<li>在 google 必应等平台搜索（适用于普遍问题）</li>\n<li>在 paddlepaddle 的<a href=\"https://github.com/PaddlePaddle/Paddle/issues\">官方仓库的issues</a>中寻找关键字。(可参考的关键词：<code>aarch64/armv8</code>)</li>\n<li>以上都无法寻找到答案，请在本仓库提出 <code>issue</code>，如果有我们碰到的问题可以帮助解答。</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p>在树莓派上没有办法直接 <code>pip install paddlepaddle</code>，需要从源码编译安装，这里踩了很多的坑，于是记录于此。</p>\n<p>(这篇文章是从我的 github 上搬下来的，仓库<a href=\"https://github.com/1099255210/Paddle-raspberry-pi-64-bit\">在此</a>)</p>\n<h2 id=\"快速上手（从本人编译好的-whl安装）\"><a href=\"#快速上手（从本人编译好的-whl安装）\" class=\"headerlink\" title=\"快速上手（从本人编译好的.whl安装）\"></a>快速上手（从本人编译好的.whl安装）</h2><p>安装要求：</p>\n<p>硬件与系统： <code>Raspberry Pi 4B 64-bit(aarch64/armv8)</code></p>\n<p>Python版本： <code>Python=3.9</code></p>\n<p>下载 <code>.whl</code> 安装包 <a href=\"https://github.com/1099255210/Paddle-raspberry-pi-64-bit/releases/download/2.4/paddlepaddle-0.0.0-cp39-cp39-linux_aarch64.whl\">Paddle2.4-Raspberry-pi-64bit</a></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install paddlepaddle-0.0.0-cp39-cp39-linux_aarch64.whl</span><br></pre></td></tr></table></figure>\n<p>安装完成后检查是否安装成功：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; import paddle</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; paddle.utils.run_check()</span></span><br></pre></td></tr></table></figure>\n<p>如果看到 <code>PaddlePaddle is installed successfully!</code>， 说明安装成功。</p>\n<h2 id=\"详细信息\"><a href=\"#详细信息\" class=\"headerlink\" title=\"详细信息\"></a>详细信息</h2><p>编译这个安装包的系统如下：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">aarch64</span><br><span class=\"line\">PRETTY_NAME=&quot;Debian GNU/Linux 11 (bullseye)&quot;</span><br><span class=\"line\">NAME=&quot;Debian GNU/Linux&quot;</span><br><span class=\"line\">VERSION_ID=&quot;11&quot;</span><br><span class=\"line\">VERSION=&quot;11 (bullseye)&quot;</span><br><span class=\"line\">VERSION_CODENAME=bullseye</span><br><span class=\"line\">ID=debian</span><br><span class=\"line\">HOME_URL=&quot;https://www.debian.org/&quot;</span><br><span class=\"line\">SUPPORT_URL=&quot;https://www.debian.org/support&quot;</span><br><span class=\"line\">BUG_REPORT_URL=&quot;https://bugs.debian.org/&quot;</span><br></pre></td></tr></table></figure>\n<p>用下面这行命令查看你的操作系统：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">uname -m &amp;&amp; cat /etc/*release</span><br></pre></td></tr></table></figure>\n<p>查看你的 Python 版本：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python -c &quot;import sys; print(sys.version)&quot;</span><br></pre></td></tr></table></figure>\n<p>本安装包需要 Python=3.9</p>\n<h2 id=\"编译指南\"><a href=\"#编译指南\" class=\"headerlink\" title=\"编译指南\"></a>编译指南</h2><p>树莓派 4B 64位 系统镜像：<a href=\"https://downloads.raspberrypi.org/raspios_arm64/images/raspios_arm64-2023-02-22/2023-02-21-raspios-bullseye-arm64.img.xz\">下载链接</a></p>\n<p>PaddlePaddle 编译流程基本参考 <a href=\"https://www.paddlepaddle.org.cn/install/quick?docurl=/documentation/docs/zh/install/compile/linux-compile.html\">飞桨官方源码编译指南</a></p>\n<p>对于 树莓派4B 64bit 而言，下面是可以参考的指南：</p>\n<p>安装 CMAKE, protobuf, patchelf:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt install cmake patchelf</span><br><span class=\"line\">pip install protobuf</span><br></pre></td></tr></table></figure>\n<p>克隆 <code>PaddlePaddle</code> 仓库：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone https://github.com/PaddlePaddle/Paddle.git &amp;&amp; cd Paddle </span><br></pre></td></tr></table></figure>\n<p>切换到 <code>develop</code> 分支, 新建 <code>build</code> 目录：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git checkout develop &amp;&amp; mkdir build &amp;&amp; cd build</span><br></pre></td></tr></table></figure>\n<p>开始编译：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cmake .. -DPY_VERSION=3 -DPYTHON_EXECUTABLE=`which python3` -DWITH_ARM=ON -DWITH_GPU=OFF -DON_INFER=ON -DWITH_XBYAK=OFF</span><br><span class=\"line\">ulimit -n 8192</span><br><span class=\"line\">make TARGET=ARMV8 -j$(nproc)</span><br></pre></td></tr></table></figure>\n<p>这里有一些需要注意的点：</p>\n<ul>\n<li>make 过程中可能会需要克隆一些仓库，如果速度特别慢可以考虑更换 git 代理。</li>\n<li>多线程编译会导致一些问题，中途会报错，此时请切换到单线程（<code>make TARGET=ARMV8</code>）继续。</li>\n<li>单线程编译到约 90% 以上时会报错，此时再切换到多线程编译，直到出错后再切换回单线程，可以进行至编译成功。</li>\n<li>整个编译过程可能长达一整天，请留足时间。</li>\n</ul>\n<p>如果在编译中遇到了其它问题，请参考以下流程：</p>\n<ol>\n<li>在 google 必应等平台搜索（适用于普遍问题）</li>\n<li>在 paddlepaddle 的<a href=\"https://github.com/PaddlePaddle/Paddle/issues\">官方仓库的issues</a>中寻找关键字。(可参考的关键词：<code>aarch64/armv8</code>)</li>\n<li>以上都无法寻找到答案，请在本仓库提出 <code>issue</code>，如果有我们碰到的问题可以帮助解答。</li>\n</ol>\n"},{"title":"如何使用 Squoosh 命令行批量压缩图片","date":"2022-09-22T15:09:08.000Z","mathjax":false,"_content":"\n先说明：windows 上无法批量压缩，原因：\n\nhttps://github.com/GoogleChromeLabs/squoosh/issues/973\n\n因为 squoosh-cli 暂不支持 windows 上的通配符。这么一个小问题，给我整崩溃了。\n\n完整过程如下\n\n先在 (https://squoosh.app) 上测试一下图片压缩的设置：\n\n![image-20220922011515991](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220922011515991.png)\n\n一切就绪后，点击右侧菜单 `Edit` 右边的控制台小图标，将命令复制下来，应该类似于这样：\n\n```shell\nnpx @squoosh/cli --resize '{\"enabled\":true,\"width\":970,\"height\":970,\"method\":\"lanczos3\",\"fitMethod\":\"stretch\",\"premultiply\":true,\"linearRGB\":true}' --mozjpeg '{\"quality\":75,\"baseline\":false,\"arithmetic\":false,\"progressive\":true,\"optimize_coding\":true,\"smoothing\":0,\"color_space\":3,\"quant_table\":3,\"trellis_multipass\":false,\"trellis_opt_zero\":false,\"trellis_opt_table\":false,\"trellis_loops\":1,\"auto_subsample\":true,\"chroma_subsample\":2,\"separate_chroma_quality\":false,\"chroma_quality\":75}'\n```\n\n如果想要先安装再使用，可以先用以下命令：\n\n```shell\nnpm i -g @squoosh/cli\n```\n\n然后就可以用 `squoosh-cli` 命令了，将上面的命令改写一下：\n\n```shell\nsquoosh-cli --resize '{\"enabled\":true,\"width\":970,\"height\":970,\"method\":\"lanczos3\",\"fitMethod\":\"stretch\",\"premultiply\":true,\"linearRGB\":true}' --mozjpeg '{\"quality\":75,\"baseline\":false,\"arithmetic\":false,\"progressive\":true,\"optimize_coding\":true,\"smoothing\":0,\"color_space\":3,\"quant_table\":3,\"trellis_multipass\":false,\"trellis_opt_zero\":false,\"trellis_opt_table\":false,\"trellis_loops\":1,\"auto_subsample\":true,\"chroma_subsample\":2,\"separate_chroma_quality\":false,\"chroma_quality\":75}'\n```\n\n还有需要改动的地方就是：目标文件夹，目标文件，长宽\n\n```shell\nsquoosh-cli --resize '{\"enabled\":true,\"width\":970,\"height\":970,\"method\":\"lanczos3\",\"fitMethod\":\"stretch\",\"premultiply\":true,\"linearRGB\":true}' --mozjpeg '{\"quality\":75,\"baseline\":false,\"arithmetic\":false,\"progressive\":true,\"optimize_coding\":true,\"smoothing\":0,\"color_space\":3,\"quant_table\":3,\"trellis_multipass\":false,\"trellis_opt_zero\":false,\"trellis_opt_table\":false,\"trellis_loops\":1,\"auto_subsample\":true,\"chroma_subsample\":2,\"separate_chroma_quality\":false,\"chroma_quality\":75}' -d \"[destinationfolder]\" \"*.png\"\n```\n\n在 `-d` 参数后写上输出的文件夹，后面再跟需要压缩的图片，可以使用通配符（windows不行！），然后前面 `--resize` 的参数里长宽可以只保留一个参数，就不会压缩图片长宽比了：\n\n```shell\nsquoosh-cli --resize '{\"enabled\":true,\"width\":500,\"method\":\"lanczos3\",\"fitMethod\":\"stretch\",\"premultiply\":true,\"linearRGB\":true}' --mozjpeg '{\"quality\":75,\"baseline\":false,\"arithmetic\":false,\"progressive\":true,\"optimize_coding\":true,\"smoothing\":0,\"color_space\":3,\"quant_table\":3,\"trellis_multipass\":false,\"trellis_opt_zero\":false,\"trellis_opt_table\":false,\"trellis_loops\":1,\"auto_subsample\":true,\"chroma_subsample\":2,\"separate_chroma_quality\":false,\"chroma_quality\":75}' -d \"[destinationfolder]\" \"*.png\"\n```\n\n然后就可以舒适地观看转换过程：\n\n![image-20220922012116526](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220922012116526.png)","source":"_posts/如何使用 Squoosh 命令行批量压缩图片.md","raw":"---\ntitle: 如何使用 Squoosh 命令行批量压缩图片\ndate: 2022-09-22 23:09:08\ntags:\nmathjax: false\n---\n\n先说明：windows 上无法批量压缩，原因：\n\nhttps://github.com/GoogleChromeLabs/squoosh/issues/973\n\n因为 squoosh-cli 暂不支持 windows 上的通配符。这么一个小问题，给我整崩溃了。\n\n完整过程如下\n\n先在 (https://squoosh.app) 上测试一下图片压缩的设置：\n\n![image-20220922011515991](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220922011515991.png)\n\n一切就绪后，点击右侧菜单 `Edit` 右边的控制台小图标，将命令复制下来，应该类似于这样：\n\n```shell\nnpx @squoosh/cli --resize '{\"enabled\":true,\"width\":970,\"height\":970,\"method\":\"lanczos3\",\"fitMethod\":\"stretch\",\"premultiply\":true,\"linearRGB\":true}' --mozjpeg '{\"quality\":75,\"baseline\":false,\"arithmetic\":false,\"progressive\":true,\"optimize_coding\":true,\"smoothing\":0,\"color_space\":3,\"quant_table\":3,\"trellis_multipass\":false,\"trellis_opt_zero\":false,\"trellis_opt_table\":false,\"trellis_loops\":1,\"auto_subsample\":true,\"chroma_subsample\":2,\"separate_chroma_quality\":false,\"chroma_quality\":75}'\n```\n\n如果想要先安装再使用，可以先用以下命令：\n\n```shell\nnpm i -g @squoosh/cli\n```\n\n然后就可以用 `squoosh-cli` 命令了，将上面的命令改写一下：\n\n```shell\nsquoosh-cli --resize '{\"enabled\":true,\"width\":970,\"height\":970,\"method\":\"lanczos3\",\"fitMethod\":\"stretch\",\"premultiply\":true,\"linearRGB\":true}' --mozjpeg '{\"quality\":75,\"baseline\":false,\"arithmetic\":false,\"progressive\":true,\"optimize_coding\":true,\"smoothing\":0,\"color_space\":3,\"quant_table\":3,\"trellis_multipass\":false,\"trellis_opt_zero\":false,\"trellis_opt_table\":false,\"trellis_loops\":1,\"auto_subsample\":true,\"chroma_subsample\":2,\"separate_chroma_quality\":false,\"chroma_quality\":75}'\n```\n\n还有需要改动的地方就是：目标文件夹，目标文件，长宽\n\n```shell\nsquoosh-cli --resize '{\"enabled\":true,\"width\":970,\"height\":970,\"method\":\"lanczos3\",\"fitMethod\":\"stretch\",\"premultiply\":true,\"linearRGB\":true}' --mozjpeg '{\"quality\":75,\"baseline\":false,\"arithmetic\":false,\"progressive\":true,\"optimize_coding\":true,\"smoothing\":0,\"color_space\":3,\"quant_table\":3,\"trellis_multipass\":false,\"trellis_opt_zero\":false,\"trellis_opt_table\":false,\"trellis_loops\":1,\"auto_subsample\":true,\"chroma_subsample\":2,\"separate_chroma_quality\":false,\"chroma_quality\":75}' -d \"[destinationfolder]\" \"*.png\"\n```\n\n在 `-d` 参数后写上输出的文件夹，后面再跟需要压缩的图片，可以使用通配符（windows不行！），然后前面 `--resize` 的参数里长宽可以只保留一个参数，就不会压缩图片长宽比了：\n\n```shell\nsquoosh-cli --resize '{\"enabled\":true,\"width\":500,\"method\":\"lanczos3\",\"fitMethod\":\"stretch\",\"premultiply\":true,\"linearRGB\":true}' --mozjpeg '{\"quality\":75,\"baseline\":false,\"arithmetic\":false,\"progressive\":true,\"optimize_coding\":true,\"smoothing\":0,\"color_space\":3,\"quant_table\":3,\"trellis_multipass\":false,\"trellis_opt_zero\":false,\"trellis_opt_table\":false,\"trellis_loops\":1,\"auto_subsample\":true,\"chroma_subsample\":2,\"separate_chroma_quality\":false,\"chroma_quality\":75}' -d \"[destinationfolder]\" \"*.png\"\n```\n\n然后就可以舒适地观看转换过程：\n\n![image-20220922012116526](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220922012116526.png)","slug":"如何使用 Squoosh 命令行批量压缩图片","published":1,"updated":"2022-09-21T17:24:10.232Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clgyiarty000ejku7ck3gat6s","content":"<p>先说明：windows 上无法批量压缩，原因：</p>\n<p><a href=\"https://github.com/GoogleChromeLabs/squoosh/issues/973\">https://github.com/GoogleChromeLabs/squoosh/issues/973</a></p>\n<p>因为 squoosh-cli 暂不支持 windows 上的通配符。这么一个小问题，给我整崩溃了。</p>\n<p>完整过程如下</p>\n<p>先在 (<a href=\"https://squoosh.app\">https://squoosh.app</a>) 上测试一下图片压缩的设置：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220922011515991.png\" alt=\"image-20220922011515991\"></p>\n<p>一切就绪后，点击右侧菜单 <code>Edit</code> 右边的控制台小图标，将命令复制下来，应该类似于这样：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npx @squoosh/cli --resize &#x27;&#123;&quot;enabled&quot;:true,&quot;width&quot;:970,&quot;height&quot;:970,&quot;method&quot;:&quot;lanczos3&quot;,&quot;fitMethod&quot;:&quot;stretch&quot;,&quot;premultiply&quot;:true,&quot;linearRGB&quot;:true&#125;&#x27; --mozjpeg &#x27;&#123;&quot;quality&quot;:75,&quot;baseline&quot;:false,&quot;arithmetic&quot;:false,&quot;progressive&quot;:true,&quot;optimize_coding&quot;:true,&quot;smoothing&quot;:0,&quot;color_space&quot;:3,&quot;quant_table&quot;:3,&quot;trellis_multipass&quot;:false,&quot;trellis_opt_zero&quot;:false,&quot;trellis_opt_table&quot;:false,&quot;trellis_loops&quot;:1,&quot;auto_subsample&quot;:true,&quot;chroma_subsample&quot;:2,&quot;separate_chroma_quality&quot;:false,&quot;chroma_quality&quot;:75&#125;&#x27;</span><br></pre></td></tr></table></figure>\n<p>如果想要先安装再使用，可以先用以下命令：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm i -g @squoosh/cli</span><br></pre></td></tr></table></figure>\n<p>然后就可以用 <code>squoosh-cli</code> 命令了，将上面的命令改写一下：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">squoosh-cli --resize &#x27;&#123;&quot;enabled&quot;:true,&quot;width&quot;:970,&quot;height&quot;:970,&quot;method&quot;:&quot;lanczos3&quot;,&quot;fitMethod&quot;:&quot;stretch&quot;,&quot;premultiply&quot;:true,&quot;linearRGB&quot;:true&#125;&#x27; --mozjpeg &#x27;&#123;&quot;quality&quot;:75,&quot;baseline&quot;:false,&quot;arithmetic&quot;:false,&quot;progressive&quot;:true,&quot;optimize_coding&quot;:true,&quot;smoothing&quot;:0,&quot;color_space&quot;:3,&quot;quant_table&quot;:3,&quot;trellis_multipass&quot;:false,&quot;trellis_opt_zero&quot;:false,&quot;trellis_opt_table&quot;:false,&quot;trellis_loops&quot;:1,&quot;auto_subsample&quot;:true,&quot;chroma_subsample&quot;:2,&quot;separate_chroma_quality&quot;:false,&quot;chroma_quality&quot;:75&#125;&#x27;</span><br></pre></td></tr></table></figure>\n<p>还有需要改动的地方就是：目标文件夹，目标文件，长宽</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">squoosh-cli --resize &#x27;&#123;&quot;enabled&quot;:true,&quot;width&quot;:970,&quot;height&quot;:970,&quot;method&quot;:&quot;lanczos3&quot;,&quot;fitMethod&quot;:&quot;stretch&quot;,&quot;premultiply&quot;:true,&quot;linearRGB&quot;:true&#125;&#x27; --mozjpeg &#x27;&#123;&quot;quality&quot;:75,&quot;baseline&quot;:false,&quot;arithmetic&quot;:false,&quot;progressive&quot;:true,&quot;optimize_coding&quot;:true,&quot;smoothing&quot;:0,&quot;color_space&quot;:3,&quot;quant_table&quot;:3,&quot;trellis_multipass&quot;:false,&quot;trellis_opt_zero&quot;:false,&quot;trellis_opt_table&quot;:false,&quot;trellis_loops&quot;:1,&quot;auto_subsample&quot;:true,&quot;chroma_subsample&quot;:2,&quot;separate_chroma_quality&quot;:false,&quot;chroma_quality&quot;:75&#125;&#x27; -d &quot;[destinationfolder]&quot; &quot;*.png&quot;</span><br></pre></td></tr></table></figure>\n<p>在 <code>-d</code> 参数后写上输出的文件夹，后面再跟需要压缩的图片，可以使用通配符（windows不行！），然后前面 <code>--resize</code> 的参数里长宽可以只保留一个参数，就不会压缩图片长宽比了：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">squoosh-cli --resize &#x27;&#123;&quot;enabled&quot;:true,&quot;width&quot;:500,&quot;method&quot;:&quot;lanczos3&quot;,&quot;fitMethod&quot;:&quot;stretch&quot;,&quot;premultiply&quot;:true,&quot;linearRGB&quot;:true&#125;&#x27; --mozjpeg &#x27;&#123;&quot;quality&quot;:75,&quot;baseline&quot;:false,&quot;arithmetic&quot;:false,&quot;progressive&quot;:true,&quot;optimize_coding&quot;:true,&quot;smoothing&quot;:0,&quot;color_space&quot;:3,&quot;quant_table&quot;:3,&quot;trellis_multipass&quot;:false,&quot;trellis_opt_zero&quot;:false,&quot;trellis_opt_table&quot;:false,&quot;trellis_loops&quot;:1,&quot;auto_subsample&quot;:true,&quot;chroma_subsample&quot;:2,&quot;separate_chroma_quality&quot;:false,&quot;chroma_quality&quot;:75&#125;&#x27; -d &quot;[destinationfolder]&quot; &quot;*.png&quot;</span><br></pre></td></tr></table></figure>\n<p>然后就可以舒适地观看转换过程：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220922012116526.png\" alt=\"image-20220922012116526\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>先说明：windows 上无法批量压缩，原因：</p>\n<p><a href=\"https://github.com/GoogleChromeLabs/squoosh/issues/973\">https://github.com/GoogleChromeLabs/squoosh/issues/973</a></p>\n<p>因为 squoosh-cli 暂不支持 windows 上的通配符。这么一个小问题，给我整崩溃了。</p>\n<p>完整过程如下</p>\n<p>先在 (<a href=\"https://squoosh.app\">https://squoosh.app</a>) 上测试一下图片压缩的设置：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220922011515991.png\" alt=\"image-20220922011515991\"></p>\n<p>一切就绪后，点击右侧菜单 <code>Edit</code> 右边的控制台小图标，将命令复制下来，应该类似于这样：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npx @squoosh/cli --resize &#x27;&#123;&quot;enabled&quot;:true,&quot;width&quot;:970,&quot;height&quot;:970,&quot;method&quot;:&quot;lanczos3&quot;,&quot;fitMethod&quot;:&quot;stretch&quot;,&quot;premultiply&quot;:true,&quot;linearRGB&quot;:true&#125;&#x27; --mozjpeg &#x27;&#123;&quot;quality&quot;:75,&quot;baseline&quot;:false,&quot;arithmetic&quot;:false,&quot;progressive&quot;:true,&quot;optimize_coding&quot;:true,&quot;smoothing&quot;:0,&quot;color_space&quot;:3,&quot;quant_table&quot;:3,&quot;trellis_multipass&quot;:false,&quot;trellis_opt_zero&quot;:false,&quot;trellis_opt_table&quot;:false,&quot;trellis_loops&quot;:1,&quot;auto_subsample&quot;:true,&quot;chroma_subsample&quot;:2,&quot;separate_chroma_quality&quot;:false,&quot;chroma_quality&quot;:75&#125;&#x27;</span><br></pre></td></tr></table></figure>\n<p>如果想要先安装再使用，可以先用以下命令：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm i -g @squoosh/cli</span><br></pre></td></tr></table></figure>\n<p>然后就可以用 <code>squoosh-cli</code> 命令了，将上面的命令改写一下：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">squoosh-cli --resize &#x27;&#123;&quot;enabled&quot;:true,&quot;width&quot;:970,&quot;height&quot;:970,&quot;method&quot;:&quot;lanczos3&quot;,&quot;fitMethod&quot;:&quot;stretch&quot;,&quot;premultiply&quot;:true,&quot;linearRGB&quot;:true&#125;&#x27; --mozjpeg &#x27;&#123;&quot;quality&quot;:75,&quot;baseline&quot;:false,&quot;arithmetic&quot;:false,&quot;progressive&quot;:true,&quot;optimize_coding&quot;:true,&quot;smoothing&quot;:0,&quot;color_space&quot;:3,&quot;quant_table&quot;:3,&quot;trellis_multipass&quot;:false,&quot;trellis_opt_zero&quot;:false,&quot;trellis_opt_table&quot;:false,&quot;trellis_loops&quot;:1,&quot;auto_subsample&quot;:true,&quot;chroma_subsample&quot;:2,&quot;separate_chroma_quality&quot;:false,&quot;chroma_quality&quot;:75&#125;&#x27;</span><br></pre></td></tr></table></figure>\n<p>还有需要改动的地方就是：目标文件夹，目标文件，长宽</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">squoosh-cli --resize &#x27;&#123;&quot;enabled&quot;:true,&quot;width&quot;:970,&quot;height&quot;:970,&quot;method&quot;:&quot;lanczos3&quot;,&quot;fitMethod&quot;:&quot;stretch&quot;,&quot;premultiply&quot;:true,&quot;linearRGB&quot;:true&#125;&#x27; --mozjpeg &#x27;&#123;&quot;quality&quot;:75,&quot;baseline&quot;:false,&quot;arithmetic&quot;:false,&quot;progressive&quot;:true,&quot;optimize_coding&quot;:true,&quot;smoothing&quot;:0,&quot;color_space&quot;:3,&quot;quant_table&quot;:3,&quot;trellis_multipass&quot;:false,&quot;trellis_opt_zero&quot;:false,&quot;trellis_opt_table&quot;:false,&quot;trellis_loops&quot;:1,&quot;auto_subsample&quot;:true,&quot;chroma_subsample&quot;:2,&quot;separate_chroma_quality&quot;:false,&quot;chroma_quality&quot;:75&#125;&#x27; -d &quot;[destinationfolder]&quot; &quot;*.png&quot;</span><br></pre></td></tr></table></figure>\n<p>在 <code>-d</code> 参数后写上输出的文件夹，后面再跟需要压缩的图片，可以使用通配符（windows不行！），然后前面 <code>--resize</code> 的参数里长宽可以只保留一个参数，就不会压缩图片长宽比了：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">squoosh-cli --resize &#x27;&#123;&quot;enabled&quot;:true,&quot;width&quot;:500,&quot;method&quot;:&quot;lanczos3&quot;,&quot;fitMethod&quot;:&quot;stretch&quot;,&quot;premultiply&quot;:true,&quot;linearRGB&quot;:true&#125;&#x27; --mozjpeg &#x27;&#123;&quot;quality&quot;:75,&quot;baseline&quot;:false,&quot;arithmetic&quot;:false,&quot;progressive&quot;:true,&quot;optimize_coding&quot;:true,&quot;smoothing&quot;:0,&quot;color_space&quot;:3,&quot;quant_table&quot;:3,&quot;trellis_multipass&quot;:false,&quot;trellis_opt_zero&quot;:false,&quot;trellis_opt_table&quot;:false,&quot;trellis_loops&quot;:1,&quot;auto_subsample&quot;:true,&quot;chroma_subsample&quot;:2,&quot;separate_chroma_quality&quot;:false,&quot;chroma_quality&quot;:75&#125;&#x27; -d &quot;[destinationfolder]&quot; &quot;*.png&quot;</span><br></pre></td></tr></table></figure>\n<p>然后就可以舒适地观看转换过程：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220922012116526.png\" alt=\"image-20220922012116526\"></p>\n"},{"title":"强化学习","date":"2022-08-08T11:09:08.000Z","mathjax":true,"_content":"\n强化学习主要由智能体（Agent）、环境（Environment）、状态（State）、动作（Action）、奖励（Reward）组成。智能体执行了某个动作后，环境将会转换到一个新的状态，对于该新的状态环境会给出奖励信号（正奖励或者负奖励）。随后，智能体根据新的状态和环境反馈的奖励，按照一定的策略执行新的动作。上述过程为智能体和环境通过状态、动作、奖励进行交互的方式。\n\n智能体通过强化学习，可以知道自己在什么状态下，应该采取什么样的动作使得自身获得最大奖励。由于智能体与环境的交互方式与人类与环境的交互方式类似，可以认为强化学习是一套通用的学习框架，可用来解决通用人工智能的问题。因此强化学习也被称为通用人工智能的机器学习方法。","source":"_posts/强化学习.md","raw":"---\ntitle: 强化学习\ndate: 2022-08-08 19:09:08\ntags:\nmathjax: true\n---\n\n强化学习主要由智能体（Agent）、环境（Environment）、状态（State）、动作（Action）、奖励（Reward）组成。智能体执行了某个动作后，环境将会转换到一个新的状态，对于该新的状态环境会给出奖励信号（正奖励或者负奖励）。随后，智能体根据新的状态和环境反馈的奖励，按照一定的策略执行新的动作。上述过程为智能体和环境通过状态、动作、奖励进行交互的方式。\n\n智能体通过强化学习，可以知道自己在什么状态下，应该采取什么样的动作使得自身获得最大奖励。由于智能体与环境的交互方式与人类与环境的交互方式类似，可以认为强化学习是一套通用的学习框架，可用来解决通用人工智能的问题。因此强化学习也被称为通用人工智能的机器学习方法。","slug":"强化学习","published":1,"updated":"2022-08-26T16:37:03.907Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clgyiartz000fjku77evp4c37","content":"<p>强化学习主要由智能体（Agent）、环境（Environment）、状态（State）、动作（Action）、奖励（Reward）组成。智能体执行了某个动作后，环境将会转换到一个新的状态，对于该新的状态环境会给出奖励信号（正奖励或者负奖励）。随后，智能体根据新的状态和环境反馈的奖励，按照一定的策略执行新的动作。上述过程为智能体和环境通过状态、动作、奖励进行交互的方式。</p>\n<p>智能体通过强化学习，可以知道自己在什么状态下，应该采取什么样的动作使得自身获得最大奖励。由于智能体与环境的交互方式与人类与环境的交互方式类似，可以认为强化学习是一套通用的学习框架，可用来解决通用人工智能的问题。因此强化学习也被称为通用人工智能的机器学习方法。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>强化学习主要由智能体（Agent）、环境（Environment）、状态（State）、动作（Action）、奖励（Reward）组成。智能体执行了某个动作后，环境将会转换到一个新的状态，对于该新的状态环境会给出奖励信号（正奖励或者负奖励）。随后，智能体根据新的状态和环境反馈的奖励，按照一定的策略执行新的动作。上述过程为智能体和环境通过状态、动作、奖励进行交互的方式。</p>\n<p>智能体通过强化学习，可以知道自己在什么状态下，应该采取什么样的动作使得自身获得最大奖励。由于智能体与环境的交互方式与人类与环境的交互方式类似，可以认为强化学习是一套通用的学习框架，可用来解决通用人工智能的问题。因此强化学习也被称为通用人工智能的机器学习方法。</p>\n"},{"title":"学着克服国内网络环境.jpg","date":"2023-04-27T15:09:08.000Z","mathjax":false,"_content":"\n创意来源：[地霊殿](https://redcontritio.github.io)\n\n捡了一些自己常用的设置放在这里方便查看~\n\n## Github 换源\n\n```shell\ngit config --global url.\"https://ghproxy.com/https://github.com/\".insteadOf \"https://github.com/\"\n```\n\n## Anaconda 换源\n\n编辑 `.condarc` 文件，换北外源：\n\n```\nchannels:\n  - defaults\nshow_channel_urls: true\ndefault_channels:\n  - https://mirrors.bfsu.edu.cn/anaconda/pkgs/main\n  - https://mirrors.bfsu.edu.cn/anaconda/pkgs/r\n  - https://mirrors.bfsu.edu.cn/anaconda/pkgs/msys2\ncustom_channels:\n  conda-forge: https://mirrors.bfsu.edu.cn/anaconda/cloud\n  msys2: https://mirrors.bfsu.edu.cn/anaconda/cloud\n  bioconda: https://mirrors.bfsu.edu.cn/anaconda/cloud\n  menpo: https://mirrors.bfsu.edu.cn/anaconda/cloud\n  pytorch: https://mirrors.bfsu.edu.cn/anaconda/cloud\n  pytorch-lts: https://mirrors.bfsu.edu.cn/anaconda/cloud\n  simpleitk: https://mirrors.bfsu.edu.cn/anaconda/cloud\n```\n\n 再用 `conda clean -i` 清理缓存即可\n\n## Pypi 换源\n\n```shell\npython -m pip install -i https://mirrors.bfsu.edu.cn/pypi/web/simple --upgrade pip\npip config set global.index-url https://mirrors.bfsu.edu.cn/pypi/web/simple\n```\n\n## Docker 换源\n\n编辑 `/etc/docker/daemon.json` 文件，换源：\n\n| 镜像加速器          | 镜像加速器地址                       |\n| ------------------- | ------------------------------------ |\n| Docker 中国官方镜像 | https://registry.docker-cn.com       |\n| DaoCloud 镜像站     | http://f1361db2.m.daocloud.io        |\n| Azure 中国镜像      | https://dockerhub.azk8s.cn           |\n| 科大镜像站          | https://docker.mirrors.ustc.edu.cn   |\n| 阿里云              | https://ud6340vz.mirror.aliyuncs.com |\n| 七牛云              | https://reg-mirror.qiniu.com         |\n| 网易云              | https://hub-mirror.c.163.com         |\n| 腾讯云              | https://mirror.ccs.tencentyun.com    |\n\n## 待更新 ...","source":"_posts/学着克服国内网络环境.jpg.md","raw":"---\ntitle: 学着克服国内网络环境.jpg\ndate: 2023-04-27 23:09:08\ntags:\nmathjax: false\n---\n\n创意来源：[地霊殿](https://redcontritio.github.io)\n\n捡了一些自己常用的设置放在这里方便查看~\n\n## Github 换源\n\n```shell\ngit config --global url.\"https://ghproxy.com/https://github.com/\".insteadOf \"https://github.com/\"\n```\n\n## Anaconda 换源\n\n编辑 `.condarc` 文件，换北外源：\n\n```\nchannels:\n  - defaults\nshow_channel_urls: true\ndefault_channels:\n  - https://mirrors.bfsu.edu.cn/anaconda/pkgs/main\n  - https://mirrors.bfsu.edu.cn/anaconda/pkgs/r\n  - https://mirrors.bfsu.edu.cn/anaconda/pkgs/msys2\ncustom_channels:\n  conda-forge: https://mirrors.bfsu.edu.cn/anaconda/cloud\n  msys2: https://mirrors.bfsu.edu.cn/anaconda/cloud\n  bioconda: https://mirrors.bfsu.edu.cn/anaconda/cloud\n  menpo: https://mirrors.bfsu.edu.cn/anaconda/cloud\n  pytorch: https://mirrors.bfsu.edu.cn/anaconda/cloud\n  pytorch-lts: https://mirrors.bfsu.edu.cn/anaconda/cloud\n  simpleitk: https://mirrors.bfsu.edu.cn/anaconda/cloud\n```\n\n 再用 `conda clean -i` 清理缓存即可\n\n## Pypi 换源\n\n```shell\npython -m pip install -i https://mirrors.bfsu.edu.cn/pypi/web/simple --upgrade pip\npip config set global.index-url https://mirrors.bfsu.edu.cn/pypi/web/simple\n```\n\n## Docker 换源\n\n编辑 `/etc/docker/daemon.json` 文件，换源：\n\n| 镜像加速器          | 镜像加速器地址                       |\n| ------------------- | ------------------------------------ |\n| Docker 中国官方镜像 | https://registry.docker-cn.com       |\n| DaoCloud 镜像站     | http://f1361db2.m.daocloud.io        |\n| Azure 中国镜像      | https://dockerhub.azk8s.cn           |\n| 科大镜像站          | https://docker.mirrors.ustc.edu.cn   |\n| 阿里云              | https://ud6340vz.mirror.aliyuncs.com |\n| 七牛云              | https://reg-mirror.qiniu.com         |\n| 网易云              | https://hub-mirror.c.163.com         |\n| 腾讯云              | https://mirror.ccs.tencentyun.com    |\n\n## 待更新 ...","slug":"学着克服国内网络环境.jpg","published":1,"updated":"2023-04-27T02:10:50.475Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clgyiartz000gjku73h662okw","content":"<p>创意来源：<a href=\"https://redcontritio.github.io\">地霊殿</a></p>\n<p>捡了一些自己常用的设置放在这里方便查看~</p>\n<h2 id=\"Github-换源\"><a href=\"#Github-换源\" class=\"headerlink\" title=\"Github 换源\"></a>Github 换源</h2><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git config --global url.&quot;https://ghproxy.com/https://github.com/&quot;.insteadOf &quot;https://github.com/&quot;</span><br></pre></td></tr></table></figure>\n<h2 id=\"Anaconda-换源\"><a href=\"#Anaconda-换源\" class=\"headerlink\" title=\"Anaconda 换源\"></a>Anaconda 换源</h2><p>编辑 <code>.condarc</code> 文件，换北外源：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">channels:</span><br><span class=\"line\">  - defaults</span><br><span class=\"line\">show_channel_urls: true</span><br><span class=\"line\">default_channels:</span><br><span class=\"line\">  - https://mirrors.bfsu.edu.cn/anaconda/pkgs/main</span><br><span class=\"line\">  - https://mirrors.bfsu.edu.cn/anaconda/pkgs/r</span><br><span class=\"line\">  - https://mirrors.bfsu.edu.cn/anaconda/pkgs/msys2</span><br><span class=\"line\">custom_channels:</span><br><span class=\"line\">  conda-forge: https://mirrors.bfsu.edu.cn/anaconda/cloud</span><br><span class=\"line\">  msys2: https://mirrors.bfsu.edu.cn/anaconda/cloud</span><br><span class=\"line\">  bioconda: https://mirrors.bfsu.edu.cn/anaconda/cloud</span><br><span class=\"line\">  menpo: https://mirrors.bfsu.edu.cn/anaconda/cloud</span><br><span class=\"line\">  pytorch: https://mirrors.bfsu.edu.cn/anaconda/cloud</span><br><span class=\"line\">  pytorch-lts: https://mirrors.bfsu.edu.cn/anaconda/cloud</span><br><span class=\"line\">  simpleitk: https://mirrors.bfsu.edu.cn/anaconda/cloud</span><br></pre></td></tr></table></figure>\n<p> 再用 <code>conda clean -i</code> 清理缓存即可</p>\n<h2 id=\"Pypi-换源\"><a href=\"#Pypi-换源\" class=\"headerlink\" title=\"Pypi 换源\"></a>Pypi 换源</h2><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python -m pip install -i https://mirrors.bfsu.edu.cn/pypi/web/simple --upgrade pip</span><br><span class=\"line\">pip config set global.index-url https://mirrors.bfsu.edu.cn/pypi/web/simple</span><br></pre></td></tr></table></figure>\n<h2 id=\"Docker-换源\"><a href=\"#Docker-换源\" class=\"headerlink\" title=\"Docker 换源\"></a>Docker 换源</h2><p>编辑 <code>/etc/docker/daemon.json</code> 文件，换源：</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>镜像加速器</th>\n<th>镜像加速器地址</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Docker 中国官方镜像</td>\n<td><a href=\"https://registry.docker-cn.com\">https://registry.docker-cn.com</a></td>\n</tr>\n<tr>\n<td>DaoCloud 镜像站</td>\n<td><a href=\"http://f1361db2.m.daocloud.io\">http://f1361db2.m.daocloud.io</a></td>\n</tr>\n<tr>\n<td>Azure 中国镜像</td>\n<td><a href=\"https://dockerhub.azk8s.cn\">https://dockerhub.azk8s.cn</a></td>\n</tr>\n<tr>\n<td>科大镜像站</td>\n<td><a href=\"https://docker.mirrors.ustc.edu.cn\">https://docker.mirrors.ustc.edu.cn</a></td>\n</tr>\n<tr>\n<td>阿里云</td>\n<td><a href=\"https://ud6340vz.mirror.aliyuncs.com\">https://ud6340vz.mirror.aliyuncs.com</a></td>\n</tr>\n<tr>\n<td>七牛云</td>\n<td><a href=\"https://reg-mirror.qiniu.com\">https://reg-mirror.qiniu.com</a></td>\n</tr>\n<tr>\n<td>网易云</td>\n<td><a href=\"https://hub-mirror.c.163.com\">https://hub-mirror.c.163.com</a></td>\n</tr>\n<tr>\n<td>腾讯云</td>\n<td><a href=\"https://mirror.ccs.tencentyun.com\">https://mirror.ccs.tencentyun.com</a></td>\n</tr>\n</tbody>\n</table>\n</div>\n<h2 id=\"待更新-…\"><a href=\"#待更新-…\" class=\"headerlink\" title=\"待更新 …\"></a>待更新 …</h2>","site":{"data":{}},"excerpt":"","more":"<p>创意来源：<a href=\"https://redcontritio.github.io\">地霊殿</a></p>\n<p>捡了一些自己常用的设置放在这里方便查看~</p>\n<h2 id=\"Github-换源\"><a href=\"#Github-换源\" class=\"headerlink\" title=\"Github 换源\"></a>Github 换源</h2><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git config --global url.&quot;https://ghproxy.com/https://github.com/&quot;.insteadOf &quot;https://github.com/&quot;</span><br></pre></td></tr></table></figure>\n<h2 id=\"Anaconda-换源\"><a href=\"#Anaconda-换源\" class=\"headerlink\" title=\"Anaconda 换源\"></a>Anaconda 换源</h2><p>编辑 <code>.condarc</code> 文件，换北外源：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">channels:</span><br><span class=\"line\">  - defaults</span><br><span class=\"line\">show_channel_urls: true</span><br><span class=\"line\">default_channels:</span><br><span class=\"line\">  - https://mirrors.bfsu.edu.cn/anaconda/pkgs/main</span><br><span class=\"line\">  - https://mirrors.bfsu.edu.cn/anaconda/pkgs/r</span><br><span class=\"line\">  - https://mirrors.bfsu.edu.cn/anaconda/pkgs/msys2</span><br><span class=\"line\">custom_channels:</span><br><span class=\"line\">  conda-forge: https://mirrors.bfsu.edu.cn/anaconda/cloud</span><br><span class=\"line\">  msys2: https://mirrors.bfsu.edu.cn/anaconda/cloud</span><br><span class=\"line\">  bioconda: https://mirrors.bfsu.edu.cn/anaconda/cloud</span><br><span class=\"line\">  menpo: https://mirrors.bfsu.edu.cn/anaconda/cloud</span><br><span class=\"line\">  pytorch: https://mirrors.bfsu.edu.cn/anaconda/cloud</span><br><span class=\"line\">  pytorch-lts: https://mirrors.bfsu.edu.cn/anaconda/cloud</span><br><span class=\"line\">  simpleitk: https://mirrors.bfsu.edu.cn/anaconda/cloud</span><br></pre></td></tr></table></figure>\n<p> 再用 <code>conda clean -i</code> 清理缓存即可</p>\n<h2 id=\"Pypi-换源\"><a href=\"#Pypi-换源\" class=\"headerlink\" title=\"Pypi 换源\"></a>Pypi 换源</h2><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python -m pip install -i https://mirrors.bfsu.edu.cn/pypi/web/simple --upgrade pip</span><br><span class=\"line\">pip config set global.index-url https://mirrors.bfsu.edu.cn/pypi/web/simple</span><br></pre></td></tr></table></figure>\n<h2 id=\"Docker-换源\"><a href=\"#Docker-换源\" class=\"headerlink\" title=\"Docker 换源\"></a>Docker 换源</h2><p>编辑 <code>/etc/docker/daemon.json</code> 文件，换源：</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>镜像加速器</th>\n<th>镜像加速器地址</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Docker 中国官方镜像</td>\n<td><a href=\"https://registry.docker-cn.com\">https://registry.docker-cn.com</a></td>\n</tr>\n<tr>\n<td>DaoCloud 镜像站</td>\n<td><a href=\"http://f1361db2.m.daocloud.io\">http://f1361db2.m.daocloud.io</a></td>\n</tr>\n<tr>\n<td>Azure 中国镜像</td>\n<td><a href=\"https://dockerhub.azk8s.cn\">https://dockerhub.azk8s.cn</a></td>\n</tr>\n<tr>\n<td>科大镜像站</td>\n<td><a href=\"https://docker.mirrors.ustc.edu.cn\">https://docker.mirrors.ustc.edu.cn</a></td>\n</tr>\n<tr>\n<td>阿里云</td>\n<td><a href=\"https://ud6340vz.mirror.aliyuncs.com\">https://ud6340vz.mirror.aliyuncs.com</a></td>\n</tr>\n<tr>\n<td>七牛云</td>\n<td><a href=\"https://reg-mirror.qiniu.com\">https://reg-mirror.qiniu.com</a></td>\n</tr>\n<tr>\n<td>网易云</td>\n<td><a href=\"https://hub-mirror.c.163.com\">https://hub-mirror.c.163.com</a></td>\n</tr>\n<tr>\n<td>腾讯云</td>\n<td><a href=\"https://mirror.ccs.tencentyun.com\">https://mirror.ccs.tencentyun.com</a></td>\n</tr>\n</tbody>\n</table>\n</div>\n<h2 id=\"待更新-…\"><a href=\"#待更新-…\" class=\"headerlink\" title=\"待更新 …\"></a>待更新 …</h2>"},{"title":"如何利用 webp 进行小程序图片加载速度的优化","date":"2020-08-17T22:56:08.000Z","_content":"\n# 如何利用 webp 进行小程序图片加载速度的优化\n\n## 导语\n\n最近很长一段时间没有更新博客，一方面是自己最近参与了小程序的开发，另一方面也是自己略有些怠惰，给自己记个过~那么现在既然回到学校那么还是要分享一些知识的。\n\n前一阵子参与微信小程序开发时遇到了一个小问题，就是图片的加载速度不理想。即使我们对于图像的大小一缩再缩，但后台服务器中200-300KB的图片在小程序里往往需要等待数十秒才能加载完毕（仅在安卓机上测试过，苹果应该优化得好一些），这个图片加载的速度导致我们的小程序体验比较糟糕。我经过搜索发现webp这种图片格式可能可以为我们提供一个可行的替代方案。\n\n## 知识补充\n\n> WebP（发音：weppy）是一种同时提供了有损压缩与无损压缩（可逆压缩）的图片文件格式，派生自影像编码格式VP8，被认为是WebM多媒体格式的姊妹项目，是由Google在购买On2 Technologies后发展出来，以BSD授权条款发布。\n> WebP最初在2010年发布，目标是减少文件大小，但达到和JPEG格式相同的图片质量，希望能够减少图片档在网络上的发送时间。根据Google较早的测试，WebP的无损压缩比网络上找到的PNG档少了45％的文件大小，即使这些PNG档在使用pngcrush和PNGOUT处理过，WebP还是可以减少28％的文件大小。\n> <p align=\"right\">维基百科</p>\n\n说人话？好。webp就是谷歌发布的一款可以在保证图片质量较为完整的情况下压缩率较高的网络图片格式。\n\n这么一看，webp岂不是无敌？实际上对于我们的项目它目前还是有局限性的：iOS端的微信小程序是不支持webp的。\n\n但是方法总是有的，既然iOS有它自己的优化方法，那么我就不让它搭上webp这班车了，我们可以判断设备的类型，然后针对安卓的手机进行优化。\n\n## 教程\n\n### 作案工具 \n\n* vscode\n* 微信小程序开发工具\n\n### 手段\n\n首先我们需要定义一个全局变量作为判断是否为苹果设备的依据。所有的全局变量均在app.js里定义，在app.js中加入全局变量iOS，并用wx.getSystemInfo获取设备信息进行判断：\n\n```javascript\nApp({\n  onLaunch: function () {\n    var that = this;\n    wx.getSystemInfo({\n      success: (res) => {\n        console.log(res);                           //res是一个对象，包含设备信息。其中res.system的内容则是设备系统的版本\n        if (res.system.indexOf(\"iOS\") != -1) {      //如果系统版本中有iOS三个字母，则判定为苹果设备\n          that.globalData.iOS = true;\n        }\n      },\n    });\n  globalData: {\n    iOS: false,                                     //默认iOS的值为false\n  },\n})\n```\n\n到这里我们做好了设备类型的判断，接下来我们就要在独立的页面中进行链接替换。打个比方，如果这台设备是苹果，那么图片链接就使用 ../a.png ；如果是安卓，那么链接就换成 ../a.webp 。\n\n给出一个示例页面：\n\nhtml:\n\n```html\n<!--简单的页面，一个view标签，一张图，图片链接用js传入数据-->\n<view class=\"container\">\n    <image src = '{{img1}}' />\n</view>\n```\n\njavascript:\n\n```javascript\nPage({                                              //传入页面的数据\n  data: {\n    img1:\"../a.png\",\n  },\n})\n```\n\n这时我们在js中加入链接替换的内容：\n\n```javascript\nconst app = getApp();                               //这行代码很重要，用来从app.js中获取全局变量\n\nPage({\n  data: {\n    img1:\"../a.png\",\n  },\n  //事件处理函数，onLoad指在页面加载中的操作\n  onLoad: function () {\n    if (!app.globalData.iOS) {                      //app.globalData.iOS即为上面定义的全局变量，如果iOS为假，则替换链接\n      this.setData({\n        img1:\"../a.webp\",\n      });\n    }\n  },\n})\n```\n\n这样就完成了webp的替换，还是挺容易的，没有想象中的那么复杂。唯一的问题是页面多了以后替换起来会比较麻烦，不过如果都把图片的数据写在js里的话，替换起来会快很多。\n","source":"_posts/微信小程序webp优化.md","raw":"---\ntitle: 如何利用 webp 进行小程序图片加载速度的优化\ndate: 2020-08-18 06:56:08\ntags:\n---\n\n# 如何利用 webp 进行小程序图片加载速度的优化\n\n## 导语\n\n最近很长一段时间没有更新博客，一方面是自己最近参与了小程序的开发，另一方面也是自己略有些怠惰，给自己记个过~那么现在既然回到学校那么还是要分享一些知识的。\n\n前一阵子参与微信小程序开发时遇到了一个小问题，就是图片的加载速度不理想。即使我们对于图像的大小一缩再缩，但后台服务器中200-300KB的图片在小程序里往往需要等待数十秒才能加载完毕（仅在安卓机上测试过，苹果应该优化得好一些），这个图片加载的速度导致我们的小程序体验比较糟糕。我经过搜索发现webp这种图片格式可能可以为我们提供一个可行的替代方案。\n\n## 知识补充\n\n> WebP（发音：weppy）是一种同时提供了有损压缩与无损压缩（可逆压缩）的图片文件格式，派生自影像编码格式VP8，被认为是WebM多媒体格式的姊妹项目，是由Google在购买On2 Technologies后发展出来，以BSD授权条款发布。\n> WebP最初在2010年发布，目标是减少文件大小，但达到和JPEG格式相同的图片质量，希望能够减少图片档在网络上的发送时间。根据Google较早的测试，WebP的无损压缩比网络上找到的PNG档少了45％的文件大小，即使这些PNG档在使用pngcrush和PNGOUT处理过，WebP还是可以减少28％的文件大小。\n> <p align=\"right\">维基百科</p>\n\n说人话？好。webp就是谷歌发布的一款可以在保证图片质量较为完整的情况下压缩率较高的网络图片格式。\n\n这么一看，webp岂不是无敌？实际上对于我们的项目它目前还是有局限性的：iOS端的微信小程序是不支持webp的。\n\n但是方法总是有的，既然iOS有它自己的优化方法，那么我就不让它搭上webp这班车了，我们可以判断设备的类型，然后针对安卓的手机进行优化。\n\n## 教程\n\n### 作案工具 \n\n* vscode\n* 微信小程序开发工具\n\n### 手段\n\n首先我们需要定义一个全局变量作为判断是否为苹果设备的依据。所有的全局变量均在app.js里定义，在app.js中加入全局变量iOS，并用wx.getSystemInfo获取设备信息进行判断：\n\n```javascript\nApp({\n  onLaunch: function () {\n    var that = this;\n    wx.getSystemInfo({\n      success: (res) => {\n        console.log(res);                           //res是一个对象，包含设备信息。其中res.system的内容则是设备系统的版本\n        if (res.system.indexOf(\"iOS\") != -1) {      //如果系统版本中有iOS三个字母，则判定为苹果设备\n          that.globalData.iOS = true;\n        }\n      },\n    });\n  globalData: {\n    iOS: false,                                     //默认iOS的值为false\n  },\n})\n```\n\n到这里我们做好了设备类型的判断，接下来我们就要在独立的页面中进行链接替换。打个比方，如果这台设备是苹果，那么图片链接就使用 ../a.png ；如果是安卓，那么链接就换成 ../a.webp 。\n\n给出一个示例页面：\n\nhtml:\n\n```html\n<!--简单的页面，一个view标签，一张图，图片链接用js传入数据-->\n<view class=\"container\">\n    <image src = '{{img1}}' />\n</view>\n```\n\njavascript:\n\n```javascript\nPage({                                              //传入页面的数据\n  data: {\n    img1:\"../a.png\",\n  },\n})\n```\n\n这时我们在js中加入链接替换的内容：\n\n```javascript\nconst app = getApp();                               //这行代码很重要，用来从app.js中获取全局变量\n\nPage({\n  data: {\n    img1:\"../a.png\",\n  },\n  //事件处理函数，onLoad指在页面加载中的操作\n  onLoad: function () {\n    if (!app.globalData.iOS) {                      //app.globalData.iOS即为上面定义的全局变量，如果iOS为假，则替换链接\n      this.setData({\n        img1:\"../a.webp\",\n      });\n    }\n  },\n})\n```\n\n这样就完成了webp的替换，还是挺容易的，没有想象中的那么复杂。唯一的问题是页面多了以后替换起来会比较麻烦，不过如果都把图片的数据写在js里的话，替换起来会快很多。\n","slug":"微信小程序webp优化","published":1,"updated":"2022-08-25T13:13:32.986Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clgyiaru0000hjku7fyme38ll","content":"<h1 id=\"如何利用-webp-进行小程序图片加载速度的优化\"><a href=\"#如何利用-webp-进行小程序图片加载速度的优化\" class=\"headerlink\" title=\"如何利用 webp 进行小程序图片加载速度的优化\"></a>如何利用 webp 进行小程序图片加载速度的优化</h1><h2 id=\"导语\"><a href=\"#导语\" class=\"headerlink\" title=\"导语\"></a>导语</h2><p>最近很长一段时间没有更新博客，一方面是自己最近参与了小程序的开发，另一方面也是自己略有些怠惰，给自己记个过~那么现在既然回到学校那么还是要分享一些知识的。</p>\n<p>前一阵子参与微信小程序开发时遇到了一个小问题，就是图片的加载速度不理想。即使我们对于图像的大小一缩再缩，但后台服务器中200-300KB的图片在小程序里往往需要等待数十秒才能加载完毕（仅在安卓机上测试过，苹果应该优化得好一些），这个图片加载的速度导致我们的小程序体验比较糟糕。我经过搜索发现webp这种图片格式可能可以为我们提供一个可行的替代方案。</p>\n<h2 id=\"知识补充\"><a href=\"#知识补充\" class=\"headerlink\" title=\"知识补充\"></a>知识补充</h2><blockquote>\n<p>WebP（发音：weppy）是一种同时提供了有损压缩与无损压缩（可逆压缩）的图片文件格式，派生自影像编码格式VP8，被认为是WebM多媒体格式的姊妹项目，是由Google在购买On2 Technologies后发展出来，以BSD授权条款发布。<br>WebP最初在2010年发布，目标是减少文件大小，但达到和JPEG格式相同的图片质量，希望能够减少图片档在网络上的发送时间。根据Google较早的测试，WebP的无损压缩比网络上找到的PNG档少了45％的文件大小，即使这些PNG档在使用pngcrush和PNGOUT处理过，WebP还是可以减少28％的文件大小。</p>\n<p align=\"right\">维基百科</p>\n\n</blockquote>\n<p>说人话？好。webp就是谷歌发布的一款可以在保证图片质量较为完整的情况下压缩率较高的网络图片格式。</p>\n<p>这么一看，webp岂不是无敌？实际上对于我们的项目它目前还是有局限性的：iOS端的微信小程序是不支持webp的。</p>\n<p>但是方法总是有的，既然iOS有它自己的优化方法，那么我就不让它搭上webp这班车了，我们可以判断设备的类型，然后针对安卓的手机进行优化。</p>\n<h2 id=\"教程\"><a href=\"#教程\" class=\"headerlink\" title=\"教程\"></a>教程</h2><h3 id=\"作案工具\"><a href=\"#作案工具\" class=\"headerlink\" title=\"作案工具\"></a>作案工具</h3><ul>\n<li>vscode</li>\n<li>微信小程序开发工具</li>\n</ul>\n<h3 id=\"手段\"><a href=\"#手段\" class=\"headerlink\" title=\"手段\"></a>手段</h3><p>首先我们需要定义一个全局变量作为判断是否为苹果设备的依据。所有的全局变量均在app.js里定义，在app.js中加入全局变量iOS，并用wx.getSystemInfo获取设备信息进行判断：</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"title class_\">App</span>(&#123;</span><br><span class=\"line\">  <span class=\"attr\">onLaunch</span>: <span class=\"keyword\">function</span> (<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">var</span> that = <span class=\"variable language_\">this</span>;</span><br><span class=\"line\">    wx.<span class=\"title function_\">getSystemInfo</span>(&#123;</span><br><span class=\"line\">      <span class=\"attr\">success</span>: <span class=\"function\">(<span class=\"params\">res</span>) =&gt;</span> &#123;</span><br><span class=\"line\">        <span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(res);                           <span class=\"comment\">//res是一个对象，包含设备信息。其中res.system的内容则是设备系统的版本</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (res.<span class=\"property\">system</span>.<span class=\"title function_\">indexOf</span>(<span class=\"string\">&quot;iOS&quot;</span>) != -<span class=\"number\">1</span>) &#123;      <span class=\"comment\">//如果系统版本中有iOS三个字母，则判定为苹果设备</span></span><br><span class=\"line\">          that.<span class=\"property\">globalData</span>.<span class=\"property\">iOS</span> = <span class=\"literal\">true</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">    &#125;);</span><br><span class=\"line\">  <span class=\"attr\">globalData</span>: &#123;</span><br><span class=\"line\">    <span class=\"attr\">iOS</span>: <span class=\"literal\">false</span>,                                     <span class=\"comment\">//默认iOS的值为false</span></span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure>\n<p>到这里我们做好了设备类型的判断，接下来我们就要在独立的页面中进行链接替换。打个比方，如果这台设备是苹果，那么图片链接就使用 ../a.png ；如果是安卓，那么链接就换成 ../a.webp 。</p>\n<p>给出一个示例页面：</p>\n<p>html:</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">&lt;!--简单的页面，一个view标签，一张图，图片链接用js传入数据--&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">view</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;container&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">image</span> <span class=\"attr\">src</span> = <span class=\"string\">&#x27;&#123;&#123;img1&#125;&#125;&#x27;</span> /&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">view</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>javascript:</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"title class_\">Page</span>(&#123;                                              <span class=\"comment\">//传入页面的数据</span></span><br><span class=\"line\">  <span class=\"attr\">data</span>: &#123;</span><br><span class=\"line\">    <span class=\"attr\">img1</span>:<span class=\"string\">&quot;../a.png&quot;</span>,</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure>\n<p>这时我们在js中加入链接替换的内容：</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">const</span> app = <span class=\"title function_\">getApp</span>();                               <span class=\"comment\">//这行代码很重要，用来从app.js中获取全局变量</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"title class_\">Page</span>(&#123;</span><br><span class=\"line\">  <span class=\"attr\">data</span>: &#123;</span><br><span class=\"line\">    <span class=\"attr\">img1</span>:<span class=\"string\">&quot;../a.png&quot;</span>,</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  <span class=\"comment\">//事件处理函数，onLoad指在页面加载中的操作</span></span><br><span class=\"line\">  <span class=\"attr\">onLoad</span>: <span class=\"keyword\">function</span> (<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!app.<span class=\"property\">globalData</span>.<span class=\"property\">iOS</span>) &#123;                      <span class=\"comment\">//app.globalData.iOS即为上面定义的全局变量，如果iOS为假，则替换链接</span></span><br><span class=\"line\">      <span class=\"variable language_\">this</span>.<span class=\"title function_\">setData</span>(&#123;</span><br><span class=\"line\">        <span class=\"attr\">img1</span>:<span class=\"string\">&quot;../a.webp&quot;</span>,</span><br><span class=\"line\">      &#125;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure>\n<p>这样就完成了webp的替换，还是挺容易的，没有想象中的那么复杂。唯一的问题是页面多了以后替换起来会比较麻烦，不过如果都把图片的数据写在js里的话，替换起来会快很多。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"如何利用-webp-进行小程序图片加载速度的优化\"><a href=\"#如何利用-webp-进行小程序图片加载速度的优化\" class=\"headerlink\" title=\"如何利用 webp 进行小程序图片加载速度的优化\"></a>如何利用 webp 进行小程序图片加载速度的优化</h1><h2 id=\"导语\"><a href=\"#导语\" class=\"headerlink\" title=\"导语\"></a>导语</h2><p>最近很长一段时间没有更新博客，一方面是自己最近参与了小程序的开发，另一方面也是自己略有些怠惰，给自己记个过~那么现在既然回到学校那么还是要分享一些知识的。</p>\n<p>前一阵子参与微信小程序开发时遇到了一个小问题，就是图片的加载速度不理想。即使我们对于图像的大小一缩再缩，但后台服务器中200-300KB的图片在小程序里往往需要等待数十秒才能加载完毕（仅在安卓机上测试过，苹果应该优化得好一些），这个图片加载的速度导致我们的小程序体验比较糟糕。我经过搜索发现webp这种图片格式可能可以为我们提供一个可行的替代方案。</p>\n<h2 id=\"知识补充\"><a href=\"#知识补充\" class=\"headerlink\" title=\"知识补充\"></a>知识补充</h2><blockquote>\n<p>WebP（发音：weppy）是一种同时提供了有损压缩与无损压缩（可逆压缩）的图片文件格式，派生自影像编码格式VP8，被认为是WebM多媒体格式的姊妹项目，是由Google在购买On2 Technologies后发展出来，以BSD授权条款发布。<br>WebP最初在2010年发布，目标是减少文件大小，但达到和JPEG格式相同的图片质量，希望能够减少图片档在网络上的发送时间。根据Google较早的测试，WebP的无损压缩比网络上找到的PNG档少了45％的文件大小，即使这些PNG档在使用pngcrush和PNGOUT处理过，WebP还是可以减少28％的文件大小。</p>\n<p align=\"right\">维基百科</p>\n\n</blockquote>\n<p>说人话？好。webp就是谷歌发布的一款可以在保证图片质量较为完整的情况下压缩率较高的网络图片格式。</p>\n<p>这么一看，webp岂不是无敌？实际上对于我们的项目它目前还是有局限性的：iOS端的微信小程序是不支持webp的。</p>\n<p>但是方法总是有的，既然iOS有它自己的优化方法，那么我就不让它搭上webp这班车了，我们可以判断设备的类型，然后针对安卓的手机进行优化。</p>\n<h2 id=\"教程\"><a href=\"#教程\" class=\"headerlink\" title=\"教程\"></a>教程</h2><h3 id=\"作案工具\"><a href=\"#作案工具\" class=\"headerlink\" title=\"作案工具\"></a>作案工具</h3><ul>\n<li>vscode</li>\n<li>微信小程序开发工具</li>\n</ul>\n<h3 id=\"手段\"><a href=\"#手段\" class=\"headerlink\" title=\"手段\"></a>手段</h3><p>首先我们需要定义一个全局变量作为判断是否为苹果设备的依据。所有的全局变量均在app.js里定义，在app.js中加入全局变量iOS，并用wx.getSystemInfo获取设备信息进行判断：</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"title class_\">App</span>(&#123;</span><br><span class=\"line\">  <span class=\"attr\">onLaunch</span>: <span class=\"keyword\">function</span> (<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">var</span> that = <span class=\"variable language_\">this</span>;</span><br><span class=\"line\">    wx.<span class=\"title function_\">getSystemInfo</span>(&#123;</span><br><span class=\"line\">      <span class=\"attr\">success</span>: <span class=\"function\">(<span class=\"params\">res</span>) =&gt;</span> &#123;</span><br><span class=\"line\">        <span class=\"variable language_\">console</span>.<span class=\"title function_\">log</span>(res);                           <span class=\"comment\">//res是一个对象，包含设备信息。其中res.system的内容则是设备系统的版本</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (res.<span class=\"property\">system</span>.<span class=\"title function_\">indexOf</span>(<span class=\"string\">&quot;iOS&quot;</span>) != -<span class=\"number\">1</span>) &#123;      <span class=\"comment\">//如果系统版本中有iOS三个字母，则判定为苹果设备</span></span><br><span class=\"line\">          that.<span class=\"property\">globalData</span>.<span class=\"property\">iOS</span> = <span class=\"literal\">true</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">    &#125;);</span><br><span class=\"line\">  <span class=\"attr\">globalData</span>: &#123;</span><br><span class=\"line\">    <span class=\"attr\">iOS</span>: <span class=\"literal\">false</span>,                                     <span class=\"comment\">//默认iOS的值为false</span></span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure>\n<p>到这里我们做好了设备类型的判断，接下来我们就要在独立的页面中进行链接替换。打个比方，如果这台设备是苹果，那么图片链接就使用 ../a.png ；如果是安卓，那么链接就换成 ../a.webp 。</p>\n<p>给出一个示例页面：</p>\n<p>html:</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">&lt;!--简单的页面，一个view标签，一张图，图片链接用js传入数据--&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">view</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;container&quot;</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">image</span> <span class=\"attr\">src</span> = <span class=\"string\">&#x27;&#123;&#123;img1&#125;&#125;&#x27;</span> /&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">view</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>javascript:</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"title class_\">Page</span>(&#123;                                              <span class=\"comment\">//传入页面的数据</span></span><br><span class=\"line\">  <span class=\"attr\">data</span>: &#123;</span><br><span class=\"line\">    <span class=\"attr\">img1</span>:<span class=\"string\">&quot;../a.png&quot;</span>,</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure>\n<p>这时我们在js中加入链接替换的内容：</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">const</span> app = <span class=\"title function_\">getApp</span>();                               <span class=\"comment\">//这行代码很重要，用来从app.js中获取全局变量</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"title class_\">Page</span>(&#123;</span><br><span class=\"line\">  <span class=\"attr\">data</span>: &#123;</span><br><span class=\"line\">    <span class=\"attr\">img1</span>:<span class=\"string\">&quot;../a.png&quot;</span>,</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  <span class=\"comment\">//事件处理函数，onLoad指在页面加载中的操作</span></span><br><span class=\"line\">  <span class=\"attr\">onLoad</span>: <span class=\"keyword\">function</span> (<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!app.<span class=\"property\">globalData</span>.<span class=\"property\">iOS</span>) &#123;                      <span class=\"comment\">//app.globalData.iOS即为上面定义的全局变量，如果iOS为假，则替换链接</span></span><br><span class=\"line\">      <span class=\"variable language_\">this</span>.<span class=\"title function_\">setData</span>(&#123;</span><br><span class=\"line\">        <span class=\"attr\">img1</span>:<span class=\"string\">&quot;../a.webp&quot;</span>,</span><br><span class=\"line\">      &#125;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure>\n<p>这样就完成了webp的替换，还是挺容易的，没有想象中的那么复杂。唯一的问题是页面多了以后替换起来会比较麻烦，不过如果都把图片的数据写在js里的话，替换起来会快很多。</p>\n"},{"title":"每周报告 20220906","date":"2022-09-12T11:09:08.000Z","mathjax":true,"_content":"\n# 2022/09/06\n\n论文阅读：\n\n- Learning Transferable Visual Models From Natural Language Supervision (CLIP)\n- PointCLIP: Point Cloud Understanding by CLIP\n- CLIPstyler: Image Style Transfer with a Single Text Condition\n- StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery\n- A Style-Based Generator Architecture for Generative Adversarial Networks (StyleGAN)\n\n实验：\n\n- 利用 CLIP 做简单的图片检索\n\n---\n\n## Learning Transferable Visual Models From Natural Language Supervision\n\nCLIP 模型的本质就是将分类任务化成了图文匹配任务，效果可与全监督方法相当。\n\nCLIP 模型使用的方法：对比学习，预测 n*n 对图像与文本数据，将图片分类任务转换成图文匹配任务。这个过程实际上就是引入了 NLP 给出的监督信号。\n\n---\n\n\n\n![image-20220904193225832](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904193225832.png)\n\n\n\n\n\n图中左侧，得到文本特征与图片的特征后可以看到，对角线上的元素都是图文匹配的，共有 N 个正样本，其余的元素都是负样本，共有 N*N-N 个。其中，文本数据使用Transformer，图片数据用了两种模型，ResNet 和 Vision Transformer (ViT)。\n\nCLIP 模型训练所用的数据集较为庞大，包含从互联网上各种公开资源收集的4亿对图像、文本，CLIP是从头开始训练的，没有使用预训练的初始参数。\n\n---\n\n## PointCLIP: Point Cloud Understanding by CLIP\n\n这是第一篇把 CLIP 用在点云上的工作，重点解决的是如何利用 CLIP 去理解点云数据。\n\n对于点云这种 3D 数据的形式，为了抽取点云的特征，采用了投影的方式，把三维的点朝 M 个方向投影，得到 M 张不同方向的 2D 点云深度图。这种图文章中提到是符合透视画法的特征：近大远小，即更加符合照片的特征。每个方向上得到的图片都是没有色彩信息的，作者这里出于运算时间与资源消耗的考虑，直接将图层复制成三个完全一致的通道传入编码器。\n\n\n\n![image-20220911090624239](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220911090624239.png)\n\n\n\n---\n\n关于这种方法（PointCLIP）的效果，文章先是做了 zero-shot 分类的实验，在 ModelNet40 数据集上准确率是 20.18%，在 ScanObjectNN 数据 集上的准确率只有 15.38%，虽然这说明这样的方法是有一定效果的，但依然没法与现有的训练好的直接学习 3D 数据的网络相比较。\n\n然后作者提到 ModelNet40 需要六个方向的视图以不同的权重贡献作出最后的分类结果，在 ScanObjectNN 这样带有噪点（地板和天花板）的数据中，顶视图与底视图几乎提供不了任何有效信息（所以效果不好）。对于视图的数目，作者尝试了 1,4,6,8,10,12，在 zero-shot 任务里 6 个视角效果最佳（后面16-shot实验里10视角最佳）；关于不同视图的贡献，实验里右视图和左视图最多。\n\n当然，和 CLIP 有关的任务里，肯定也要涉及到 Prompt Design 的问题，作者这里用了以下的 Prompt:\n\n```markdown\na photo of a [CLASS].\na point cloud photo of a [CLASS].\npoint cloud of a [CLASS].\npoint cloud of a big [CLASS].\t\t    # 16-shot best\npoint cloud depth map of a [CLASS]. # zero-shot best\n```\n\n 再就是图像编码器的选择，作者使用RN.*16的效果在zero-shot中表现最佳，使用RN101在16-shot中表现最佳。\n\n---\n\nzero-shot 的方式相对于有监督的方法效果较差，于是作者又加了一个小网络 few-shot finetune 一下。将 M 个特征图拉成一维然后经过两个全连接得到一个全局特征，然后用这个全局特征在通过以下公式得到 M 个特征，与原先的特征残差连接得到新的 M 个特征。\n$$\nf^a_i = f_i + ReLU(f_{global}W^T_{3i})\n$$\n![image-20220912100708331](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220912100708331.png)\n\n\n\n这样一来，效果就有了大幅度的提升，在 ModelNet40上的结果从20.18%提升到了87.20%，据说是只用了1/10的数据就达到了这样的效果。\n\n---\n\n最后，作者考虑是否可以通过模型融合的方式得到更好的效果，简单地说就是将不同模型预测的各类别分数相加得到最终分数。通过实验，用 PointCLIP 和其他有监督方法得到的模型融合，结果有所提升。PointCLIP 和 CurveNet 融合的结果达到了 sota。两个效果最好的有监督模型融合得到的效果与之相比反而达不到最高。\n\n![image-20220912102819460](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220912102819460.png)\n\n\n\n---\n\n## CLIPstyler: Image Style Transfer with a Single Text Condition\n\n现有的神经风格迁移需要提供参考图像才能将纹理信息转移给目标图像，但是在很多情况下用户手头是没有参考图像的。这篇文章提出了 CLIPstyler，旨在使用简单的对风格的文字描述来进行风格迁移。\n\n问题的两个难点在于：1）如何释放来自 CLIP 模型的语义化的“材质”信息，将其应用到目标图片上；2）如何规范化训练，使得输出的图像不会受质量上的影响。\n\n作者这里设计了一个 StyleNet 用来捕捉内容图像的分层视觉特征，同时在深度特征空间中对图像进行风格化处理，以获得真实的纹理表示。\n\n\n\n关于损失函数，作者这里阐述了几个他们用到的概念：\n\n---\n\n1) CLIP loss \n\n最简单的基于CLIP的最小化全局损失函数如下：\n$$\nL_{global}=D_{CLIP}(f(I_c),t_{sty})\n$$\n$D_{CLIP}$ 指在 CLIP 空间的余弦距离。但是如果用这样的损失函数，图像的质量会受损，而且图像优化过程的稳定性无法保证，于是他们参考了 StyleGAN-NADA 提出的一种定向的 CLIP loss 函数，将图像-文本对的输入和输出在CLIP空间对齐，从而解决了上面的问题，这个损失函数如下：\n$$\n\\Delta T = E_T (t_{sty}) − E_T (t_{src}), \\\\\n\\Delta I = E_I (f (I_c)) − E_I (I_c), \\\\\nL_{dir} = 1 − \\frac{\\Delta I · \\Delta T }{\\lvert \\Delta I \\rvert \\lvert \\Delta T\\rvert }\n$$\n其中 $E_T, E_I$ 指的就是 CLIP 的文本和图像编码器，$t_{sty},t_{src}$ 则是风格描述文本和原图像文本（默认设置为：Photo）\n\n---\n\n2. PatchCLIP loss \n\n上文的 $L_{dir}$ 在调整预训练的生成式模型效果较好，但是和作者的目标不完全一样，单纯使用这个损失函数也会造成输出质量的降低。为了解决这个问题，作者提出了一种新的专为纹理迁移设计的损失函数 PatchCLIP loss。他们将通过 StyleNet 得到的图像随机地分成一些块（块的尺寸固定），然后对不同的块做不同的数据增强，这里的增强统一使用了透视变换\n\n![image-20220912203957486](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220912203957486.png)\n\n透视变换的好处是，所有的分块在不同的点观察时都能够具有相同（或类似）的语义，CLIP 模型的语义信息也就可以被重建为更类似于 3D 的结构。\n\n---\n\n3. Threshold rejection\n\n由于分块的随机性，以上的方法很可能导致在某些已经具有较为符合目标风格的分块上进行过度的风格化（因为这样的分块更加容易被优化），针对这个情况，作者加入正则化，设定了一个阈值，当分块得分低于这一阈值时忽略这一分块的梯度优化过程。这样就得到了整个$L_{patch}$的定义：\n$$\n\\Delta T = E_T (t_{sty}) − E_T (t_{src}), \\\\\n\\Delta I = E_I (aug(\\hat{I}^i_{cs})) − E_I (I_c), \\\\\nL_{dir} = 1 − \\frac{\\Delta I · \\Delta T }{\\lvert \\Delta I \\rvert \\lvert \\Delta T\\rvert } \\\\\nL_{patch}=\\frac{1}{N}\\sum^N_iR(l^i_{patch},\\tau) \\\\\n\n\n\\begin{aligned}\n\t\\text{where} \\quad R(s,\\tau)=\n\t\\begin{cases}\n\t\t0,&if\\quad s\\le\\tau \\\\\n\t\ts,&\\text{otherwise}\n\t\\end{cases}\n\\end{aligned}\n$$\n\n---\n\n4. Total loss\n\n最终总体的损失函数还加入了 content loss $L_c$ 和 total varation regularization loss $L_{tv}$：\n$$\nL_{total} = λ_dL_{dir} + λ_pL_{patch} + λ_cL_c + λ_{tv}L_{tv}\n$$\n\n---\n\nCLIPstyler 有可供尝试的 demo，网址：https://replicate.com/paper11667/clipstyler\n\n![clipstyler](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/clipstyler.gif)\n\n左图原图，文本 “Chinese style” ，训练迭代次数 100 次，右图为输出图像\n\n---\n\n## StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery\n\nStyleGAN 可以用于生成高质量且真实的图像。然而，想要发掘潜层对于图像的影响对应怎样的语义信息，则需要大量的人工检查以及注释。在这项工作中，作者引入了 CLIP 模型，以帮助 StyleGAN 解除大量人力工作的限制。\n\nStyleGAN 的原理主要是通过 latent code 去控制图像的风格。对于一张图片，先通过 image inversion 将图片表示为latent code，然后去编辑它们，就可以达到编辑图像的目的。\n\nStyleGAN 的网络结构包含两个部分，第一个是 Mapping network，由隐藏变量 z 生成 中间隐藏变量 w的过程，这个 w 就是用来控制生成图像的风格； 第二个是 Synthesis network，它的作用是生成图像。\n\n---\n\n![image-20220913083904923](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220913083904923.png)\n\n---\n\n作者总结了三种将 CLIP 和 StyleGAN 结合的方式：\n\n### Latent optimization\n\n这里利用 CLIP 模型计算损失，通过对 latent code 不断迭代，达到图像编辑的目的，需要很多次的迭代，耗时较长。\n\n要解算的是下面这个问题：\n$$\n\\mathop{arg\\:min}_{w∈W+}D_{CLIP}(G(w), t) + λ_{L2} ∥w − ws∥_2 + λ_{ID}L_{ID}(w),\n$$\n\n---\n\n### Latent Mapper\n\n这个方法是指定一些 text prompt 进行训练，模型训练好后只需一次 forward 可以得到结果，通过指定 text prompt，可以预先控制图像编辑的范围区域，这就类似于利用属性分类器来辅助训练，只不过利用CLIP模型可以省略掉分类器，这种方法效率更高。\n\nStyleGAN2 的不同分辨率层控制了不同程度的图像语义，一般分为 coarse, medium, fine 三组。作者通过三个全连接层网络，对 latent code 进行重新编辑，残差连接后进入 StyleGAN 用来生成图像。\n\n![image-20220912235028225](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220912235028225.png)\n\n---\n\n### Global Directions\n\n这部分比较复杂，还没有搞明白。\n\n---\n\n## 实验：利用 CLIP 做简单的图片检索\n\n基本流程就是先读入用户的输入，即搜索图片的关键词/句，然后将文本编码得到特征，然后分别对应所有图片的特征计算相似度，取相似度最高的三张图片输出。\n\n图片我没有找网上的一些数据集（因为大部分应该已经有人测试过了），我想看一看这个模型到底有多强大，于是我选取了三十几张我自己拍的一些生活照，进行了一下图像压缩，控制在500kb之内。\n\n展示的部分是利用 Gradio 搭建的，可以迅速将模型转化为交互式的界面。\n\n下面展示几张效果：\n\n---\n\n![image-20220913084627975](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220913084627975.png)\n\n---\n\n![image-20220913084705720](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220913084705720.png)\n\n---\n\n![image-20220913084744050](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220913084744050.png)\n\n---\n\n测试的时候发现有意思的地方是这个例子：我这里是输入了这个midi键盘的名称（是刻在键盘上的文字），结果它也能给我识别出来：\n\n![image-20220913084843362](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220913084843362.png)\n\n---\n\n![image-20220904200950871](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904200950871.png)\n\n这是否说明 CLIP 模型直接具有 OCR 的能力呢？我的理解是 CLIP 在大量的样本训练中，将每个字母也看做一个对象去学习了他们的特征，所以能够直接识别到图片中的文字了。\n\n---\n\n代码：(github仓库：https://github.com/1099255210/simple-CLIP-based-image-retrival)\n\n```python\nimport gradio as gr\n\nimport torch\nimport clip\nimport os\nfrom PIL import Image\nfrom IPython.display import display\nfrom tqdm.notebook import tqdm\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\n\ndata_location =  \"./imgs\"\nimg_dict = {}\nfor inx, f in enumerate(os.listdir(data_location)):\n  img_dict[inx] = f\nimg_nums = len(img_dict)\n```\n\n---\n\n```python\ndef fn(instr):\n  text_input = clip.tokenize(instr).to(device)\n  with torch.no_grad():\n    text_f = model.encode_text(text_input)\n  text_f /= text_f.norm(dim=-1, keepdim=True)\n\n  sim = {}\n\n  for i in range(img_nums):\n    \n    image_path = f'{data_location}/{img_dict[i]}'\n    img = Image.open(image_path)\n    img_input = preprocess(img).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n      img_f = model.encode_image(img_input)\n\n    img_f /= img_f.norm(dim=-1, keepdim=True)\n    similarity = 100 * img_f @ text_f.T\n    sim[i] = similarity\n\n  res = sorted(sim.items(), key=lambda s:s[1], reverse=True)\n  retval = [ f'{data_location}/{img_dict[res[i][0]]}' for i in range(3) ]\n  return retval\n```\n\n---\n\n```python\ncss_output = \".object-contain {height: 100px !important}\"\n\ndemo = gr.Interface(\n  fn = fn,\n  inputs = 'text', \n  outputs = [gr.Image(type='file', label=None) for _ in range(3)],\n  css = css_output,\n)\ndemo.launch(enable_queue = True,)\n```\n\n","source":"_posts/每周报告 20220906.md","raw":"---\ntitle: 每周报告 20220906\ndate: 2022-09-12 19:09:08\ntags:\nmathjax: true\n---\n\n# 2022/09/06\n\n论文阅读：\n\n- Learning Transferable Visual Models From Natural Language Supervision (CLIP)\n- PointCLIP: Point Cloud Understanding by CLIP\n- CLIPstyler: Image Style Transfer with a Single Text Condition\n- StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery\n- A Style-Based Generator Architecture for Generative Adversarial Networks (StyleGAN)\n\n实验：\n\n- 利用 CLIP 做简单的图片检索\n\n---\n\n## Learning Transferable Visual Models From Natural Language Supervision\n\nCLIP 模型的本质就是将分类任务化成了图文匹配任务，效果可与全监督方法相当。\n\nCLIP 模型使用的方法：对比学习，预测 n*n 对图像与文本数据，将图片分类任务转换成图文匹配任务。这个过程实际上就是引入了 NLP 给出的监督信号。\n\n---\n\n\n\n![image-20220904193225832](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904193225832.png)\n\n\n\n\n\n图中左侧，得到文本特征与图片的特征后可以看到，对角线上的元素都是图文匹配的，共有 N 个正样本，其余的元素都是负样本，共有 N*N-N 个。其中，文本数据使用Transformer，图片数据用了两种模型，ResNet 和 Vision Transformer (ViT)。\n\nCLIP 模型训练所用的数据集较为庞大，包含从互联网上各种公开资源收集的4亿对图像、文本，CLIP是从头开始训练的，没有使用预训练的初始参数。\n\n---\n\n## PointCLIP: Point Cloud Understanding by CLIP\n\n这是第一篇把 CLIP 用在点云上的工作，重点解决的是如何利用 CLIP 去理解点云数据。\n\n对于点云这种 3D 数据的形式，为了抽取点云的特征，采用了投影的方式，把三维的点朝 M 个方向投影，得到 M 张不同方向的 2D 点云深度图。这种图文章中提到是符合透视画法的特征：近大远小，即更加符合照片的特征。每个方向上得到的图片都是没有色彩信息的，作者这里出于运算时间与资源消耗的考虑，直接将图层复制成三个完全一致的通道传入编码器。\n\n\n\n![image-20220911090624239](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220911090624239.png)\n\n\n\n---\n\n关于这种方法（PointCLIP）的效果，文章先是做了 zero-shot 分类的实验，在 ModelNet40 数据集上准确率是 20.18%，在 ScanObjectNN 数据 集上的准确率只有 15.38%，虽然这说明这样的方法是有一定效果的，但依然没法与现有的训练好的直接学习 3D 数据的网络相比较。\n\n然后作者提到 ModelNet40 需要六个方向的视图以不同的权重贡献作出最后的分类结果，在 ScanObjectNN 这样带有噪点（地板和天花板）的数据中，顶视图与底视图几乎提供不了任何有效信息（所以效果不好）。对于视图的数目，作者尝试了 1,4,6,8,10,12，在 zero-shot 任务里 6 个视角效果最佳（后面16-shot实验里10视角最佳）；关于不同视图的贡献，实验里右视图和左视图最多。\n\n当然，和 CLIP 有关的任务里，肯定也要涉及到 Prompt Design 的问题，作者这里用了以下的 Prompt:\n\n```markdown\na photo of a [CLASS].\na point cloud photo of a [CLASS].\npoint cloud of a [CLASS].\npoint cloud of a big [CLASS].\t\t    # 16-shot best\npoint cloud depth map of a [CLASS]. # zero-shot best\n```\n\n 再就是图像编码器的选择，作者使用RN.*16的效果在zero-shot中表现最佳，使用RN101在16-shot中表现最佳。\n\n---\n\nzero-shot 的方式相对于有监督的方法效果较差，于是作者又加了一个小网络 few-shot finetune 一下。将 M 个特征图拉成一维然后经过两个全连接得到一个全局特征，然后用这个全局特征在通过以下公式得到 M 个特征，与原先的特征残差连接得到新的 M 个特征。\n$$\nf^a_i = f_i + ReLU(f_{global}W^T_{3i})\n$$\n![image-20220912100708331](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220912100708331.png)\n\n\n\n这样一来，效果就有了大幅度的提升，在 ModelNet40上的结果从20.18%提升到了87.20%，据说是只用了1/10的数据就达到了这样的效果。\n\n---\n\n最后，作者考虑是否可以通过模型融合的方式得到更好的效果，简单地说就是将不同模型预测的各类别分数相加得到最终分数。通过实验，用 PointCLIP 和其他有监督方法得到的模型融合，结果有所提升。PointCLIP 和 CurveNet 融合的结果达到了 sota。两个效果最好的有监督模型融合得到的效果与之相比反而达不到最高。\n\n![image-20220912102819460](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220912102819460.png)\n\n\n\n---\n\n## CLIPstyler: Image Style Transfer with a Single Text Condition\n\n现有的神经风格迁移需要提供参考图像才能将纹理信息转移给目标图像，但是在很多情况下用户手头是没有参考图像的。这篇文章提出了 CLIPstyler，旨在使用简单的对风格的文字描述来进行风格迁移。\n\n问题的两个难点在于：1）如何释放来自 CLIP 模型的语义化的“材质”信息，将其应用到目标图片上；2）如何规范化训练，使得输出的图像不会受质量上的影响。\n\n作者这里设计了一个 StyleNet 用来捕捉内容图像的分层视觉特征，同时在深度特征空间中对图像进行风格化处理，以获得真实的纹理表示。\n\n\n\n关于损失函数，作者这里阐述了几个他们用到的概念：\n\n---\n\n1) CLIP loss \n\n最简单的基于CLIP的最小化全局损失函数如下：\n$$\nL_{global}=D_{CLIP}(f(I_c),t_{sty})\n$$\n$D_{CLIP}$ 指在 CLIP 空间的余弦距离。但是如果用这样的损失函数，图像的质量会受损，而且图像优化过程的稳定性无法保证，于是他们参考了 StyleGAN-NADA 提出的一种定向的 CLIP loss 函数，将图像-文本对的输入和输出在CLIP空间对齐，从而解决了上面的问题，这个损失函数如下：\n$$\n\\Delta T = E_T (t_{sty}) − E_T (t_{src}), \\\\\n\\Delta I = E_I (f (I_c)) − E_I (I_c), \\\\\nL_{dir} = 1 − \\frac{\\Delta I · \\Delta T }{\\lvert \\Delta I \\rvert \\lvert \\Delta T\\rvert }\n$$\n其中 $E_T, E_I$ 指的就是 CLIP 的文本和图像编码器，$t_{sty},t_{src}$ 则是风格描述文本和原图像文本（默认设置为：Photo）\n\n---\n\n2. PatchCLIP loss \n\n上文的 $L_{dir}$ 在调整预训练的生成式模型效果较好，但是和作者的目标不完全一样，单纯使用这个损失函数也会造成输出质量的降低。为了解决这个问题，作者提出了一种新的专为纹理迁移设计的损失函数 PatchCLIP loss。他们将通过 StyleNet 得到的图像随机地分成一些块（块的尺寸固定），然后对不同的块做不同的数据增强，这里的增强统一使用了透视变换\n\n![image-20220912203957486](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220912203957486.png)\n\n透视变换的好处是，所有的分块在不同的点观察时都能够具有相同（或类似）的语义，CLIP 模型的语义信息也就可以被重建为更类似于 3D 的结构。\n\n---\n\n3. Threshold rejection\n\n由于分块的随机性，以上的方法很可能导致在某些已经具有较为符合目标风格的分块上进行过度的风格化（因为这样的分块更加容易被优化），针对这个情况，作者加入正则化，设定了一个阈值，当分块得分低于这一阈值时忽略这一分块的梯度优化过程。这样就得到了整个$L_{patch}$的定义：\n$$\n\\Delta T = E_T (t_{sty}) − E_T (t_{src}), \\\\\n\\Delta I = E_I (aug(\\hat{I}^i_{cs})) − E_I (I_c), \\\\\nL_{dir} = 1 − \\frac{\\Delta I · \\Delta T }{\\lvert \\Delta I \\rvert \\lvert \\Delta T\\rvert } \\\\\nL_{patch}=\\frac{1}{N}\\sum^N_iR(l^i_{patch},\\tau) \\\\\n\n\n\\begin{aligned}\n\t\\text{where} \\quad R(s,\\tau)=\n\t\\begin{cases}\n\t\t0,&if\\quad s\\le\\tau \\\\\n\t\ts,&\\text{otherwise}\n\t\\end{cases}\n\\end{aligned}\n$$\n\n---\n\n4. Total loss\n\n最终总体的损失函数还加入了 content loss $L_c$ 和 total varation regularization loss $L_{tv}$：\n$$\nL_{total} = λ_dL_{dir} + λ_pL_{patch} + λ_cL_c + λ_{tv}L_{tv}\n$$\n\n---\n\nCLIPstyler 有可供尝试的 demo，网址：https://replicate.com/paper11667/clipstyler\n\n![clipstyler](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/clipstyler.gif)\n\n左图原图，文本 “Chinese style” ，训练迭代次数 100 次，右图为输出图像\n\n---\n\n## StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery\n\nStyleGAN 可以用于生成高质量且真实的图像。然而，想要发掘潜层对于图像的影响对应怎样的语义信息，则需要大量的人工检查以及注释。在这项工作中，作者引入了 CLIP 模型，以帮助 StyleGAN 解除大量人力工作的限制。\n\nStyleGAN 的原理主要是通过 latent code 去控制图像的风格。对于一张图片，先通过 image inversion 将图片表示为latent code，然后去编辑它们，就可以达到编辑图像的目的。\n\nStyleGAN 的网络结构包含两个部分，第一个是 Mapping network，由隐藏变量 z 生成 中间隐藏变量 w的过程，这个 w 就是用来控制生成图像的风格； 第二个是 Synthesis network，它的作用是生成图像。\n\n---\n\n![image-20220913083904923](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220913083904923.png)\n\n---\n\n作者总结了三种将 CLIP 和 StyleGAN 结合的方式：\n\n### Latent optimization\n\n这里利用 CLIP 模型计算损失，通过对 latent code 不断迭代，达到图像编辑的目的，需要很多次的迭代，耗时较长。\n\n要解算的是下面这个问题：\n$$\n\\mathop{arg\\:min}_{w∈W+}D_{CLIP}(G(w), t) + λ_{L2} ∥w − ws∥_2 + λ_{ID}L_{ID}(w),\n$$\n\n---\n\n### Latent Mapper\n\n这个方法是指定一些 text prompt 进行训练，模型训练好后只需一次 forward 可以得到结果，通过指定 text prompt，可以预先控制图像编辑的范围区域，这就类似于利用属性分类器来辅助训练，只不过利用CLIP模型可以省略掉分类器，这种方法效率更高。\n\nStyleGAN2 的不同分辨率层控制了不同程度的图像语义，一般分为 coarse, medium, fine 三组。作者通过三个全连接层网络，对 latent code 进行重新编辑，残差连接后进入 StyleGAN 用来生成图像。\n\n![image-20220912235028225](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220912235028225.png)\n\n---\n\n### Global Directions\n\n这部分比较复杂，还没有搞明白。\n\n---\n\n## 实验：利用 CLIP 做简单的图片检索\n\n基本流程就是先读入用户的输入，即搜索图片的关键词/句，然后将文本编码得到特征，然后分别对应所有图片的特征计算相似度，取相似度最高的三张图片输出。\n\n图片我没有找网上的一些数据集（因为大部分应该已经有人测试过了），我想看一看这个模型到底有多强大，于是我选取了三十几张我自己拍的一些生活照，进行了一下图像压缩，控制在500kb之内。\n\n展示的部分是利用 Gradio 搭建的，可以迅速将模型转化为交互式的界面。\n\n下面展示几张效果：\n\n---\n\n![image-20220913084627975](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220913084627975.png)\n\n---\n\n![image-20220913084705720](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220913084705720.png)\n\n---\n\n![image-20220913084744050](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220913084744050.png)\n\n---\n\n测试的时候发现有意思的地方是这个例子：我这里是输入了这个midi键盘的名称（是刻在键盘上的文字），结果它也能给我识别出来：\n\n![image-20220913084843362](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220913084843362.png)\n\n---\n\n![image-20220904200950871](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904200950871.png)\n\n这是否说明 CLIP 模型直接具有 OCR 的能力呢？我的理解是 CLIP 在大量的样本训练中，将每个字母也看做一个对象去学习了他们的特征，所以能够直接识别到图片中的文字了。\n\n---\n\n代码：(github仓库：https://github.com/1099255210/simple-CLIP-based-image-retrival)\n\n```python\nimport gradio as gr\n\nimport torch\nimport clip\nimport os\nfrom PIL import Image\nfrom IPython.display import display\nfrom tqdm.notebook import tqdm\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\n\ndata_location =  \"./imgs\"\nimg_dict = {}\nfor inx, f in enumerate(os.listdir(data_location)):\n  img_dict[inx] = f\nimg_nums = len(img_dict)\n```\n\n---\n\n```python\ndef fn(instr):\n  text_input = clip.tokenize(instr).to(device)\n  with torch.no_grad():\n    text_f = model.encode_text(text_input)\n  text_f /= text_f.norm(dim=-1, keepdim=True)\n\n  sim = {}\n\n  for i in range(img_nums):\n    \n    image_path = f'{data_location}/{img_dict[i]}'\n    img = Image.open(image_path)\n    img_input = preprocess(img).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n      img_f = model.encode_image(img_input)\n\n    img_f /= img_f.norm(dim=-1, keepdim=True)\n    similarity = 100 * img_f @ text_f.T\n    sim[i] = similarity\n\n  res = sorted(sim.items(), key=lambda s:s[1], reverse=True)\n  retval = [ f'{data_location}/{img_dict[res[i][0]]}' for i in range(3) ]\n  return retval\n```\n\n---\n\n```python\ncss_output = \".object-contain {height: 100px !important}\"\n\ndemo = gr.Interface(\n  fn = fn,\n  inputs = 'text', \n  outputs = [gr.Image(type='file', label=None) for _ in range(3)],\n  css = css_output,\n)\ndemo.launch(enable_queue = True,)\n```\n\n","slug":"每周报告 20220906","published":1,"updated":"2022-09-14T11:39:59.860Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clgyiaru0000ijku7drdsbbfz","content":"<h1 id=\"2022-09-06\"><a href=\"#2022-09-06\" class=\"headerlink\" title=\"2022/09/06\"></a>2022/09/06</h1><p>论文阅读：</p>\n<ul>\n<li>Learning Transferable Visual Models From Natural Language Supervision (CLIP)</li>\n<li>PointCLIP: Point Cloud Understanding by CLIP</li>\n<li>CLIPstyler: Image Style Transfer with a Single Text Condition</li>\n<li>StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery</li>\n<li>A Style-Based Generator Architecture for Generative Adversarial Networks (StyleGAN)</li>\n</ul>\n<p>实验：</p>\n<ul>\n<li>利用 CLIP 做简单的图片检索</li>\n</ul>\n<hr>\n<h2 id=\"Learning-Transferable-Visual-Models-From-Natural-Language-Supervision\"><a href=\"#Learning-Transferable-Visual-Models-From-Natural-Language-Supervision\" class=\"headerlink\" title=\"Learning Transferable Visual Models From Natural Language Supervision\"></a>Learning Transferable Visual Models From Natural Language Supervision</h2><p>CLIP 模型的本质就是将分类任务化成了图文匹配任务，效果可与全监督方法相当。</p>\n<p>CLIP 模型使用的方法：对比学习，预测 n*n 对图像与文本数据，将图片分类任务转换成图文匹配任务。这个过程实际上就是引入了 NLP 给出的监督信号。</p>\n<hr>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904193225832.png\" alt=\"image-20220904193225832\"></p>\n<p>图中左侧，得到文本特征与图片的特征后可以看到，对角线上的元素都是图文匹配的，共有 N 个正样本，其余的元素都是负样本，共有 N*N-N 个。其中，文本数据使用Transformer，图片数据用了两种模型，ResNet 和 Vision Transformer (ViT)。</p>\n<p>CLIP 模型训练所用的数据集较为庞大，包含从互联网上各种公开资源收集的4亿对图像、文本，CLIP是从头开始训练的，没有使用预训练的初始参数。</p>\n<hr>\n<h2 id=\"PointCLIP-Point-Cloud-Understanding-by-CLIP\"><a href=\"#PointCLIP-Point-Cloud-Understanding-by-CLIP\" class=\"headerlink\" title=\"PointCLIP: Point Cloud Understanding by CLIP\"></a>PointCLIP: Point Cloud Understanding by CLIP</h2><p>这是第一篇把 CLIP 用在点云上的工作，重点解决的是如何利用 CLIP 去理解点云数据。</p>\n<p>对于点云这种 3D 数据的形式，为了抽取点云的特征，采用了投影的方式，把三维的点朝 M 个方向投影，得到 M 张不同方向的 2D 点云深度图。这种图文章中提到是符合透视画法的特征：近大远小，即更加符合照片的特征。每个方向上得到的图片都是没有色彩信息的，作者这里出于运算时间与资源消耗的考虑，直接将图层复制成三个完全一致的通道传入编码器。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220911090624239.png\" alt=\"image-20220911090624239\"></p>\n<hr>\n<p>关于这种方法（PointCLIP）的效果，文章先是做了 zero-shot 分类的实验，在 ModelNet40 数据集上准确率是 20.18%，在 ScanObjectNN 数据 集上的准确率只有 15.38%，虽然这说明这样的方法是有一定效果的，但依然没法与现有的训练好的直接学习 3D 数据的网络相比较。</p>\n<p>然后作者提到 ModelNet40 需要六个方向的视图以不同的权重贡献作出最后的分类结果，在 ScanObjectNN 这样带有噪点（地板和天花板）的数据中，顶视图与底视图几乎提供不了任何有效信息（所以效果不好）。对于视图的数目，作者尝试了 1,4,6,8,10,12，在 zero-shot 任务里 6 个视角效果最佳（后面16-shot实验里10视角最佳）；关于不同视图的贡献，实验里右视图和左视图最多。</p>\n<p>当然，和 CLIP 有关的任务里，肯定也要涉及到 Prompt Design 的问题，作者这里用了以下的 Prompt:</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a photo of a [CLASS].</span><br><span class=\"line\">a point cloud photo of a [CLASS].</span><br><span class=\"line\">point cloud of a [CLASS].</span><br><span class=\"line\">point cloud of a big [CLASS].\t\t    # 16-shot best</span><br><span class=\"line\">point cloud depth map of a [CLASS]. # zero-shot best</span><br></pre></td></tr></table></figure>\n<p> 再就是图像编码器的选择，作者使用RN.*16的效果在zero-shot中表现最佳，使用RN101在16-shot中表现最佳。</p>\n<hr>\n<p>zero-shot 的方式相对于有监督的方法效果较差，于是作者又加了一个小网络 few-shot finetune 一下。将 M 个特征图拉成一维然后经过两个全连接得到一个全局特征，然后用这个全局特征在通过以下公式得到 M 个特征，与原先的特征残差连接得到新的 M 个特征。</p>\n<script type=\"math/tex; mode=display\">\nf^a_i = f_i + ReLU(f_{global}W^T_{3i})</script><p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220912100708331.png\" alt=\"image-20220912100708331\"></p>\n<p>这样一来，效果就有了大幅度的提升，在 ModelNet40上的结果从20.18%提升到了87.20%，据说是只用了1/10的数据就达到了这样的效果。</p>\n<hr>\n<p>最后，作者考虑是否可以通过模型融合的方式得到更好的效果，简单地说就是将不同模型预测的各类别分数相加得到最终分数。通过实验，用 PointCLIP 和其他有监督方法得到的模型融合，结果有所提升。PointCLIP 和 CurveNet 融合的结果达到了 sota。两个效果最好的有监督模型融合得到的效果与之相比反而达不到最高。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220912102819460.png\" alt=\"image-20220912102819460\"></p>\n<hr>\n<h2 id=\"CLIPstyler-Image-Style-Transfer-with-a-Single-Text-Condition\"><a href=\"#CLIPstyler-Image-Style-Transfer-with-a-Single-Text-Condition\" class=\"headerlink\" title=\"CLIPstyler: Image Style Transfer with a Single Text Condition\"></a>CLIPstyler: Image Style Transfer with a Single Text Condition</h2><p>现有的神经风格迁移需要提供参考图像才能将纹理信息转移给目标图像，但是在很多情况下用户手头是没有参考图像的。这篇文章提出了 CLIPstyler，旨在使用简单的对风格的文字描述来进行风格迁移。</p>\n<p>问题的两个难点在于：1）如何释放来自 CLIP 模型的语义化的“材质”信息，将其应用到目标图片上；2）如何规范化训练，使得输出的图像不会受质量上的影响。</p>\n<p>作者这里设计了一个 StyleNet 用来捕捉内容图像的分层视觉特征，同时在深度特征空间中对图像进行风格化处理，以获得真实的纹理表示。</p>\n<p>关于损失函数，作者这里阐述了几个他们用到的概念：</p>\n<hr>\n<p>1) CLIP loss </p>\n<p>最简单的基于CLIP的最小化全局损失函数如下：</p>\n<script type=\"math/tex; mode=display\">\nL_{global}=D_{CLIP}(f(I_c),t_{sty})</script><p>$D_{CLIP}$ 指在 CLIP 空间的余弦距离。但是如果用这样的损失函数，图像的质量会受损，而且图像优化过程的稳定性无法保证，于是他们参考了 StyleGAN-NADA 提出的一种定向的 CLIP loss 函数，将图像-文本对的输入和输出在CLIP空间对齐，从而解决了上面的问题，这个损失函数如下：</p>\n<script type=\"math/tex; mode=display\">\n\\Delta T = E_T (t_{sty}) − E_T (t_{src}), \\\\\n\\Delta I = E_I (f (I_c)) − E_I (I_c), \\\\\nL_{dir} = 1 − \\frac{\\Delta I · \\Delta T }{\\lvert \\Delta I \\rvert \\lvert \\Delta T\\rvert }</script><p>其中 $E_T, E_I$ 指的就是 CLIP 的文本和图像编码器，$t_{sty},t_{src}$ 则是风格描述文本和原图像文本（默认设置为：Photo）</p>\n<hr>\n<ol>\n<li>PatchCLIP loss </li>\n</ol>\n<p>上文的 $L_{dir}$ 在调整预训练的生成式模型效果较好，但是和作者的目标不完全一样，单纯使用这个损失函数也会造成输出质量的降低。为了解决这个问题，作者提出了一种新的专为纹理迁移设计的损失函数 PatchCLIP loss。他们将通过 StyleNet 得到的图像随机地分成一些块（块的尺寸固定），然后对不同的块做不同的数据增强，这里的增强统一使用了透视变换</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220912203957486.png\" alt=\"image-20220912203957486\"></p>\n<p>透视变换的好处是，所有的分块在不同的点观察时都能够具有相同（或类似）的语义，CLIP 模型的语义信息也就可以被重建为更类似于 3D 的结构。</p>\n<hr>\n<ol>\n<li>Threshold rejection</li>\n</ol>\n<p>由于分块的随机性，以上的方法很可能导致在某些已经具有较为符合目标风格的分块上进行过度的风格化（因为这样的分块更加容易被优化），针对这个情况，作者加入正则化，设定了一个阈值，当分块得分低于这一阈值时忽略这一分块的梯度优化过程。这样就得到了整个$L_{patch}$的定义：</p>\n<script type=\"math/tex; mode=display\">\n\\Delta T = E_T (t_{sty}) − E_T (t_{src}), \\\\\n\\Delta I = E_I (aug(\\hat{I}^i_{cs})) − E_I (I_c), \\\\\nL_{dir} = 1 − \\frac{\\Delta I · \\Delta T }{\\lvert \\Delta I \\rvert \\lvert \\Delta T\\rvert } \\\\\nL_{patch}=\\frac{1}{N}\\sum^N_iR(l^i_{patch},\\tau) \\\\\n\n\n\\begin{aligned}\n    \\text{where} \\quad R(s,\\tau)=\n    \\begin{cases}\n        0,&if\\quad s\\le\\tau \\\\\n        s,&\\text{otherwise}\n    \\end{cases}\n\\end{aligned}</script><hr>\n<ol>\n<li>Total loss</li>\n</ol>\n<p>最终总体的损失函数还加入了 content loss $L_c$ 和 total varation regularization loss $L_{tv}$：</p>\n<script type=\"math/tex; mode=display\">\nL_{total} = λ_dL_{dir} + λ_pL_{patch} + λ_cL_c + λ_{tv}L_{tv}</script><hr>\n<p>CLIPstyler 有可供尝试的 demo，网址：<a href=\"https://replicate.com/paper11667/clipstyler\">https://replicate.com/paper11667/clipstyler</a></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/clipstyler.gif\" alt=\"clipstyler\"></p>\n<p>左图原图，文本 “Chinese style” ，训练迭代次数 100 次，右图为输出图像</p>\n<hr>\n<h2 id=\"StyleCLIP-Text-Driven-Manipulation-of-StyleGAN-Imagery\"><a href=\"#StyleCLIP-Text-Driven-Manipulation-of-StyleGAN-Imagery\" class=\"headerlink\" title=\"StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery\"></a>StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery</h2><p>StyleGAN 可以用于生成高质量且真实的图像。然而，想要发掘潜层对于图像的影响对应怎样的语义信息，则需要大量的人工检查以及注释。在这项工作中，作者引入了 CLIP 模型，以帮助 StyleGAN 解除大量人力工作的限制。</p>\n<p>StyleGAN 的原理主要是通过 latent code 去控制图像的风格。对于一张图片，先通过 image inversion 将图片表示为latent code，然后去编辑它们，就可以达到编辑图像的目的。</p>\n<p>StyleGAN 的网络结构包含两个部分，第一个是 Mapping network，由隐藏变量 z 生成 中间隐藏变量 w的过程，这个 w 就是用来控制生成图像的风格； 第二个是 Synthesis network，它的作用是生成图像。</p>\n<hr>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220913083904923.png\" alt=\"image-20220913083904923\"></p>\n<hr>\n<p>作者总结了三种将 CLIP 和 StyleGAN 结合的方式：</p>\n<h3 id=\"Latent-optimization\"><a href=\"#Latent-optimization\" class=\"headerlink\" title=\"Latent optimization\"></a>Latent optimization</h3><p>这里利用 CLIP 模型计算损失，通过对 latent code 不断迭代，达到图像编辑的目的，需要很多次的迭代，耗时较长。</p>\n<p>要解算的是下面这个问题：</p>\n<script type=\"math/tex; mode=display\">\n\\mathop{arg\\:min}_{w∈W+}D_{CLIP}(G(w), t) + λ_{L2} ∥w − ws∥_2 + λ_{ID}L_{ID}(w),</script><hr>\n<h3 id=\"Latent-Mapper\"><a href=\"#Latent-Mapper\" class=\"headerlink\" title=\"Latent Mapper\"></a>Latent Mapper</h3><p>这个方法是指定一些 text prompt 进行训练，模型训练好后只需一次 forward 可以得到结果，通过指定 text prompt，可以预先控制图像编辑的范围区域，这就类似于利用属性分类器来辅助训练，只不过利用CLIP模型可以省略掉分类器，这种方法效率更高。</p>\n<p>StyleGAN2 的不同分辨率层控制了不同程度的图像语义，一般分为 coarse, medium, fine 三组。作者通过三个全连接层网络，对 latent code 进行重新编辑，残差连接后进入 StyleGAN 用来生成图像。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220912235028225.png\" alt=\"image-20220912235028225\"></p>\n<hr>\n<h3 id=\"Global-Directions\"><a href=\"#Global-Directions\" class=\"headerlink\" title=\"Global Directions\"></a>Global Directions</h3><p>这部分比较复杂，还没有搞明白。</p>\n<hr>\n<h2 id=\"实验：利用-CLIP-做简单的图片检索\"><a href=\"#实验：利用-CLIP-做简单的图片检索\" class=\"headerlink\" title=\"实验：利用 CLIP 做简单的图片检索\"></a>实验：利用 CLIP 做简单的图片检索</h2><p>基本流程就是先读入用户的输入，即搜索图片的关键词/句，然后将文本编码得到特征，然后分别对应所有图片的特征计算相似度，取相似度最高的三张图片输出。</p>\n<p>图片我没有找网上的一些数据集（因为大部分应该已经有人测试过了），我想看一看这个模型到底有多强大，于是我选取了三十几张我自己拍的一些生活照，进行了一下图像压缩，控制在500kb之内。</p>\n<p>展示的部分是利用 Gradio 搭建的，可以迅速将模型转化为交互式的界面。</p>\n<p>下面展示几张效果：</p>\n<hr>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220913084627975.png\" alt=\"image-20220913084627975\"></p>\n<hr>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220913084705720.png\" alt=\"image-20220913084705720\"></p>\n<hr>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220913084744050.png\" alt=\"image-20220913084744050\"></p>\n<hr>\n<p>测试的时候发现有意思的地方是这个例子：我这里是输入了这个midi键盘的名称（是刻在键盘上的文字），结果它也能给我识别出来：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220913084843362.png\" alt=\"image-20220913084843362\"></p>\n<hr>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904200950871.png\" alt=\"image-20220904200950871\"></p>\n<p>这是否说明 CLIP 模型直接具有 OCR 的能力呢？我的理解是 CLIP 在大量的样本训练中，将每个字母也看做一个对象去学习了他们的特征，所以能够直接识别到图片中的文字了。</p>\n<hr>\n<p>代码：(github仓库：<a href=\"https://github.com/1099255210/simple-CLIP-based-image-retrival\">https://github.com/1099255210/simple-CLIP-based-image-retrival</a>)</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> gradio <span class=\"keyword\">as</span> gr</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> clip</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">from</span> PIL <span class=\"keyword\">import</span> Image</span><br><span class=\"line\"><span class=\"keyword\">from</span> IPython.display <span class=\"keyword\">import</span> display</span><br><span class=\"line\"><span class=\"keyword\">from</span> tqdm.notebook <span class=\"keyword\">import</span> tqdm</span><br><span class=\"line\"></span><br><span class=\"line\">device = <span class=\"string\">&quot;cuda&quot;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&quot;cpu&quot;</span></span><br><span class=\"line\">model, preprocess = clip.load(<span class=\"string\">&quot;ViT-B/32&quot;</span>, device=device)</span><br><span class=\"line\"></span><br><span class=\"line\">data_location =  <span class=\"string\">&quot;./imgs&quot;</span></span><br><span class=\"line\">img_dict = &#123;&#125;</span><br><span class=\"line\"><span class=\"keyword\">for</span> inx, f <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(os.listdir(data_location)):</span><br><span class=\"line\">  img_dict[inx] = f</span><br><span class=\"line\">img_nums = <span class=\"built_in\">len</span>(img_dict)</span><br></pre></td></tr></table></figure>\n<hr>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">fn</span>(<span class=\"params\">instr</span>):</span><br><span class=\"line\">  text_input = clip.tokenize(instr).to(device)</span><br><span class=\"line\">  <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">    text_f = model.encode_text(text_input)</span><br><span class=\"line\">  text_f /= text_f.norm(dim=-<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">  sim = &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(img_nums):</span><br><span class=\"line\">    </span><br><span class=\"line\">    image_path = <span class=\"string\">f&#x27;<span class=\"subst\">&#123;data_location&#125;</span>/<span class=\"subst\">&#123;img_dict[i]&#125;</span>&#x27;</span></span><br><span class=\"line\">    img = Image.<span class=\"built_in\">open</span>(image_path)</span><br><span class=\"line\">    img_input = preprocess(img).unsqueeze(<span class=\"number\">0</span>).to(device)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">      img_f = model.encode_image(img_input)</span><br><span class=\"line\"></span><br><span class=\"line\">    img_f /= img_f.norm(dim=-<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    similarity = <span class=\"number\">100</span> * img_f @ text_f.T</span><br><span class=\"line\">    sim[i] = similarity</span><br><span class=\"line\"></span><br><span class=\"line\">  res = <span class=\"built_in\">sorted</span>(sim.items(), key=<span class=\"keyword\">lambda</span> s:s[<span class=\"number\">1</span>], reverse=<span class=\"literal\">True</span>)</span><br><span class=\"line\">  retval = [ <span class=\"string\">f&#x27;<span class=\"subst\">&#123;data_location&#125;</span>/<span class=\"subst\">&#123;img_dict[res[i][<span class=\"number\">0</span>]]&#125;</span>&#x27;</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">3</span>) ]</span><br><span class=\"line\">  <span class=\"keyword\">return</span> retval</span><br></pre></td></tr></table></figure>\n<hr>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">css_output = <span class=\"string\">&quot;.object-contain &#123;height: 100px !important&#125;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">demo = gr.Interface(</span><br><span class=\"line\">  fn = fn,</span><br><span class=\"line\">  inputs = <span class=\"string\">&#x27;text&#x27;</span>, </span><br><span class=\"line\">  outputs = [gr.Image(<span class=\"built_in\">type</span>=<span class=\"string\">&#x27;file&#x27;</span>, label=<span class=\"literal\">None</span>) <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">3</span>)],</span><br><span class=\"line\">  css = css_output,</span><br><span class=\"line\">)</span><br><span class=\"line\">demo.launch(enable_queue = <span class=\"literal\">True</span>,)</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"2022-09-06\"><a href=\"#2022-09-06\" class=\"headerlink\" title=\"2022/09/06\"></a>2022/09/06</h1><p>论文阅读：</p>\n<ul>\n<li>Learning Transferable Visual Models From Natural Language Supervision (CLIP)</li>\n<li>PointCLIP: Point Cloud Understanding by CLIP</li>\n<li>CLIPstyler: Image Style Transfer with a Single Text Condition</li>\n<li>StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery</li>\n<li>A Style-Based Generator Architecture for Generative Adversarial Networks (StyleGAN)</li>\n</ul>\n<p>实验：</p>\n<ul>\n<li>利用 CLIP 做简单的图片检索</li>\n</ul>\n<hr>\n<h2 id=\"Learning-Transferable-Visual-Models-From-Natural-Language-Supervision\"><a href=\"#Learning-Transferable-Visual-Models-From-Natural-Language-Supervision\" class=\"headerlink\" title=\"Learning Transferable Visual Models From Natural Language Supervision\"></a>Learning Transferable Visual Models From Natural Language Supervision</h2><p>CLIP 模型的本质就是将分类任务化成了图文匹配任务，效果可与全监督方法相当。</p>\n<p>CLIP 模型使用的方法：对比学习，预测 n*n 对图像与文本数据，将图片分类任务转换成图文匹配任务。这个过程实际上就是引入了 NLP 给出的监督信号。</p>\n<hr>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904193225832.png\" alt=\"image-20220904193225832\"></p>\n<p>图中左侧，得到文本特征与图片的特征后可以看到，对角线上的元素都是图文匹配的，共有 N 个正样本，其余的元素都是负样本，共有 N*N-N 个。其中，文本数据使用Transformer，图片数据用了两种模型，ResNet 和 Vision Transformer (ViT)。</p>\n<p>CLIP 模型训练所用的数据集较为庞大，包含从互联网上各种公开资源收集的4亿对图像、文本，CLIP是从头开始训练的，没有使用预训练的初始参数。</p>\n<hr>\n<h2 id=\"PointCLIP-Point-Cloud-Understanding-by-CLIP\"><a href=\"#PointCLIP-Point-Cloud-Understanding-by-CLIP\" class=\"headerlink\" title=\"PointCLIP: Point Cloud Understanding by CLIP\"></a>PointCLIP: Point Cloud Understanding by CLIP</h2><p>这是第一篇把 CLIP 用在点云上的工作，重点解决的是如何利用 CLIP 去理解点云数据。</p>\n<p>对于点云这种 3D 数据的形式，为了抽取点云的特征，采用了投影的方式，把三维的点朝 M 个方向投影，得到 M 张不同方向的 2D 点云深度图。这种图文章中提到是符合透视画法的特征：近大远小，即更加符合照片的特征。每个方向上得到的图片都是没有色彩信息的，作者这里出于运算时间与资源消耗的考虑，直接将图层复制成三个完全一致的通道传入编码器。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220911090624239.png\" alt=\"image-20220911090624239\"></p>\n<hr>\n<p>关于这种方法（PointCLIP）的效果，文章先是做了 zero-shot 分类的实验，在 ModelNet40 数据集上准确率是 20.18%，在 ScanObjectNN 数据 集上的准确率只有 15.38%，虽然这说明这样的方法是有一定效果的，但依然没法与现有的训练好的直接学习 3D 数据的网络相比较。</p>\n<p>然后作者提到 ModelNet40 需要六个方向的视图以不同的权重贡献作出最后的分类结果，在 ScanObjectNN 这样带有噪点（地板和天花板）的数据中，顶视图与底视图几乎提供不了任何有效信息（所以效果不好）。对于视图的数目，作者尝试了 1,4,6,8,10,12，在 zero-shot 任务里 6 个视角效果最佳（后面16-shot实验里10视角最佳）；关于不同视图的贡献，实验里右视图和左视图最多。</p>\n<p>当然，和 CLIP 有关的任务里，肯定也要涉及到 Prompt Design 的问题，作者这里用了以下的 Prompt:</p>\n<figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a photo of a [CLASS].</span><br><span class=\"line\">a point cloud photo of a [CLASS].</span><br><span class=\"line\">point cloud of a [CLASS].</span><br><span class=\"line\">point cloud of a big [CLASS].\t\t    # 16-shot best</span><br><span class=\"line\">point cloud depth map of a [CLASS]. # zero-shot best</span><br></pre></td></tr></table></figure>\n<p> 再就是图像编码器的选择，作者使用RN.*16的效果在zero-shot中表现最佳，使用RN101在16-shot中表现最佳。</p>\n<hr>\n<p>zero-shot 的方式相对于有监督的方法效果较差，于是作者又加了一个小网络 few-shot finetune 一下。将 M 个特征图拉成一维然后经过两个全连接得到一个全局特征，然后用这个全局特征在通过以下公式得到 M 个特征，与原先的特征残差连接得到新的 M 个特征。</p>\n<script type=\"math/tex; mode=display\">\nf^a_i = f_i + ReLU(f_{global}W^T_{3i})</script><p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220912100708331.png\" alt=\"image-20220912100708331\"></p>\n<p>这样一来，效果就有了大幅度的提升，在 ModelNet40上的结果从20.18%提升到了87.20%，据说是只用了1/10的数据就达到了这样的效果。</p>\n<hr>\n<p>最后，作者考虑是否可以通过模型融合的方式得到更好的效果，简单地说就是将不同模型预测的各类别分数相加得到最终分数。通过实验，用 PointCLIP 和其他有监督方法得到的模型融合，结果有所提升。PointCLIP 和 CurveNet 融合的结果达到了 sota。两个效果最好的有监督模型融合得到的效果与之相比反而达不到最高。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220912102819460.png\" alt=\"image-20220912102819460\"></p>\n<hr>\n<h2 id=\"CLIPstyler-Image-Style-Transfer-with-a-Single-Text-Condition\"><a href=\"#CLIPstyler-Image-Style-Transfer-with-a-Single-Text-Condition\" class=\"headerlink\" title=\"CLIPstyler: Image Style Transfer with a Single Text Condition\"></a>CLIPstyler: Image Style Transfer with a Single Text Condition</h2><p>现有的神经风格迁移需要提供参考图像才能将纹理信息转移给目标图像，但是在很多情况下用户手头是没有参考图像的。这篇文章提出了 CLIPstyler，旨在使用简单的对风格的文字描述来进行风格迁移。</p>\n<p>问题的两个难点在于：1）如何释放来自 CLIP 模型的语义化的“材质”信息，将其应用到目标图片上；2）如何规范化训练，使得输出的图像不会受质量上的影响。</p>\n<p>作者这里设计了一个 StyleNet 用来捕捉内容图像的分层视觉特征，同时在深度特征空间中对图像进行风格化处理，以获得真实的纹理表示。</p>\n<p>关于损失函数，作者这里阐述了几个他们用到的概念：</p>\n<hr>\n<p>1) CLIP loss </p>\n<p>最简单的基于CLIP的最小化全局损失函数如下：</p>\n<script type=\"math/tex; mode=display\">\nL_{global}=D_{CLIP}(f(I_c),t_{sty})</script><p>$D_{CLIP}$ 指在 CLIP 空间的余弦距离。但是如果用这样的损失函数，图像的质量会受损，而且图像优化过程的稳定性无法保证，于是他们参考了 StyleGAN-NADA 提出的一种定向的 CLIP loss 函数，将图像-文本对的输入和输出在CLIP空间对齐，从而解决了上面的问题，这个损失函数如下：</p>\n<script type=\"math/tex; mode=display\">\n\\Delta T = E_T (t_{sty}) − E_T (t_{src}), \\\\\n\\Delta I = E_I (f (I_c)) − E_I (I_c), \\\\\nL_{dir} = 1 − \\frac{\\Delta I · \\Delta T }{\\lvert \\Delta I \\rvert \\lvert \\Delta T\\rvert }</script><p>其中 $E_T, E_I$ 指的就是 CLIP 的文本和图像编码器，$t_{sty},t_{src}$ 则是风格描述文本和原图像文本（默认设置为：Photo）</p>\n<hr>\n<ol>\n<li>PatchCLIP loss </li>\n</ol>\n<p>上文的 $L_{dir}$ 在调整预训练的生成式模型效果较好，但是和作者的目标不完全一样，单纯使用这个损失函数也会造成输出质量的降低。为了解决这个问题，作者提出了一种新的专为纹理迁移设计的损失函数 PatchCLIP loss。他们将通过 StyleNet 得到的图像随机地分成一些块（块的尺寸固定），然后对不同的块做不同的数据增强，这里的增强统一使用了透视变换</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220912203957486.png\" alt=\"image-20220912203957486\"></p>\n<p>透视变换的好处是，所有的分块在不同的点观察时都能够具有相同（或类似）的语义，CLIP 模型的语义信息也就可以被重建为更类似于 3D 的结构。</p>\n<hr>\n<ol>\n<li>Threshold rejection</li>\n</ol>\n<p>由于分块的随机性，以上的方法很可能导致在某些已经具有较为符合目标风格的分块上进行过度的风格化（因为这样的分块更加容易被优化），针对这个情况，作者加入正则化，设定了一个阈值，当分块得分低于这一阈值时忽略这一分块的梯度优化过程。这样就得到了整个$L_{patch}$的定义：</p>\n<script type=\"math/tex; mode=display\">\n\\Delta T = E_T (t_{sty}) − E_T (t_{src}), \\\\\n\\Delta I = E_I (aug(\\hat{I}^i_{cs})) − E_I (I_c), \\\\\nL_{dir} = 1 − \\frac{\\Delta I · \\Delta T }{\\lvert \\Delta I \\rvert \\lvert \\Delta T\\rvert } \\\\\nL_{patch}=\\frac{1}{N}\\sum^N_iR(l^i_{patch},\\tau) \\\\\n\n\n\\begin{aligned}\n    \\text{where} \\quad R(s,\\tau)=\n    \\begin{cases}\n        0,&if\\quad s\\le\\tau \\\\\n        s,&\\text{otherwise}\n    \\end{cases}\n\\end{aligned}</script><hr>\n<ol>\n<li>Total loss</li>\n</ol>\n<p>最终总体的损失函数还加入了 content loss $L_c$ 和 total varation regularization loss $L_{tv}$：</p>\n<script type=\"math/tex; mode=display\">\nL_{total} = λ_dL_{dir} + λ_pL_{patch} + λ_cL_c + λ_{tv}L_{tv}</script><hr>\n<p>CLIPstyler 有可供尝试的 demo，网址：<a href=\"https://replicate.com/paper11667/clipstyler\">https://replicate.com/paper11667/clipstyler</a></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/clipstyler.gif\" alt=\"clipstyler\"></p>\n<p>左图原图，文本 “Chinese style” ，训练迭代次数 100 次，右图为输出图像</p>\n<hr>\n<h2 id=\"StyleCLIP-Text-Driven-Manipulation-of-StyleGAN-Imagery\"><a href=\"#StyleCLIP-Text-Driven-Manipulation-of-StyleGAN-Imagery\" class=\"headerlink\" title=\"StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery\"></a>StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery</h2><p>StyleGAN 可以用于生成高质量且真实的图像。然而，想要发掘潜层对于图像的影响对应怎样的语义信息，则需要大量的人工检查以及注释。在这项工作中，作者引入了 CLIP 模型，以帮助 StyleGAN 解除大量人力工作的限制。</p>\n<p>StyleGAN 的原理主要是通过 latent code 去控制图像的风格。对于一张图片，先通过 image inversion 将图片表示为latent code，然后去编辑它们，就可以达到编辑图像的目的。</p>\n<p>StyleGAN 的网络结构包含两个部分，第一个是 Mapping network，由隐藏变量 z 生成 中间隐藏变量 w的过程，这个 w 就是用来控制生成图像的风格； 第二个是 Synthesis network，它的作用是生成图像。</p>\n<hr>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220913083904923.png\" alt=\"image-20220913083904923\"></p>\n<hr>\n<p>作者总结了三种将 CLIP 和 StyleGAN 结合的方式：</p>\n<h3 id=\"Latent-optimization\"><a href=\"#Latent-optimization\" class=\"headerlink\" title=\"Latent optimization\"></a>Latent optimization</h3><p>这里利用 CLIP 模型计算损失，通过对 latent code 不断迭代，达到图像编辑的目的，需要很多次的迭代，耗时较长。</p>\n<p>要解算的是下面这个问题：</p>\n<script type=\"math/tex; mode=display\">\n\\mathop{arg\\:min}_{w∈W+}D_{CLIP}(G(w), t) + λ_{L2} ∥w − ws∥_2 + λ_{ID}L_{ID}(w),</script><hr>\n<h3 id=\"Latent-Mapper\"><a href=\"#Latent-Mapper\" class=\"headerlink\" title=\"Latent Mapper\"></a>Latent Mapper</h3><p>这个方法是指定一些 text prompt 进行训练，模型训练好后只需一次 forward 可以得到结果，通过指定 text prompt，可以预先控制图像编辑的范围区域，这就类似于利用属性分类器来辅助训练，只不过利用CLIP模型可以省略掉分类器，这种方法效率更高。</p>\n<p>StyleGAN2 的不同分辨率层控制了不同程度的图像语义，一般分为 coarse, medium, fine 三组。作者通过三个全连接层网络，对 latent code 进行重新编辑，残差连接后进入 StyleGAN 用来生成图像。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220912235028225.png\" alt=\"image-20220912235028225\"></p>\n<hr>\n<h3 id=\"Global-Directions\"><a href=\"#Global-Directions\" class=\"headerlink\" title=\"Global Directions\"></a>Global Directions</h3><p>这部分比较复杂，还没有搞明白。</p>\n<hr>\n<h2 id=\"实验：利用-CLIP-做简单的图片检索\"><a href=\"#实验：利用-CLIP-做简单的图片检索\" class=\"headerlink\" title=\"实验：利用 CLIP 做简单的图片检索\"></a>实验：利用 CLIP 做简单的图片检索</h2><p>基本流程就是先读入用户的输入，即搜索图片的关键词/句，然后将文本编码得到特征，然后分别对应所有图片的特征计算相似度，取相似度最高的三张图片输出。</p>\n<p>图片我没有找网上的一些数据集（因为大部分应该已经有人测试过了），我想看一看这个模型到底有多强大，于是我选取了三十几张我自己拍的一些生活照，进行了一下图像压缩，控制在500kb之内。</p>\n<p>展示的部分是利用 Gradio 搭建的，可以迅速将模型转化为交互式的界面。</p>\n<p>下面展示几张效果：</p>\n<hr>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220913084627975.png\" alt=\"image-20220913084627975\"></p>\n<hr>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220913084705720.png\" alt=\"image-20220913084705720\"></p>\n<hr>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220913084744050.png\" alt=\"image-20220913084744050\"></p>\n<hr>\n<p>测试的时候发现有意思的地方是这个例子：我这里是输入了这个midi键盘的名称（是刻在键盘上的文字），结果它也能给我识别出来：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220913084843362.png\" alt=\"image-20220913084843362\"></p>\n<hr>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220904200950871.png\" alt=\"image-20220904200950871\"></p>\n<p>这是否说明 CLIP 模型直接具有 OCR 的能力呢？我的理解是 CLIP 在大量的样本训练中，将每个字母也看做一个对象去学习了他们的特征，所以能够直接识别到图片中的文字了。</p>\n<hr>\n<p>代码：(github仓库：<a href=\"https://github.com/1099255210/simple-CLIP-based-image-retrival\">https://github.com/1099255210/simple-CLIP-based-image-retrival</a>)</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> gradio <span class=\"keyword\">as</span> gr</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> clip</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">from</span> PIL <span class=\"keyword\">import</span> Image</span><br><span class=\"line\"><span class=\"keyword\">from</span> IPython.display <span class=\"keyword\">import</span> display</span><br><span class=\"line\"><span class=\"keyword\">from</span> tqdm.notebook <span class=\"keyword\">import</span> tqdm</span><br><span class=\"line\"></span><br><span class=\"line\">device = <span class=\"string\">&quot;cuda&quot;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&quot;cpu&quot;</span></span><br><span class=\"line\">model, preprocess = clip.load(<span class=\"string\">&quot;ViT-B/32&quot;</span>, device=device)</span><br><span class=\"line\"></span><br><span class=\"line\">data_location =  <span class=\"string\">&quot;./imgs&quot;</span></span><br><span class=\"line\">img_dict = &#123;&#125;</span><br><span class=\"line\"><span class=\"keyword\">for</span> inx, f <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(os.listdir(data_location)):</span><br><span class=\"line\">  img_dict[inx] = f</span><br><span class=\"line\">img_nums = <span class=\"built_in\">len</span>(img_dict)</span><br></pre></td></tr></table></figure>\n<hr>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">fn</span>(<span class=\"params\">instr</span>):</span><br><span class=\"line\">  text_input = clip.tokenize(instr).to(device)</span><br><span class=\"line\">  <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">    text_f = model.encode_text(text_input)</span><br><span class=\"line\">  text_f /= text_f.norm(dim=-<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">  sim = &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(img_nums):</span><br><span class=\"line\">    </span><br><span class=\"line\">    image_path = <span class=\"string\">f&#x27;<span class=\"subst\">&#123;data_location&#125;</span>/<span class=\"subst\">&#123;img_dict[i]&#125;</span>&#x27;</span></span><br><span class=\"line\">    img = Image.<span class=\"built_in\">open</span>(image_path)</span><br><span class=\"line\">    img_input = preprocess(img).unsqueeze(<span class=\"number\">0</span>).to(device)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">      img_f = model.encode_image(img_input)</span><br><span class=\"line\"></span><br><span class=\"line\">    img_f /= img_f.norm(dim=-<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    similarity = <span class=\"number\">100</span> * img_f @ text_f.T</span><br><span class=\"line\">    sim[i] = similarity</span><br><span class=\"line\"></span><br><span class=\"line\">  res = <span class=\"built_in\">sorted</span>(sim.items(), key=<span class=\"keyword\">lambda</span> s:s[<span class=\"number\">1</span>], reverse=<span class=\"literal\">True</span>)</span><br><span class=\"line\">  retval = [ <span class=\"string\">f&#x27;<span class=\"subst\">&#123;data_location&#125;</span>/<span class=\"subst\">&#123;img_dict[res[i][<span class=\"number\">0</span>]]&#125;</span>&#x27;</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">3</span>) ]</span><br><span class=\"line\">  <span class=\"keyword\">return</span> retval</span><br></pre></td></tr></table></figure>\n<hr>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">css_output = <span class=\"string\">&quot;.object-contain &#123;height: 100px !important&#125;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">demo = gr.Interface(</span><br><span class=\"line\">  fn = fn,</span><br><span class=\"line\">  inputs = <span class=\"string\">&#x27;text&#x27;</span>, </span><br><span class=\"line\">  outputs = [gr.Image(<span class=\"built_in\">type</span>=<span class=\"string\">&#x27;file&#x27;</span>, label=<span class=\"literal\">None</span>) <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">3</span>)],</span><br><span class=\"line\">  css = css_output,</span><br><span class=\"line\">)</span><br><span class=\"line\">demo.launch(enable_queue = <span class=\"literal\">True</span>,)</span><br></pre></td></tr></table></figure>\n"},{"title":"搭建 CS:GO 服务器 (On Windows)","date":"2021-01-01T13:17:08.000Z","_content":"\n## 步骤\n\n[SteamCMD](https://steamcdn-a.akamaihd.net/client/installer/steamcmd.zip)\n\n下载`SteamCMD`，**打开**`steamcmd.exe`，启动命令行。\n\n第一次启动需要等待更新。\n\n等到可以输入时，**输入**：\n\n```\nlogin anonymous\napp_update 740 validate\n```\n\n（复制代码后，在命令窗口右键可以直接粘贴）\n\n![命令行_1](https://i.imgur.com/J6pjONd.png)\n\n等待服务器下载（约29G）\n\n---\n\n等待过程中先去做一些准备工作\n\n\n**下载**两个mod：\n\n[Metamod](https://mms.alliedmods.net/mmsdrop/1.11/mmsource-1.11.0-git1145-windows.zip)\n\n[SourceMod](https://sm.alliedmods.net/smdrop/1.10/sourcemod-1.10.0-git6528-windows.zip)\n\n分别**解压**待用\n\n---\n\n顺便**检查**一下电脑里是否有命令行工具（建议使用 Windows Terminal）\n\n[Windows Terminal](https://www.microsoft.com/en-us/p/windows-terminal/9n0dx20hk701?activetab=pivot:overviewtab)\n\n安装好命令行工具后，在`Windows徽标`右键，可以看到`Windows终端(管理员)`\n\n**输入如下代码**（用来打开电脑的端口）：\n\n```shell=\nnetsh advfirewall firewall add rule name=\"CSGO 27015 TCP\" dir=in action=allow protocol=TCP localport=27015\nnetsh advfirewall firewall add rule name=\"CSGO 27015 UDP\" dir=in action=allow protocol=UDP localport=27015\nnetsh advfirewall firewall add rule name=\"CSGO 27020 UDP\" dir=in action=allow protocol=UDP localport=27020\nnetsh advfirewall firewall add rule name=\"CSGO 27015 TCP-o\" dir=out action=allow protocol=TCP localport=27015\nnetsh advfirewall firewall add rule name=\"CSGO 27015 UDP-o\" dir=out action=allow protocol=UDP localport=27015\nnetsh advfirewall firewall add rule name=\"CSGO 27020 UDP-o\" dir=out action=allow protocol=UDP localport=27020\n```\n\n---\n\n尝试启动游戏\n\n在 `SteamCMD` 的目录下，右键**打开命令行工具**：\n\n```shell=\ncd '.\\steamapps\\common\\Counter-Strike Global Offensive Beta - Dedicated Server\\'\n\n.\\srcds.exe -game csgo -console +map de_dust2\n```\n\n![命令行_2](https://i.imgur.com/xsnDsVz.png)\n\n初步启动完成，可以暂时关闭窗口\n\n---\n\n在 `csgo/cfg` 文件夹里**创建** `server.cfg`，文件内容复制粘贴如下链接里的内容即可：\n\n[server.cfg](https://hackmd.io/@1099255210/rk7UMX5Vq)\n\n---\n\n接下来**安装**`Metamod`和`Sourcemod`\n\n将 `mmsource-1.11.0-git1145-windows` 里的文件夹 `addons` 拖到`csgo` 文件夹中\n\n![](https://i.imgur.com/5JdYipL.png)\n\n将 `sourcemod-1.10.0-git6528-windows` 里的文件夹 `addons` 和 `cfg` 拖到 `csgo` 文件夹中\n\n![](https://i.imgur.com/gXHPlEr.png)\n\n安装完成\n\n---\n\n接下来**配置服务器管理员**\n\n打开 `\\csgo\\addons\\sourcemod\\configs\\admins_simple.ini`\n\n[获取SteamID](https://steamid.io/)\n\n搜索到自己，第一个 steamID 就是我们需要的\n\n在文件末尾**加一行**：\n\n```\n\"STEAM_0:0:......\" \"99:z\"\n```\n\n前面替换成你的 steamID\n\n---\n\n启动测试，还是来到 `\\Counter-Strike Global Offensive Beta - Dedicated Server\\` 文件夹，**打开命令行**：\n\n```\n.\\srcds -game csgo -console -usercon +game_type 1 +game_mode 2 +mapgroup mg_allclassic +map de_dust\n```\n\n这时候服务器应该已经正常启动了\n\n我们**重新打开一个命令行**，输入 `ipconfig`，在结果里找到：\n\n```\n# 这是我自己电脑的信息\n无线局域网适配器 WLAN:\n\n   连接特定的 DNS 后缀 . . . . . . . :\n   本地链接 IPv6 地址. . . . . . . . : fe80::7d80:a5ec:6d19:69a7%18\n   IPv4 地址 . . . . . . . . . . . . : 192.168.31.196\n   子网掩码  . . . . . . . . . . . . : 255.255.255.0\n   默认网关. . . . . . . . . . . . . : 192.168.31.1\n```\n\n记住其中的 `IPv4地址`\n\n游玩游戏的电脑需要和服务器连在同一个wifi内\n\n在游戏端**输入** `connect [IPv4地址:27015]` 应该能正常进入服务器\n\n在聊天框内**输入** `!admin` 如果左边弹出选项，说明管理员认证成功\n\n---\n\n## 安装插件\n\n**安装**mysql\n\n[mysql](https://dev.mysql.com/get/Downloads/MySQLInstaller/mysql-installer-community-8.0.28.0.msi)\n\n![](https://i.imgur.com/3jjhOrh.png)\n\n这个页面选择下面一个\n\n其他按照默认选项安装即可，有错误直接跳过\n\n密码设置自己记得住的，后面要使用\n\n**添加**系统环境变量\n\n设置 > 系统 > 设备规格 > 高级系统设置 > 环境变量 > 用户变量 > Path > 编辑 > 新建\n\n粘贴 `C:\\Program Files\\MySQL\\MySQL Server 8.0\\bin`\n\n确定即可\n\n重新打开终端，**输入** `mysql -uroot -p` 以及密码，测试是否设置成功。\n\n若可以正常使用 `mysql` 命令行，**执行**如下命令：\n\n```mysql\nCREATE DATABASE `sticker`;\n```\n\n---\n\n**打开** `\\csgo\\addons\\sourcemod\\configs\\databases.cfg`\n\n在 `Databases` 下一级**添加**：\n\n```\n\"csgo_weaponstickers\"\n{\n    \"driver\"    \"mysql\"\n    \"host\"      \"localhost\"\n    \"database\"  \"sticker\"\n    \"user\"      \"root\"\n    \"pass\"      \"\"\n}\n```\n\n![](https://i.imgur.com/hC4FIhC.png)\n\n\n\n\n## 附加内容\n\n### 开放到公网\n\n首先到 [Steam服务器管理页面](https://steamcommunity.com/dev/managegameservers) **申请一个令牌**：\n\n![令牌_1](https://i.imgur.com/uPAi8Om.png)\n\nApp ID 必须填写730，备忘录无所谓\n\n![令牌_2](https://i.imgur.com/1XOFGKW.png)\n\n申请得到的登录令牌复制**保存**好，不要泄露\n\n在 `server.cfg` 中**加入一行**：\n\n```\nsv_setsteamaccount \"你的令牌\"\n```\n\n**修改**文件中的 `sv_lan 1` 改为 `sv_lan 0`\n\n重启服务器即可\n\n---\n\n### 启用远程控制台\n\n在 `server.cfg` 中找到 `rcon_password \"\"` 添加你的密码\n\n然后在游戏中连接到服务器，在控制台输入 `rcon_password \"你的密码\"`\n\n之后你就可以在你需要使用的指令前加 `rcon ` 来执行\n\n---","source":"_posts/搭建 CSGO 服务器(on windows).md","raw":"---\ntitle: 搭建 CS:GO 服务器 (On Windows)\ndate: 2021-01-01 21:17:08\ntags:\n---\n\n## 步骤\n\n[SteamCMD](https://steamcdn-a.akamaihd.net/client/installer/steamcmd.zip)\n\n下载`SteamCMD`，**打开**`steamcmd.exe`，启动命令行。\n\n第一次启动需要等待更新。\n\n等到可以输入时，**输入**：\n\n```\nlogin anonymous\napp_update 740 validate\n```\n\n（复制代码后，在命令窗口右键可以直接粘贴）\n\n![命令行_1](https://i.imgur.com/J6pjONd.png)\n\n等待服务器下载（约29G）\n\n---\n\n等待过程中先去做一些准备工作\n\n\n**下载**两个mod：\n\n[Metamod](https://mms.alliedmods.net/mmsdrop/1.11/mmsource-1.11.0-git1145-windows.zip)\n\n[SourceMod](https://sm.alliedmods.net/smdrop/1.10/sourcemod-1.10.0-git6528-windows.zip)\n\n分别**解压**待用\n\n---\n\n顺便**检查**一下电脑里是否有命令行工具（建议使用 Windows Terminal）\n\n[Windows Terminal](https://www.microsoft.com/en-us/p/windows-terminal/9n0dx20hk701?activetab=pivot:overviewtab)\n\n安装好命令行工具后，在`Windows徽标`右键，可以看到`Windows终端(管理员)`\n\n**输入如下代码**（用来打开电脑的端口）：\n\n```shell=\nnetsh advfirewall firewall add rule name=\"CSGO 27015 TCP\" dir=in action=allow protocol=TCP localport=27015\nnetsh advfirewall firewall add rule name=\"CSGO 27015 UDP\" dir=in action=allow protocol=UDP localport=27015\nnetsh advfirewall firewall add rule name=\"CSGO 27020 UDP\" dir=in action=allow protocol=UDP localport=27020\nnetsh advfirewall firewall add rule name=\"CSGO 27015 TCP-o\" dir=out action=allow protocol=TCP localport=27015\nnetsh advfirewall firewall add rule name=\"CSGO 27015 UDP-o\" dir=out action=allow protocol=UDP localport=27015\nnetsh advfirewall firewall add rule name=\"CSGO 27020 UDP-o\" dir=out action=allow protocol=UDP localport=27020\n```\n\n---\n\n尝试启动游戏\n\n在 `SteamCMD` 的目录下，右键**打开命令行工具**：\n\n```shell=\ncd '.\\steamapps\\common\\Counter-Strike Global Offensive Beta - Dedicated Server\\'\n\n.\\srcds.exe -game csgo -console +map de_dust2\n```\n\n![命令行_2](https://i.imgur.com/xsnDsVz.png)\n\n初步启动完成，可以暂时关闭窗口\n\n---\n\n在 `csgo/cfg` 文件夹里**创建** `server.cfg`，文件内容复制粘贴如下链接里的内容即可：\n\n[server.cfg](https://hackmd.io/@1099255210/rk7UMX5Vq)\n\n---\n\n接下来**安装**`Metamod`和`Sourcemod`\n\n将 `mmsource-1.11.0-git1145-windows` 里的文件夹 `addons` 拖到`csgo` 文件夹中\n\n![](https://i.imgur.com/5JdYipL.png)\n\n将 `sourcemod-1.10.0-git6528-windows` 里的文件夹 `addons` 和 `cfg` 拖到 `csgo` 文件夹中\n\n![](https://i.imgur.com/gXHPlEr.png)\n\n安装完成\n\n---\n\n接下来**配置服务器管理员**\n\n打开 `\\csgo\\addons\\sourcemod\\configs\\admins_simple.ini`\n\n[获取SteamID](https://steamid.io/)\n\n搜索到自己，第一个 steamID 就是我们需要的\n\n在文件末尾**加一行**：\n\n```\n\"STEAM_0:0:......\" \"99:z\"\n```\n\n前面替换成你的 steamID\n\n---\n\n启动测试，还是来到 `\\Counter-Strike Global Offensive Beta - Dedicated Server\\` 文件夹，**打开命令行**：\n\n```\n.\\srcds -game csgo -console -usercon +game_type 1 +game_mode 2 +mapgroup mg_allclassic +map de_dust\n```\n\n这时候服务器应该已经正常启动了\n\n我们**重新打开一个命令行**，输入 `ipconfig`，在结果里找到：\n\n```\n# 这是我自己电脑的信息\n无线局域网适配器 WLAN:\n\n   连接特定的 DNS 后缀 . . . . . . . :\n   本地链接 IPv6 地址. . . . . . . . : fe80::7d80:a5ec:6d19:69a7%18\n   IPv4 地址 . . . . . . . . . . . . : 192.168.31.196\n   子网掩码  . . . . . . . . . . . . : 255.255.255.0\n   默认网关. . . . . . . . . . . . . : 192.168.31.1\n```\n\n记住其中的 `IPv4地址`\n\n游玩游戏的电脑需要和服务器连在同一个wifi内\n\n在游戏端**输入** `connect [IPv4地址:27015]` 应该能正常进入服务器\n\n在聊天框内**输入** `!admin` 如果左边弹出选项，说明管理员认证成功\n\n---\n\n## 安装插件\n\n**安装**mysql\n\n[mysql](https://dev.mysql.com/get/Downloads/MySQLInstaller/mysql-installer-community-8.0.28.0.msi)\n\n![](https://i.imgur.com/3jjhOrh.png)\n\n这个页面选择下面一个\n\n其他按照默认选项安装即可，有错误直接跳过\n\n密码设置自己记得住的，后面要使用\n\n**添加**系统环境变量\n\n设置 > 系统 > 设备规格 > 高级系统设置 > 环境变量 > 用户变量 > Path > 编辑 > 新建\n\n粘贴 `C:\\Program Files\\MySQL\\MySQL Server 8.0\\bin`\n\n确定即可\n\n重新打开终端，**输入** `mysql -uroot -p` 以及密码，测试是否设置成功。\n\n若可以正常使用 `mysql` 命令行，**执行**如下命令：\n\n```mysql\nCREATE DATABASE `sticker`;\n```\n\n---\n\n**打开** `\\csgo\\addons\\sourcemod\\configs\\databases.cfg`\n\n在 `Databases` 下一级**添加**：\n\n```\n\"csgo_weaponstickers\"\n{\n    \"driver\"    \"mysql\"\n    \"host\"      \"localhost\"\n    \"database\"  \"sticker\"\n    \"user\"      \"root\"\n    \"pass\"      \"\"\n}\n```\n\n![](https://i.imgur.com/hC4FIhC.png)\n\n\n\n\n## 附加内容\n\n### 开放到公网\n\n首先到 [Steam服务器管理页面](https://steamcommunity.com/dev/managegameservers) **申请一个令牌**：\n\n![令牌_1](https://i.imgur.com/uPAi8Om.png)\n\nApp ID 必须填写730，备忘录无所谓\n\n![令牌_2](https://i.imgur.com/1XOFGKW.png)\n\n申请得到的登录令牌复制**保存**好，不要泄露\n\n在 `server.cfg` 中**加入一行**：\n\n```\nsv_setsteamaccount \"你的令牌\"\n```\n\n**修改**文件中的 `sv_lan 1` 改为 `sv_lan 0`\n\n重启服务器即可\n\n---\n\n### 启用远程控制台\n\n在 `server.cfg` 中找到 `rcon_password \"\"` 添加你的密码\n\n然后在游戏中连接到服务器，在控制台输入 `rcon_password \"你的密码\"`\n\n之后你就可以在你需要使用的指令前加 `rcon ` 来执行\n\n---","slug":"搭建 CSGO 服务器(on windows)","published":1,"updated":"2022-08-25T13:25:15.077Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clgyiaru1000jjku70up114ca","content":"<h2 id=\"步骤\"><a href=\"#步骤\" class=\"headerlink\" title=\"步骤\"></a>步骤</h2><p><a href=\"https://steamcdn-a.akamaihd.net/client/installer/steamcmd.zip\">SteamCMD</a></p>\n<p>下载<code>SteamCMD</code>，<strong>打开</strong><code>steamcmd.exe</code>，启动命令行。</p>\n<p>第一次启动需要等待更新。</p>\n<p>等到可以输入时，<strong>输入</strong>：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">login anonymous</span><br><span class=\"line\">app_update 740 validate</span><br></pre></td></tr></table></figure>\n<p>（复制代码后，在命令窗口右键可以直接粘贴）</p>\n<p><img src=\"https://i.imgur.com/J6pjONd.png\" alt=\"命令行_1\"></p>\n<p>等待服务器下载（约29G）</p>\n<hr>\n<p>等待过程中先去做一些准备工作</p>\n<p><strong>下载</strong>两个mod：</p>\n<p><a href=\"https://mms.alliedmods.net/mmsdrop/1.11/mmsource-1.11.0-git1145-windows.zip\">Metamod</a></p>\n<p><a href=\"https://sm.alliedmods.net/smdrop/1.10/sourcemod-1.10.0-git6528-windows.zip\">SourceMod</a></p>\n<p>分别<strong>解压</strong>待用</p>\n<hr>\n<p>顺便<strong>检查</strong>一下电脑里是否有命令行工具（建议使用 Windows Terminal）</p>\n<p><a href=\"https://www.microsoft.com/en-us/p/windows-terminal/9n0dx20hk701?activetab=pivot:overviewtab\">Windows Terminal</a></p>\n<p>安装好命令行工具后，在<code>Windows徽标</code>右键，可以看到<code>Windows终端(管理员)</code></p>\n<p><strong>输入如下代码</strong>（用来打开电脑的端口）：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">netsh advfirewall firewall add rule name=&quot;CSGO 27015 TCP&quot; dir=in action=allow protocol=TCP localport=27015</span><br><span class=\"line\">netsh advfirewall firewall add rule name=&quot;CSGO 27015 UDP&quot; dir=in action=allow protocol=UDP localport=27015</span><br><span class=\"line\">netsh advfirewall firewall add rule name=&quot;CSGO 27020 UDP&quot; dir=in action=allow protocol=UDP localport=27020</span><br><span class=\"line\">netsh advfirewall firewall add rule name=&quot;CSGO 27015 TCP-o&quot; dir=out action=allow protocol=TCP localport=27015</span><br><span class=\"line\">netsh advfirewall firewall add rule name=&quot;CSGO 27015 UDP-o&quot; dir=out action=allow protocol=UDP localport=27015</span><br><span class=\"line\">netsh advfirewall firewall add rule name=&quot;CSGO 27020 UDP-o&quot; dir=out action=allow protocol=UDP localport=27020</span><br></pre></td></tr></table></figure>\n<hr>\n<p>尝试启动游戏</p>\n<p>在 <code>SteamCMD</code> 的目录下，右键<strong>打开命令行工具</strong>：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd &#x27;.\\steamapps\\common\\Counter-Strike Global Offensive Beta - Dedicated Server\\&#x27;</span><br><span class=\"line\"></span><br><span class=\"line\">.\\srcds.exe -game csgo -console +map de_dust2</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://i.imgur.com/xsnDsVz.png\" alt=\"命令行_2\"></p>\n<p>初步启动完成，可以暂时关闭窗口</p>\n<hr>\n<p>在 <code>csgo/cfg</code> 文件夹里<strong>创建</strong> <code>server.cfg</code>，文件内容复制粘贴如下链接里的内容即可：</p>\n<p><a href=\"https://hackmd.io/@1099255210/rk7UMX5Vq\">server.cfg</a></p>\n<hr>\n<p>接下来<strong>安装</strong><code>Metamod</code>和<code>Sourcemod</code></p>\n<p>将 <code>mmsource-1.11.0-git1145-windows</code> 里的文件夹 <code>addons</code> 拖到<code>csgo</code> 文件夹中</p>\n<p><img src=\"https://i.imgur.com/5JdYipL.png\" alt=\"\"></p>\n<p>将 <code>sourcemod-1.10.0-git6528-windows</code> 里的文件夹 <code>addons</code> 和 <code>cfg</code> 拖到 <code>csgo</code> 文件夹中</p>\n<p><img src=\"https://i.imgur.com/gXHPlEr.png\" alt=\"\"></p>\n<p>安装完成</p>\n<hr>\n<p>接下来<strong>配置服务器管理员</strong></p>\n<p>打开 <code>\\csgo\\addons\\sourcemod\\configs\\admins_simple.ini</code></p>\n<p><a href=\"https://steamid.io/\">获取SteamID</a></p>\n<p>搜索到自己，第一个 steamID 就是我们需要的</p>\n<p>在文件末尾<strong>加一行</strong>：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&quot;STEAM_0:0:......&quot; &quot;99:z&quot;</span><br></pre></td></tr></table></figure>\n<p>前面替换成你的 steamID</p>\n<hr>\n<p>启动测试，还是来到 <code>\\Counter-Strike Global Offensive Beta - Dedicated Server\\</code> 文件夹，<strong>打开命令行</strong>：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">.\\srcds -game csgo -console -usercon +game_type 1 +game_mode 2 +mapgroup mg_allclassic +map de_dust</span><br></pre></td></tr></table></figure>\n<p>这时候服务器应该已经正常启动了</p>\n<p>我们<strong>重新打开一个命令行</strong>，输入 <code>ipconfig</code>，在结果里找到：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 这是我自己电脑的信息</span><br><span class=\"line\">无线局域网适配器 WLAN:</span><br><span class=\"line\"></span><br><span class=\"line\">   连接特定的 DNS 后缀 . . . . . . . :</span><br><span class=\"line\">   本地链接 IPv6 地址. . . . . . . . : fe80::7d80:a5ec:6d19:69a7%18</span><br><span class=\"line\">   IPv4 地址 . . . . . . . . . . . . : 192.168.31.196</span><br><span class=\"line\">   子网掩码  . . . . . . . . . . . . : 255.255.255.0</span><br><span class=\"line\">   默认网关. . . . . . . . . . . . . : 192.168.31.1</span><br></pre></td></tr></table></figure>\n<p>记住其中的 <code>IPv4地址</code></p>\n<p>游玩游戏的电脑需要和服务器连在同一个wifi内</p>\n<p>在游戏端<strong>输入</strong> <code>connect [IPv4地址:27015]</code> 应该能正常进入服务器</p>\n<p>在聊天框内<strong>输入</strong> <code>!admin</code> 如果左边弹出选项，说明管理员认证成功</p>\n<hr>\n<h2 id=\"安装插件\"><a href=\"#安装插件\" class=\"headerlink\" title=\"安装插件\"></a>安装插件</h2><p><strong>安装</strong>mysql</p>\n<p><a href=\"https://dev.mysql.com/get/Downloads/MySQLInstaller/mysql-installer-community-8.0.28.0.msi\">mysql</a></p>\n<p><img src=\"https://i.imgur.com/3jjhOrh.png\" alt=\"\"></p>\n<p>这个页面选择下面一个</p>\n<p>其他按照默认选项安装即可，有错误直接跳过</p>\n<p>密码设置自己记得住的，后面要使用</p>\n<p><strong>添加</strong>系统环境变量</p>\n<p>设置 &gt; 系统 &gt; 设备规格 &gt; 高级系统设置 &gt; 环境变量 &gt; 用户变量 &gt; Path &gt; 编辑 &gt; 新建</p>\n<p>粘贴 <code>C:\\Program Files\\MySQL\\MySQL Server 8.0\\bin</code></p>\n<p>确定即可</p>\n<p>重新打开终端，<strong>输入</strong> <code>mysql -uroot -p</code> 以及密码，测试是否设置成功。</p>\n<p>若可以正常使用 <code>mysql</code> 命令行，<strong>执行</strong>如下命令：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE DATABASE `sticker`;</span><br></pre></td></tr></table></figure>\n<hr>\n<p><strong>打开</strong> <code>\\csgo\\addons\\sourcemod\\configs\\databases.cfg</code></p>\n<p>在 <code>Databases</code> 下一级<strong>添加</strong>：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&quot;csgo_weaponstickers&quot;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;driver&quot;    &quot;mysql&quot;</span><br><span class=\"line\">    &quot;host&quot;      &quot;localhost&quot;</span><br><span class=\"line\">    &quot;database&quot;  &quot;sticker&quot;</span><br><span class=\"line\">    &quot;user&quot;      &quot;root&quot;</span><br><span class=\"line\">    &quot;pass&quot;      &quot;&quot;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://i.imgur.com/hC4FIhC.png\" alt=\"\"></p>\n<h2 id=\"附加内容\"><a href=\"#附加内容\" class=\"headerlink\" title=\"附加内容\"></a>附加内容</h2><h3 id=\"开放到公网\"><a href=\"#开放到公网\" class=\"headerlink\" title=\"开放到公网\"></a>开放到公网</h3><p>首先到 <a href=\"https://steamcommunity.com/dev/managegameservers\">Steam服务器管理页面</a> <strong>申请一个令牌</strong>：</p>\n<p><img src=\"https://i.imgur.com/uPAi8Om.png\" alt=\"令牌_1\"></p>\n<p>App ID 必须填写730，备忘录无所谓</p>\n<p><img src=\"https://i.imgur.com/1XOFGKW.png\" alt=\"令牌_2\"></p>\n<p>申请得到的登录令牌复制<strong>保存</strong>好，不要泄露</p>\n<p>在 <code>server.cfg</code> 中<strong>加入一行</strong>：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sv_setsteamaccount &quot;你的令牌&quot;</span><br></pre></td></tr></table></figure>\n<p><strong>修改</strong>文件中的 <code>sv_lan 1</code> 改为 <code>sv_lan 0</code></p>\n<p>重启服务器即可</p>\n<hr>\n<h3 id=\"启用远程控制台\"><a href=\"#启用远程控制台\" class=\"headerlink\" title=\"启用远程控制台\"></a>启用远程控制台</h3><p>在 <code>server.cfg</code> 中找到 <code>rcon_password &quot;&quot;</code> 添加你的密码</p>\n<p>然后在游戏中连接到服务器，在控制台输入 <code>rcon_password &quot;你的密码&quot;</code></p>\n<p>之后你就可以在你需要使用的指令前加 <code>rcon</code> 来执行</p>\n<hr>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"步骤\"><a href=\"#步骤\" class=\"headerlink\" title=\"步骤\"></a>步骤</h2><p><a href=\"https://steamcdn-a.akamaihd.net/client/installer/steamcmd.zip\">SteamCMD</a></p>\n<p>下载<code>SteamCMD</code>，<strong>打开</strong><code>steamcmd.exe</code>，启动命令行。</p>\n<p>第一次启动需要等待更新。</p>\n<p>等到可以输入时，<strong>输入</strong>：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">login anonymous</span><br><span class=\"line\">app_update 740 validate</span><br></pre></td></tr></table></figure>\n<p>（复制代码后，在命令窗口右键可以直接粘贴）</p>\n<p><img src=\"https://i.imgur.com/J6pjONd.png\" alt=\"命令行_1\"></p>\n<p>等待服务器下载（约29G）</p>\n<hr>\n<p>等待过程中先去做一些准备工作</p>\n<p><strong>下载</strong>两个mod：</p>\n<p><a href=\"https://mms.alliedmods.net/mmsdrop/1.11/mmsource-1.11.0-git1145-windows.zip\">Metamod</a></p>\n<p><a href=\"https://sm.alliedmods.net/smdrop/1.10/sourcemod-1.10.0-git6528-windows.zip\">SourceMod</a></p>\n<p>分别<strong>解压</strong>待用</p>\n<hr>\n<p>顺便<strong>检查</strong>一下电脑里是否有命令行工具（建议使用 Windows Terminal）</p>\n<p><a href=\"https://www.microsoft.com/en-us/p/windows-terminal/9n0dx20hk701?activetab=pivot:overviewtab\">Windows Terminal</a></p>\n<p>安装好命令行工具后，在<code>Windows徽标</code>右键，可以看到<code>Windows终端(管理员)</code></p>\n<p><strong>输入如下代码</strong>（用来打开电脑的端口）：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">netsh advfirewall firewall add rule name=&quot;CSGO 27015 TCP&quot; dir=in action=allow protocol=TCP localport=27015</span><br><span class=\"line\">netsh advfirewall firewall add rule name=&quot;CSGO 27015 UDP&quot; dir=in action=allow protocol=UDP localport=27015</span><br><span class=\"line\">netsh advfirewall firewall add rule name=&quot;CSGO 27020 UDP&quot; dir=in action=allow protocol=UDP localport=27020</span><br><span class=\"line\">netsh advfirewall firewall add rule name=&quot;CSGO 27015 TCP-o&quot; dir=out action=allow protocol=TCP localport=27015</span><br><span class=\"line\">netsh advfirewall firewall add rule name=&quot;CSGO 27015 UDP-o&quot; dir=out action=allow protocol=UDP localport=27015</span><br><span class=\"line\">netsh advfirewall firewall add rule name=&quot;CSGO 27020 UDP-o&quot; dir=out action=allow protocol=UDP localport=27020</span><br></pre></td></tr></table></figure>\n<hr>\n<p>尝试启动游戏</p>\n<p>在 <code>SteamCMD</code> 的目录下，右键<strong>打开命令行工具</strong>：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd &#x27;.\\steamapps\\common\\Counter-Strike Global Offensive Beta - Dedicated Server\\&#x27;</span><br><span class=\"line\"></span><br><span class=\"line\">.\\srcds.exe -game csgo -console +map de_dust2</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://i.imgur.com/xsnDsVz.png\" alt=\"命令行_2\"></p>\n<p>初步启动完成，可以暂时关闭窗口</p>\n<hr>\n<p>在 <code>csgo/cfg</code> 文件夹里<strong>创建</strong> <code>server.cfg</code>，文件内容复制粘贴如下链接里的内容即可：</p>\n<p><a href=\"https://hackmd.io/@1099255210/rk7UMX5Vq\">server.cfg</a></p>\n<hr>\n<p>接下来<strong>安装</strong><code>Metamod</code>和<code>Sourcemod</code></p>\n<p>将 <code>mmsource-1.11.0-git1145-windows</code> 里的文件夹 <code>addons</code> 拖到<code>csgo</code> 文件夹中</p>\n<p><img src=\"https://i.imgur.com/5JdYipL.png\" alt=\"\"></p>\n<p>将 <code>sourcemod-1.10.0-git6528-windows</code> 里的文件夹 <code>addons</code> 和 <code>cfg</code> 拖到 <code>csgo</code> 文件夹中</p>\n<p><img src=\"https://i.imgur.com/gXHPlEr.png\" alt=\"\"></p>\n<p>安装完成</p>\n<hr>\n<p>接下来<strong>配置服务器管理员</strong></p>\n<p>打开 <code>\\csgo\\addons\\sourcemod\\configs\\admins_simple.ini</code></p>\n<p><a href=\"https://steamid.io/\">获取SteamID</a></p>\n<p>搜索到自己，第一个 steamID 就是我们需要的</p>\n<p>在文件末尾<strong>加一行</strong>：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&quot;STEAM_0:0:......&quot; &quot;99:z&quot;</span><br></pre></td></tr></table></figure>\n<p>前面替换成你的 steamID</p>\n<hr>\n<p>启动测试，还是来到 <code>\\Counter-Strike Global Offensive Beta - Dedicated Server\\</code> 文件夹，<strong>打开命令行</strong>：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">.\\srcds -game csgo -console -usercon +game_type 1 +game_mode 2 +mapgroup mg_allclassic +map de_dust</span><br></pre></td></tr></table></figure>\n<p>这时候服务器应该已经正常启动了</p>\n<p>我们<strong>重新打开一个命令行</strong>，输入 <code>ipconfig</code>，在结果里找到：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 这是我自己电脑的信息</span><br><span class=\"line\">无线局域网适配器 WLAN:</span><br><span class=\"line\"></span><br><span class=\"line\">   连接特定的 DNS 后缀 . . . . . . . :</span><br><span class=\"line\">   本地链接 IPv6 地址. . . . . . . . : fe80::7d80:a5ec:6d19:69a7%18</span><br><span class=\"line\">   IPv4 地址 . . . . . . . . . . . . : 192.168.31.196</span><br><span class=\"line\">   子网掩码  . . . . . . . . . . . . : 255.255.255.0</span><br><span class=\"line\">   默认网关. . . . . . . . . . . . . : 192.168.31.1</span><br></pre></td></tr></table></figure>\n<p>记住其中的 <code>IPv4地址</code></p>\n<p>游玩游戏的电脑需要和服务器连在同一个wifi内</p>\n<p>在游戏端<strong>输入</strong> <code>connect [IPv4地址:27015]</code> 应该能正常进入服务器</p>\n<p>在聊天框内<strong>输入</strong> <code>!admin</code> 如果左边弹出选项，说明管理员认证成功</p>\n<hr>\n<h2 id=\"安装插件\"><a href=\"#安装插件\" class=\"headerlink\" title=\"安装插件\"></a>安装插件</h2><p><strong>安装</strong>mysql</p>\n<p><a href=\"https://dev.mysql.com/get/Downloads/MySQLInstaller/mysql-installer-community-8.0.28.0.msi\">mysql</a></p>\n<p><img src=\"https://i.imgur.com/3jjhOrh.png\" alt=\"\"></p>\n<p>这个页面选择下面一个</p>\n<p>其他按照默认选项安装即可，有错误直接跳过</p>\n<p>密码设置自己记得住的，后面要使用</p>\n<p><strong>添加</strong>系统环境变量</p>\n<p>设置 &gt; 系统 &gt; 设备规格 &gt; 高级系统设置 &gt; 环境变量 &gt; 用户变量 &gt; Path &gt; 编辑 &gt; 新建</p>\n<p>粘贴 <code>C:\\Program Files\\MySQL\\MySQL Server 8.0\\bin</code></p>\n<p>确定即可</p>\n<p>重新打开终端，<strong>输入</strong> <code>mysql -uroot -p</code> 以及密码，测试是否设置成功。</p>\n<p>若可以正常使用 <code>mysql</code> 命令行，<strong>执行</strong>如下命令：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE DATABASE `sticker`;</span><br></pre></td></tr></table></figure>\n<hr>\n<p><strong>打开</strong> <code>\\csgo\\addons\\sourcemod\\configs\\databases.cfg</code></p>\n<p>在 <code>Databases</code> 下一级<strong>添加</strong>：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&quot;csgo_weaponstickers&quot;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;driver&quot;    &quot;mysql&quot;</span><br><span class=\"line\">    &quot;host&quot;      &quot;localhost&quot;</span><br><span class=\"line\">    &quot;database&quot;  &quot;sticker&quot;</span><br><span class=\"line\">    &quot;user&quot;      &quot;root&quot;</span><br><span class=\"line\">    &quot;pass&quot;      &quot;&quot;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://i.imgur.com/hC4FIhC.png\" alt=\"\"></p>\n<h2 id=\"附加内容\"><a href=\"#附加内容\" class=\"headerlink\" title=\"附加内容\"></a>附加内容</h2><h3 id=\"开放到公网\"><a href=\"#开放到公网\" class=\"headerlink\" title=\"开放到公网\"></a>开放到公网</h3><p>首先到 <a href=\"https://steamcommunity.com/dev/managegameservers\">Steam服务器管理页面</a> <strong>申请一个令牌</strong>：</p>\n<p><img src=\"https://i.imgur.com/uPAi8Om.png\" alt=\"令牌_1\"></p>\n<p>App ID 必须填写730，备忘录无所谓</p>\n<p><img src=\"https://i.imgur.com/1XOFGKW.png\" alt=\"令牌_2\"></p>\n<p>申请得到的登录令牌复制<strong>保存</strong>好，不要泄露</p>\n<p>在 <code>server.cfg</code> 中<strong>加入一行</strong>：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sv_setsteamaccount &quot;你的令牌&quot;</span><br></pre></td></tr></table></figure>\n<p><strong>修改</strong>文件中的 <code>sv_lan 1</code> 改为 <code>sv_lan 0</code></p>\n<p>重启服务器即可</p>\n<hr>\n<h3 id=\"启用远程控制台\"><a href=\"#启用远程控制台\" class=\"headerlink\" title=\"启用远程控制台\"></a>启用远程控制台</h3><p>在 <code>server.cfg</code> 中找到 <code>rcon_password &quot;&quot;</code> 添加你的密码</p>\n<p>然后在游戏中连接到服务器，在控制台输入 <code>rcon_password &quot;你的密码&quot;</code></p>\n<p>之后你就可以在你需要使用的指令前加 <code>rcon</code> 来执行</p>\n<hr>\n"},{"title":"注意力机制","date":"2022-07-24T22:39:08.000Z","mathjax":true,"_content":"\n# 注意力机制\n\n## 什么是注意力机制\n\n在深度学习领域，模型往往需要接收和处理大量的数据，然而在特定的某个时刻，往往只有少部分的某些数据是重要的\n\n心理学框架：人类根据**随意线索**和**不随意线索**选择注意点\n\n卷积、全连接、池化层都只考虑不随意线索（将本身容易抽取的特征抽取出来）\n\n注意力机制则考虑**随意线索**\n\n- 随意线索对应查询（query）\n- 每个输入是值（value）和不随意线索（key）的对\n- 通过注意力池化层来有偏向性地选择某些输入\n\n## 非参注意力池化层\n\n对于给定的数据$(x_i,y_i),i=1,...,n$）（key, value） \n\n最简单的方案：平均池化 $f(x)=\\frac{1}{n}\\sum\\limits_{i}{y_i}$\n\n```python\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\nn_train = 50\n# torch.rand()产生一个服从均匀分布的张量，张量内的数据包含从区间[0,1)的随机数。\n# 参数size是一个整数序列，用于定义张量大小\n# torch.sort()返回两个值 第一个为排序后的张量，第二个为原来的索引\nx_train, _ = torch.sort(torch.rand(n_train) * 5)   # *5表示将[0,1)扩展到[0,5)\n\ndef f(x):   # 真实的f(x)  需要被拟合\n  return 2 * torch.cos(x) + x ** 0.7\n\ny_train = f(x_train) + torch.normal(0, 0.5, (n_train,))\n# 测试集\nx_test = torch.arange(0, 5, 0.1)\ny_truth = f(x_test)\nn_test = len(x_test)\n\ndef plot_kernel_reg(y_hat):\n  d2l.plot(x_test, [y_truth, y_hat], 'x', 'y', legend=['Truth', 'pred'], xlim=[0, 5], ylim=[-1, 5])\n  d2l.plt.plot(x_train, y_train, 'o', alpha=0.5)\n  plt.show()\n\ny_hat = torch.repeat_interleave(y_train.mean(), n_test)\nplot_kernel_reg(y_hat)\n```\n\n![image-20220802055649966](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220802055649966.png)\n\n更好的方案：Nadaraya-Watson 核回归：\n\n$$\nf(x)=\\sum_{i=1}^{n}\\frac{K(x-x_i)}{\\sum_{j=1}^{n}K(x-x_j)}y_i\n$$\n（找到离已知值最近的值，类似于KNN）\n\n```python\n#X_repeat的形状为(n_test, n_train) 同一行的所有元素（测试输入，查询）都相同 每一行的结果为一个点\nX_repeat = x_test.repeat_interleave(n_train).reshape((-1, n_train))  #重复n_train次\n# x_train包含着键。attention_weights的形状：(n_test,n_train)\n# 每一行都包含着要在给定的每个查询的值（y_train）之间分配的注意力权重\nattention_weights = nn.functional.softmax(-(X_repeat - x_train) ** 2 / 2, dim=1)\n# y_hat的每个元素都是值的加权平均值，其中的权重是注意力权重\n# y_train = y_train.reshape(-1, 1)  #这一步可以不用做 matmul可以实现不同维度的矩阵成绩 但如果使用mm函数进行矩阵乘积必须转化为一列\ny_hat = torch.matmul(attention_weights, y_train)\nplot_kernel_reg(y_hat)\n```\n\n\n\n![image-20220802055728239](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220802055728239.png)\n\n## 参数化注意力机制\n\n在此基础上引入可学习的参数w\n$$\nf(x)=\\sum_{i=1}^{n}softmax(-\\frac{1}{2}((x-x_i)w)^2)y_i\n$$\n\n```python\nclass NWKernelRegression(nn.Module):\n  def __init__(self, **kwargs):\n    super(NWKernelRegression, self).__init__()\n    self.w = nn.Parameter(torch.rand((1, ), requires_grad=True))\n\n  def forward(self, queries, keys, values):\n    #queries和attention_weights的形状为(查询个数, 键-值 对个数)\n    queries = queries.repeat_interleave(keys.shape[1]).reshape((-1, keys.shape[1]))\n    self.attention_weights = nn.functional.softmax(\n      -((queries - keys) * self.w) ** 2 / 2, dim=1   #dim=1表示按行计算\n    )\n    return torch.bmm(\n      self.attention_weights.unsqueeze(1),\n      values.unsqueeze(-1)\n    ).reshape(-1)  #并将结果变成一维\n\nnet = NWKernelRegression()\nloss = nn.MSELoss(reduction='none')\noptimizer = torch.optim.SGD(net.parameters(), lr=0.5)\nanimator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[1, 5])\n\nfor epoch in range(5):\n  # keys的形状:(n_test，n_train)，每一行包含着相同的训练输入（例如，相同的键）\n  keys = x_train.repeat((n_test, 1))\n  # value的形状:(n_test，n_train)\n  values = y_train.repeat((n_test, 1))\n  y_hat = net(x_test, keys, values).unsqueeze(1).detach()\n  \n  optimizer.zero_grad()  #梯度清零\n  l = loss(net(x_train, keys, values), y_train)  #计算损失\n  l.sum().backward()  #将损失之和进行反向传播\n  optimizer.step()  #更新\n  print(f'epoch {epoch + 1}, loss {float(l.sum()):.6f}')\n  animator.add(epoch + 1, float(l.sum()))\n\nplt.show()\nplot_kernel_reg(y_hat)\n```\n\n![image-20220802055910798](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220802055910798.png)\n\n得到的曲线不如之前的平滑，但是更加接近真实值\n\n![image-20220802060021336](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220802060021336.png)","source":"_posts/注意力机制.md","raw":"---\ntitle: 注意力机制\ndate: 2022-07-25 06:39:08\ntags:\nmathjax: true\n---\n\n# 注意力机制\n\n## 什么是注意力机制\n\n在深度学习领域，模型往往需要接收和处理大量的数据，然而在特定的某个时刻，往往只有少部分的某些数据是重要的\n\n心理学框架：人类根据**随意线索**和**不随意线索**选择注意点\n\n卷积、全连接、池化层都只考虑不随意线索（将本身容易抽取的特征抽取出来）\n\n注意力机制则考虑**随意线索**\n\n- 随意线索对应查询（query）\n- 每个输入是值（value）和不随意线索（key）的对\n- 通过注意力池化层来有偏向性地选择某些输入\n\n## 非参注意力池化层\n\n对于给定的数据$(x_i,y_i),i=1,...,n$）（key, value） \n\n最简单的方案：平均池化 $f(x)=\\frac{1}{n}\\sum\\limits_{i}{y_i}$\n\n```python\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\nn_train = 50\n# torch.rand()产生一个服从均匀分布的张量，张量内的数据包含从区间[0,1)的随机数。\n# 参数size是一个整数序列，用于定义张量大小\n# torch.sort()返回两个值 第一个为排序后的张量，第二个为原来的索引\nx_train, _ = torch.sort(torch.rand(n_train) * 5)   # *5表示将[0,1)扩展到[0,5)\n\ndef f(x):   # 真实的f(x)  需要被拟合\n  return 2 * torch.cos(x) + x ** 0.7\n\ny_train = f(x_train) + torch.normal(0, 0.5, (n_train,))\n# 测试集\nx_test = torch.arange(0, 5, 0.1)\ny_truth = f(x_test)\nn_test = len(x_test)\n\ndef plot_kernel_reg(y_hat):\n  d2l.plot(x_test, [y_truth, y_hat], 'x', 'y', legend=['Truth', 'pred'], xlim=[0, 5], ylim=[-1, 5])\n  d2l.plt.plot(x_train, y_train, 'o', alpha=0.5)\n  plt.show()\n\ny_hat = torch.repeat_interleave(y_train.mean(), n_test)\nplot_kernel_reg(y_hat)\n```\n\n![image-20220802055649966](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220802055649966.png)\n\n更好的方案：Nadaraya-Watson 核回归：\n\n$$\nf(x)=\\sum_{i=1}^{n}\\frac{K(x-x_i)}{\\sum_{j=1}^{n}K(x-x_j)}y_i\n$$\n（找到离已知值最近的值，类似于KNN）\n\n```python\n#X_repeat的形状为(n_test, n_train) 同一行的所有元素（测试输入，查询）都相同 每一行的结果为一个点\nX_repeat = x_test.repeat_interleave(n_train).reshape((-1, n_train))  #重复n_train次\n# x_train包含着键。attention_weights的形状：(n_test,n_train)\n# 每一行都包含着要在给定的每个查询的值（y_train）之间分配的注意力权重\nattention_weights = nn.functional.softmax(-(X_repeat - x_train) ** 2 / 2, dim=1)\n# y_hat的每个元素都是值的加权平均值，其中的权重是注意力权重\n# y_train = y_train.reshape(-1, 1)  #这一步可以不用做 matmul可以实现不同维度的矩阵成绩 但如果使用mm函数进行矩阵乘积必须转化为一列\ny_hat = torch.matmul(attention_weights, y_train)\nplot_kernel_reg(y_hat)\n```\n\n\n\n![image-20220802055728239](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220802055728239.png)\n\n## 参数化注意力机制\n\n在此基础上引入可学习的参数w\n$$\nf(x)=\\sum_{i=1}^{n}softmax(-\\frac{1}{2}((x-x_i)w)^2)y_i\n$$\n\n```python\nclass NWKernelRegression(nn.Module):\n  def __init__(self, **kwargs):\n    super(NWKernelRegression, self).__init__()\n    self.w = nn.Parameter(torch.rand((1, ), requires_grad=True))\n\n  def forward(self, queries, keys, values):\n    #queries和attention_weights的形状为(查询个数, 键-值 对个数)\n    queries = queries.repeat_interleave(keys.shape[1]).reshape((-1, keys.shape[1]))\n    self.attention_weights = nn.functional.softmax(\n      -((queries - keys) * self.w) ** 2 / 2, dim=1   #dim=1表示按行计算\n    )\n    return torch.bmm(\n      self.attention_weights.unsqueeze(1),\n      values.unsqueeze(-1)\n    ).reshape(-1)  #并将结果变成一维\n\nnet = NWKernelRegression()\nloss = nn.MSELoss(reduction='none')\noptimizer = torch.optim.SGD(net.parameters(), lr=0.5)\nanimator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[1, 5])\n\nfor epoch in range(5):\n  # keys的形状:(n_test，n_train)，每一行包含着相同的训练输入（例如，相同的键）\n  keys = x_train.repeat((n_test, 1))\n  # value的形状:(n_test，n_train)\n  values = y_train.repeat((n_test, 1))\n  y_hat = net(x_test, keys, values).unsqueeze(1).detach()\n  \n  optimizer.zero_grad()  #梯度清零\n  l = loss(net(x_train, keys, values), y_train)  #计算损失\n  l.sum().backward()  #将损失之和进行反向传播\n  optimizer.step()  #更新\n  print(f'epoch {epoch + 1}, loss {float(l.sum()):.6f}')\n  animator.add(epoch + 1, float(l.sum()))\n\nplt.show()\nplot_kernel_reg(y_hat)\n```\n\n![image-20220802055910798](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220802055910798.png)\n\n得到的曲线不如之前的平滑，但是更加接近真实值\n\n![image-20220802060021336](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220802060021336.png)","slug":"注意力机制","published":1,"updated":"2022-08-25T13:13:32.986Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clgyiaru1000kjku7d1h0b2e0","content":"<h1 id=\"注意力机制\"><a href=\"#注意力机制\" class=\"headerlink\" title=\"注意力机制\"></a>注意力机制</h1><h2 id=\"什么是注意力机制\"><a href=\"#什么是注意力机制\" class=\"headerlink\" title=\"什么是注意力机制\"></a>什么是注意力机制</h2><p>在深度学习领域，模型往往需要接收和处理大量的数据，然而在特定的某个时刻，往往只有少部分的某些数据是重要的</p>\n<p>心理学框架：人类根据<strong>随意线索</strong>和<strong>不随意线索</strong>选择注意点</p>\n<p>卷积、全连接、池化层都只考虑不随意线索（将本身容易抽取的特征抽取出来）</p>\n<p>注意力机制则考虑<strong>随意线索</strong></p>\n<ul>\n<li>随意线索对应查询（query）</li>\n<li>每个输入是值（value）和不随意线索（key）的对</li>\n<li>通过注意力池化层来有偏向性地选择某些输入</li>\n</ul>\n<h2 id=\"非参注意力池化层\"><a href=\"#非参注意力池化层\" class=\"headerlink\" title=\"非参注意力池化层\"></a>非参注意力池化层</h2><p>对于给定的数据$(x_i,y_i),i=1,…,n$）（key, value） </p>\n<p>最简单的方案：平均池化 $f(x)=\\frac{1}{n}\\sum\\limits_{i}{y_i}$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">n_train = <span class=\"number\">50</span></span><br><span class=\"line\"><span class=\"comment\"># torch.rand()产生一个服从均匀分布的张量，张量内的数据包含从区间[0,1)的随机数。</span></span><br><span class=\"line\"><span class=\"comment\"># 参数size是一个整数序列，用于定义张量大小</span></span><br><span class=\"line\"><span class=\"comment\"># torch.sort()返回两个值 第一个为排序后的张量，第二个为原来的索引</span></span><br><span class=\"line\">x_train, _ = torch.sort(torch.rand(n_train) * <span class=\"number\">5</span>)   <span class=\"comment\"># *5表示将[0,1)扩展到[0,5)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">f</span>(<span class=\"params\">x</span>):   <span class=\"comment\"># 真实的f(x)  需要被拟合</span></span><br><span class=\"line\">  <span class=\"keyword\">return</span> <span class=\"number\">2</span> * torch.cos(x) + x ** <span class=\"number\">0.7</span></span><br><span class=\"line\"></span><br><span class=\"line\">y_train = f(x_train) + torch.normal(<span class=\"number\">0</span>, <span class=\"number\">0.5</span>, (n_train,))</span><br><span class=\"line\"><span class=\"comment\"># 测试集</span></span><br><span class=\"line\">x_test = torch.arange(<span class=\"number\">0</span>, <span class=\"number\">5</span>, <span class=\"number\">0.1</span>)</span><br><span class=\"line\">y_truth = f(x_test)</span><br><span class=\"line\">n_test = <span class=\"built_in\">len</span>(x_test)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">plot_kernel_reg</span>(<span class=\"params\">y_hat</span>):</span><br><span class=\"line\">  d2l.plot(x_test, [y_truth, y_hat], <span class=\"string\">&#x27;x&#x27;</span>, <span class=\"string\">&#x27;y&#x27;</span>, legend=[<span class=\"string\">&#x27;Truth&#x27;</span>, <span class=\"string\">&#x27;pred&#x27;</span>], xlim=[<span class=\"number\">0</span>, <span class=\"number\">5</span>], ylim=[-<span class=\"number\">1</span>, <span class=\"number\">5</span>])</span><br><span class=\"line\">  d2l.plt.plot(x_train, y_train, <span class=\"string\">&#x27;o&#x27;</span>, alpha=<span class=\"number\">0.5</span>)</span><br><span class=\"line\">  plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\">y_hat = torch.repeat_interleave(y_train.mean(), n_test)</span><br><span class=\"line\">plot_kernel_reg(y_hat)</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220802055649966.png\" alt=\"image-20220802055649966\"></p>\n<p>更好的方案：Nadaraya-Watson 核回归：</p>\n<script type=\"math/tex; mode=display\">\nf(x)=\\sum_{i=1}^{n}\\frac{K(x-x_i)}{\\sum_{j=1}^{n}K(x-x_j)}y_i</script><p>（找到离已知值最近的值，类似于KNN）</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#X_repeat的形状为(n_test, n_train) 同一行的所有元素（测试输入，查询）都相同 每一行的结果为一个点</span></span><br><span class=\"line\">X_repeat = x_test.repeat_interleave(n_train).reshape((-<span class=\"number\">1</span>, n_train))  <span class=\"comment\">#重复n_train次</span></span><br><span class=\"line\"><span class=\"comment\"># x_train包含着键。attention_weights的形状：(n_test,n_train)</span></span><br><span class=\"line\"><span class=\"comment\"># 每一行都包含着要在给定的每个查询的值（y_train）之间分配的注意力权重</span></span><br><span class=\"line\">attention_weights = nn.functional.softmax(-(X_repeat - x_train) ** <span class=\"number\">2</span> / <span class=\"number\">2</span>, dim=<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\"># y_hat的每个元素都是值的加权平均值，其中的权重是注意力权重</span></span><br><span class=\"line\"><span class=\"comment\"># y_train = y_train.reshape(-1, 1)  #这一步可以不用做 matmul可以实现不同维度的矩阵成绩 但如果使用mm函数进行矩阵乘积必须转化为一列</span></span><br><span class=\"line\">y_hat = torch.matmul(attention_weights, y_train)</span><br><span class=\"line\">plot_kernel_reg(y_hat)</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220802055728239.png\" alt=\"image-20220802055728239\"></p>\n<h2 id=\"参数化注意力机制\"><a href=\"#参数化注意力机制\" class=\"headerlink\" title=\"参数化注意力机制\"></a>参数化注意力机制</h2><p>在此基础上引入可学习的参数w</p>\n<script type=\"math/tex; mode=display\">\nf(x)=\\sum_{i=1}^{n}softmax(-\\frac{1}{2}((x-x_i)w)^2)y_i</script><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">NWKernelRegression</span>(nn.Module):</span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, **kwargs</span>):</span><br><span class=\"line\">    <span class=\"built_in\">super</span>(NWKernelRegression, self).__init__()</span><br><span class=\"line\">    self.w = nn.Parameter(torch.rand((<span class=\"number\">1</span>, ), requires_grad=<span class=\"literal\">True</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, queries, keys, values</span>):</span><br><span class=\"line\">    <span class=\"comment\">#queries和attention_weights的形状为(查询个数, 键-值 对个数)</span></span><br><span class=\"line\">    queries = queries.repeat_interleave(keys.shape[<span class=\"number\">1</span>]).reshape((-<span class=\"number\">1</span>, keys.shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">    self.attention_weights = nn.functional.softmax(</span><br><span class=\"line\">      -((queries - keys) * self.w) ** <span class=\"number\">2</span> / <span class=\"number\">2</span>, dim=<span class=\"number\">1</span>   <span class=\"comment\">#dim=1表示按行计算</span></span><br><span class=\"line\">    )</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.bmm(</span><br><span class=\"line\">      self.attention_weights.unsqueeze(<span class=\"number\">1</span>),</span><br><span class=\"line\">      values.unsqueeze(-<span class=\"number\">1</span>)</span><br><span class=\"line\">    ).reshape(-<span class=\"number\">1</span>)  <span class=\"comment\">#并将结果变成一维</span></span><br><span class=\"line\"></span><br><span class=\"line\">net = NWKernelRegression()</span><br><span class=\"line\">loss = nn.MSELoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\">optimizer = torch.optim.SGD(net.parameters(), lr=<span class=\"number\">0.5</span>)</span><br><span class=\"line\">animator = d2l.Animator(xlabel=<span class=\"string\">&#x27;epoch&#x27;</span>, ylabel=<span class=\"string\">&#x27;loss&#x27;</span>, xlim=[<span class=\"number\">1</span>, <span class=\"number\">5</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">5</span>):</span><br><span class=\"line\">  <span class=\"comment\"># keys的形状:(n_test，n_train)，每一行包含着相同的训练输入（例如，相同的键）</span></span><br><span class=\"line\">  keys = x_train.repeat((n_test, <span class=\"number\">1</span>))</span><br><span class=\"line\">  <span class=\"comment\"># value的形状:(n_test，n_train)</span></span><br><span class=\"line\">  values = y_train.repeat((n_test, <span class=\"number\">1</span>))</span><br><span class=\"line\">  y_hat = net(x_test, keys, values).unsqueeze(<span class=\"number\">1</span>).detach()</span><br><span class=\"line\">  </span><br><span class=\"line\">  optimizer.zero_grad()  <span class=\"comment\">#梯度清零</span></span><br><span class=\"line\">  l = loss(net(x_train, keys, values), y_train)  <span class=\"comment\">#计算损失</span></span><br><span class=\"line\">  l.<span class=\"built_in\">sum</span>().backward()  <span class=\"comment\">#将损失之和进行反向传播</span></span><br><span class=\"line\">  optimizer.step()  <span class=\"comment\">#更新</span></span><br><span class=\"line\">  <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;epoch <span class=\"subst\">&#123;epoch + <span class=\"number\">1</span>&#125;</span>, loss <span class=\"subst\">&#123;<span class=\"built_in\">float</span>(l.<span class=\"built_in\">sum</span>()):<span class=\"number\">.6</span>f&#125;</span>&#x27;</span>)</span><br><span class=\"line\">  animator.add(epoch + <span class=\"number\">1</span>, <span class=\"built_in\">float</span>(l.<span class=\"built_in\">sum</span>()))</span><br><span class=\"line\"></span><br><span class=\"line\">plt.show()</span><br><span class=\"line\">plot_kernel_reg(y_hat)</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220802055910798.png\" alt=\"image-20220802055910798\"></p>\n<p>得到的曲线不如之前的平滑，但是更加接近真实值</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220802060021336.png\" alt=\"image-20220802060021336\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"注意力机制\"><a href=\"#注意力机制\" class=\"headerlink\" title=\"注意力机制\"></a>注意力机制</h1><h2 id=\"什么是注意力机制\"><a href=\"#什么是注意力机制\" class=\"headerlink\" title=\"什么是注意力机制\"></a>什么是注意力机制</h2><p>在深度学习领域，模型往往需要接收和处理大量的数据，然而在特定的某个时刻，往往只有少部分的某些数据是重要的</p>\n<p>心理学框架：人类根据<strong>随意线索</strong>和<strong>不随意线索</strong>选择注意点</p>\n<p>卷积、全连接、池化层都只考虑不随意线索（将本身容易抽取的特征抽取出来）</p>\n<p>注意力机制则考虑<strong>随意线索</strong></p>\n<ul>\n<li>随意线索对应查询（query）</li>\n<li>每个输入是值（value）和不随意线索（key）的对</li>\n<li>通过注意力池化层来有偏向性地选择某些输入</li>\n</ul>\n<h2 id=\"非参注意力池化层\"><a href=\"#非参注意力池化层\" class=\"headerlink\" title=\"非参注意力池化层\"></a>非参注意力池化层</h2><p>对于给定的数据$(x_i,y_i),i=1,…,n$）（key, value） </p>\n<p>最简单的方案：平均池化 $f(x)=\\frac{1}{n}\\sum\\limits_{i}{y_i}$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">n_train = <span class=\"number\">50</span></span><br><span class=\"line\"><span class=\"comment\"># torch.rand()产生一个服从均匀分布的张量，张量内的数据包含从区间[0,1)的随机数。</span></span><br><span class=\"line\"><span class=\"comment\"># 参数size是一个整数序列，用于定义张量大小</span></span><br><span class=\"line\"><span class=\"comment\"># torch.sort()返回两个值 第一个为排序后的张量，第二个为原来的索引</span></span><br><span class=\"line\">x_train, _ = torch.sort(torch.rand(n_train) * <span class=\"number\">5</span>)   <span class=\"comment\"># *5表示将[0,1)扩展到[0,5)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">f</span>(<span class=\"params\">x</span>):   <span class=\"comment\"># 真实的f(x)  需要被拟合</span></span><br><span class=\"line\">  <span class=\"keyword\">return</span> <span class=\"number\">2</span> * torch.cos(x) + x ** <span class=\"number\">0.7</span></span><br><span class=\"line\"></span><br><span class=\"line\">y_train = f(x_train) + torch.normal(<span class=\"number\">0</span>, <span class=\"number\">0.5</span>, (n_train,))</span><br><span class=\"line\"><span class=\"comment\"># 测试集</span></span><br><span class=\"line\">x_test = torch.arange(<span class=\"number\">0</span>, <span class=\"number\">5</span>, <span class=\"number\">0.1</span>)</span><br><span class=\"line\">y_truth = f(x_test)</span><br><span class=\"line\">n_test = <span class=\"built_in\">len</span>(x_test)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">plot_kernel_reg</span>(<span class=\"params\">y_hat</span>):</span><br><span class=\"line\">  d2l.plot(x_test, [y_truth, y_hat], <span class=\"string\">&#x27;x&#x27;</span>, <span class=\"string\">&#x27;y&#x27;</span>, legend=[<span class=\"string\">&#x27;Truth&#x27;</span>, <span class=\"string\">&#x27;pred&#x27;</span>], xlim=[<span class=\"number\">0</span>, <span class=\"number\">5</span>], ylim=[-<span class=\"number\">1</span>, <span class=\"number\">5</span>])</span><br><span class=\"line\">  d2l.plt.plot(x_train, y_train, <span class=\"string\">&#x27;o&#x27;</span>, alpha=<span class=\"number\">0.5</span>)</span><br><span class=\"line\">  plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\">y_hat = torch.repeat_interleave(y_train.mean(), n_test)</span><br><span class=\"line\">plot_kernel_reg(y_hat)</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220802055649966.png\" alt=\"image-20220802055649966\"></p>\n<p>更好的方案：Nadaraya-Watson 核回归：</p>\n<script type=\"math/tex; mode=display\">\nf(x)=\\sum_{i=1}^{n}\\frac{K(x-x_i)}{\\sum_{j=1}^{n}K(x-x_j)}y_i</script><p>（找到离已知值最近的值，类似于KNN）</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#X_repeat的形状为(n_test, n_train) 同一行的所有元素（测试输入，查询）都相同 每一行的结果为一个点</span></span><br><span class=\"line\">X_repeat = x_test.repeat_interleave(n_train).reshape((-<span class=\"number\">1</span>, n_train))  <span class=\"comment\">#重复n_train次</span></span><br><span class=\"line\"><span class=\"comment\"># x_train包含着键。attention_weights的形状：(n_test,n_train)</span></span><br><span class=\"line\"><span class=\"comment\"># 每一行都包含着要在给定的每个查询的值（y_train）之间分配的注意力权重</span></span><br><span class=\"line\">attention_weights = nn.functional.softmax(-(X_repeat - x_train) ** <span class=\"number\">2</span> / <span class=\"number\">2</span>, dim=<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\"># y_hat的每个元素都是值的加权平均值，其中的权重是注意力权重</span></span><br><span class=\"line\"><span class=\"comment\"># y_train = y_train.reshape(-1, 1)  #这一步可以不用做 matmul可以实现不同维度的矩阵成绩 但如果使用mm函数进行矩阵乘积必须转化为一列</span></span><br><span class=\"line\">y_hat = torch.matmul(attention_weights, y_train)</span><br><span class=\"line\">plot_kernel_reg(y_hat)</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220802055728239.png\" alt=\"image-20220802055728239\"></p>\n<h2 id=\"参数化注意力机制\"><a href=\"#参数化注意力机制\" class=\"headerlink\" title=\"参数化注意力机制\"></a>参数化注意力机制</h2><p>在此基础上引入可学习的参数w</p>\n<script type=\"math/tex; mode=display\">\nf(x)=\\sum_{i=1}^{n}softmax(-\\frac{1}{2}((x-x_i)w)^2)y_i</script><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">NWKernelRegression</span>(nn.Module):</span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, **kwargs</span>):</span><br><span class=\"line\">    <span class=\"built_in\">super</span>(NWKernelRegression, self).__init__()</span><br><span class=\"line\">    self.w = nn.Parameter(torch.rand((<span class=\"number\">1</span>, ), requires_grad=<span class=\"literal\">True</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, queries, keys, values</span>):</span><br><span class=\"line\">    <span class=\"comment\">#queries和attention_weights的形状为(查询个数, 键-值 对个数)</span></span><br><span class=\"line\">    queries = queries.repeat_interleave(keys.shape[<span class=\"number\">1</span>]).reshape((-<span class=\"number\">1</span>, keys.shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">    self.attention_weights = nn.functional.softmax(</span><br><span class=\"line\">      -((queries - keys) * self.w) ** <span class=\"number\">2</span> / <span class=\"number\">2</span>, dim=<span class=\"number\">1</span>   <span class=\"comment\">#dim=1表示按行计算</span></span><br><span class=\"line\">    )</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.bmm(</span><br><span class=\"line\">      self.attention_weights.unsqueeze(<span class=\"number\">1</span>),</span><br><span class=\"line\">      values.unsqueeze(-<span class=\"number\">1</span>)</span><br><span class=\"line\">    ).reshape(-<span class=\"number\">1</span>)  <span class=\"comment\">#并将结果变成一维</span></span><br><span class=\"line\"></span><br><span class=\"line\">net = NWKernelRegression()</span><br><span class=\"line\">loss = nn.MSELoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\">optimizer = torch.optim.SGD(net.parameters(), lr=<span class=\"number\">0.5</span>)</span><br><span class=\"line\">animator = d2l.Animator(xlabel=<span class=\"string\">&#x27;epoch&#x27;</span>, ylabel=<span class=\"string\">&#x27;loss&#x27;</span>, xlim=[<span class=\"number\">1</span>, <span class=\"number\">5</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">5</span>):</span><br><span class=\"line\">  <span class=\"comment\"># keys的形状:(n_test，n_train)，每一行包含着相同的训练输入（例如，相同的键）</span></span><br><span class=\"line\">  keys = x_train.repeat((n_test, <span class=\"number\">1</span>))</span><br><span class=\"line\">  <span class=\"comment\"># value的形状:(n_test，n_train)</span></span><br><span class=\"line\">  values = y_train.repeat((n_test, <span class=\"number\">1</span>))</span><br><span class=\"line\">  y_hat = net(x_test, keys, values).unsqueeze(<span class=\"number\">1</span>).detach()</span><br><span class=\"line\">  </span><br><span class=\"line\">  optimizer.zero_grad()  <span class=\"comment\">#梯度清零</span></span><br><span class=\"line\">  l = loss(net(x_train, keys, values), y_train)  <span class=\"comment\">#计算损失</span></span><br><span class=\"line\">  l.<span class=\"built_in\">sum</span>().backward()  <span class=\"comment\">#将损失之和进行反向传播</span></span><br><span class=\"line\">  optimizer.step()  <span class=\"comment\">#更新</span></span><br><span class=\"line\">  <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;epoch <span class=\"subst\">&#123;epoch + <span class=\"number\">1</span>&#125;</span>, loss <span class=\"subst\">&#123;<span class=\"built_in\">float</span>(l.<span class=\"built_in\">sum</span>()):<span class=\"number\">.6</span>f&#125;</span>&#x27;</span>)</span><br><span class=\"line\">  animator.add(epoch + <span class=\"number\">1</span>, <span class=\"built_in\">float</span>(l.<span class=\"built_in\">sum</span>()))</span><br><span class=\"line\"></span><br><span class=\"line\">plt.show()</span><br><span class=\"line\">plot_kernel_reg(y_hat)</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220802055910798.png\" alt=\"image-20220802055910798\"></p>\n<p>得到的曲线不如之前的平滑，但是更加接近真实值</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220802060021336.png\" alt=\"image-20220802060021336\"></p>\n"},{"title":"神经网络模型","date":"2022-07-26T12:09:08.000Z","mathjax":true,"_content":"\n# 神经网络模型\n\n## Lenet\n\n最初应用为识别手写数字\n\n早期成功的神经网络\n\n先使用卷积层学习图片的空间信息，池化层(平均)降低图片敏感度，然后使用全连接层来转换到类别空间\n\n![image-20220725190350958](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220725190350958.png)\n\n```python\nclass LeNet(nn.Module):\n  def __init__(self):\n    super(LeNet, self).__init__()\n    self.relu = nn.ReLU()\n    self.pool = nn.AvgPool2d((2,2), (2,2))\n    self.conv1 = nn.Conv2d(1, 6, (5,5), (1,1), (0,0))\n    self.conv2 = nn.Conv2d(6, 16, (5,5), (1,1), (0,0))\n    self.conv3 = nn.Conv2d(16, 120, (5,5), (1,1), (0,0))\n    self.linear1 = nn.Linear(120, 84)\n    self.linear2 = nn.Linear(84, 10)\n\n  def forward(self, x):\n    x = self.relu(self.conv1(x))\n    x = self.pool(x)\n    x = self.relu(self.conv2(x))\n    x = self.pool(x)\n    x = self.relu(self.conv3(x))\n    x = x.reshape(x.shape[0], -1)\n    x = self.relu(self.linear1(x))\n    x = self.linear2(x)\n    return x\n```\n\n## AlexNet\n\nwon 2012 ImageNet\n\n本质上是更深更大的 LeNet\n\n改进了：\n\n- 丢弃法\n- ReLu\n- MaxPooling\n- Overlapping Pooling (stride=2, kernel_size=3)\n\n架构：\n\n![image-20220727185015997](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220727185015997.png)\n\n写代码的时候出现了一个问题：AlexNet 的输入图像的宽高到底是224还是227？因为这影响到卷积层参数的计算。\n\n[网络上的相关讨论](https://datascience.stackexchange.com/questions/29245/what-is-the-input-size-of-alex-net)\n\n这里我写代码的时候暂且按照 227 * 227 * 3 的输入作处理\n\n```python\nclass AlexNet(nn.Module):\n  def __init__(self):\n    super(AlexNet,self).__init__()\n    self.relu = nn.ReLU()\n    self.pool = nn.MaxPool2d((3,3), (2,2))\n    self.dropout = nn.Dropout(0.5)\n    self.flatten = nn.Flatten()\n    self.conv1 = nn.Conv2d(3, 48, (11,11), (4,4), (0,0))\n    self.conv2 = nn.Conv2d(48, 128, (5,5), (1,1), (2,2))\n    self.conv3 = nn.Conv2d(128, 192, (3,3), (1,1), (1,1))\n    self.conv4 = nn.Conv2d(192, 192, (3,3), (1,1), (1,1))\n    self.conv5 = nn.Conv2d(192, 128, (3,3), (1,1), (1,1))\n    self.linear1 = nn.Linear(6*6*128, 2048)\n    self.linear2 = nn.Linear(2048, 2048)\n    self.linear3 = nn.Linear(2048, 1000)\n\n  def forward(self, x):\n    x = self.relu(self.conv1(x))\n    x = self.pool(x)\n    x = self.relu(self.conv2(x))\n    x = self.pool(x)\n    x = self.relu(self.conv3(x))\n    x = self.relu(self.conv4(x))\n    x = self.relu(self.conv5(x))\n    x = self.pool(x)\n\n    x = self.flatten(x)\n    x = self.relu(self.linear1(x))\n    x = self.dropout(x)\n    x = self.relu(self.linear2(x))\n    x = self.dropout(x)\n    x = self.linear3(x)\n    return x\n```\n\n## VGG\n\n![image-20220731073437523](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220731073437523.png)\n\n特点：\n\n- 小卷积核、多卷积子层\n- 小池化核\n- 通道数多\n\nvgg代码(这里简单实现了一个VGG16)\n\n```python\nclass VGG16(nn.Module):\n  def __init__(self):\n    super(VGG16,self).__init__()\n    self.relu = nn.ReLU()\n    self.pool = nn.MaxPool2d((2,2), (2,2))\n    self.dropout = nn.Dropout(0.5)\n    self.flatten = nn.Flatten()\n    self.conv1_1 = nn.Conv2d(3, 64, (3,3), (1,1), (1,1))\n    self.conv1_2 = nn.Conv2d(64, 64, (3,3), (1,1), (1,1))\n    self.conv2_1 = nn.Conv2d(64, 128, (3,3), (1,1), (1,1))\n    self.conv2_2 = nn.Conv2d(128, 128, (3,3), (1,1), (1,1))\n    self.conv3_1 = nn.Conv2d(128, 256, (3,3), (1,1), (1,1))\n    self.conv3_2 = nn.Conv2d(256, 256, (3,3), (1,1), (1,1))\n    self.conv4_1 = nn.Conv2d(256, 512, (3,3), (1,1), (1,1))\n    self.conv4_2 = nn.Conv2d(512, 512, (3,3), (1,1), (1,1))\n    self.linear1 = nn.Linear(7*7*512, 4096)\n    self.linear2 = nn.Linear(4096, 4096)\n    self.linear3 = nn.Linear(4096, 1000)\n\n  def forward(self, x):\n    x = self.relu(self.conv1_1(x))\n    x = self.relu(self.conv1_2(x))\n    x = self.pool(x)\n    x = self.relu(self.conv2_1(x))\n    x = self.relu(self.conv2_2(x))\n    x = self.pool(x)\n    x = self.relu(self.conv3_1(x))\n    x = self.relu(self.conv3_2(x))\n    x = self.relu(self.conv3_2(x))\n    x = self.pool(x)\n    x = self.relu(self.conv4_1(x))\n    x = self.relu(self.conv4_2(x))\n    x = self.relu(self.conv4_2(x))\n    x = self.pool(x)\n    x = self.relu(self.conv4_2(x))\n    x = self.relu(self.conv4_2(x))\n    x = self.relu(self.conv4_2(x))\n    x = self.pool(x)\n\n    x = self.flatten(x)\n    x = self.relu(self.linear1(x))\n    x = self.dropout(x)\n    x = self.relu(self.linear2(x))\n    x = self.dropout(x)\n    x = self.relu(self.linear3(x))\n    return x\n```\n\n\n\nto be continued...","source":"_posts/神经网络模型.md","raw":"---\ntitle: 神经网络模型\ndate: 2022-07-26 20:09:08\ntags:\nmathjax: true\n---\n\n# 神经网络模型\n\n## Lenet\n\n最初应用为识别手写数字\n\n早期成功的神经网络\n\n先使用卷积层学习图片的空间信息，池化层(平均)降低图片敏感度，然后使用全连接层来转换到类别空间\n\n![image-20220725190350958](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220725190350958.png)\n\n```python\nclass LeNet(nn.Module):\n  def __init__(self):\n    super(LeNet, self).__init__()\n    self.relu = nn.ReLU()\n    self.pool = nn.AvgPool2d((2,2), (2,2))\n    self.conv1 = nn.Conv2d(1, 6, (5,5), (1,1), (0,0))\n    self.conv2 = nn.Conv2d(6, 16, (5,5), (1,1), (0,0))\n    self.conv3 = nn.Conv2d(16, 120, (5,5), (1,1), (0,0))\n    self.linear1 = nn.Linear(120, 84)\n    self.linear2 = nn.Linear(84, 10)\n\n  def forward(self, x):\n    x = self.relu(self.conv1(x))\n    x = self.pool(x)\n    x = self.relu(self.conv2(x))\n    x = self.pool(x)\n    x = self.relu(self.conv3(x))\n    x = x.reshape(x.shape[0], -1)\n    x = self.relu(self.linear1(x))\n    x = self.linear2(x)\n    return x\n```\n\n## AlexNet\n\nwon 2012 ImageNet\n\n本质上是更深更大的 LeNet\n\n改进了：\n\n- 丢弃法\n- ReLu\n- MaxPooling\n- Overlapping Pooling (stride=2, kernel_size=3)\n\n架构：\n\n![image-20220727185015997](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220727185015997.png)\n\n写代码的时候出现了一个问题：AlexNet 的输入图像的宽高到底是224还是227？因为这影响到卷积层参数的计算。\n\n[网络上的相关讨论](https://datascience.stackexchange.com/questions/29245/what-is-the-input-size-of-alex-net)\n\n这里我写代码的时候暂且按照 227 * 227 * 3 的输入作处理\n\n```python\nclass AlexNet(nn.Module):\n  def __init__(self):\n    super(AlexNet,self).__init__()\n    self.relu = nn.ReLU()\n    self.pool = nn.MaxPool2d((3,3), (2,2))\n    self.dropout = nn.Dropout(0.5)\n    self.flatten = nn.Flatten()\n    self.conv1 = nn.Conv2d(3, 48, (11,11), (4,4), (0,0))\n    self.conv2 = nn.Conv2d(48, 128, (5,5), (1,1), (2,2))\n    self.conv3 = nn.Conv2d(128, 192, (3,3), (1,1), (1,1))\n    self.conv4 = nn.Conv2d(192, 192, (3,3), (1,1), (1,1))\n    self.conv5 = nn.Conv2d(192, 128, (3,3), (1,1), (1,1))\n    self.linear1 = nn.Linear(6*6*128, 2048)\n    self.linear2 = nn.Linear(2048, 2048)\n    self.linear3 = nn.Linear(2048, 1000)\n\n  def forward(self, x):\n    x = self.relu(self.conv1(x))\n    x = self.pool(x)\n    x = self.relu(self.conv2(x))\n    x = self.pool(x)\n    x = self.relu(self.conv3(x))\n    x = self.relu(self.conv4(x))\n    x = self.relu(self.conv5(x))\n    x = self.pool(x)\n\n    x = self.flatten(x)\n    x = self.relu(self.linear1(x))\n    x = self.dropout(x)\n    x = self.relu(self.linear2(x))\n    x = self.dropout(x)\n    x = self.linear3(x)\n    return x\n```\n\n## VGG\n\n![image-20220731073437523](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220731073437523.png)\n\n特点：\n\n- 小卷积核、多卷积子层\n- 小池化核\n- 通道数多\n\nvgg代码(这里简单实现了一个VGG16)\n\n```python\nclass VGG16(nn.Module):\n  def __init__(self):\n    super(VGG16,self).__init__()\n    self.relu = nn.ReLU()\n    self.pool = nn.MaxPool2d((2,2), (2,2))\n    self.dropout = nn.Dropout(0.5)\n    self.flatten = nn.Flatten()\n    self.conv1_1 = nn.Conv2d(3, 64, (3,3), (1,1), (1,1))\n    self.conv1_2 = nn.Conv2d(64, 64, (3,3), (1,1), (1,1))\n    self.conv2_1 = nn.Conv2d(64, 128, (3,3), (1,1), (1,1))\n    self.conv2_2 = nn.Conv2d(128, 128, (3,3), (1,1), (1,1))\n    self.conv3_1 = nn.Conv2d(128, 256, (3,3), (1,1), (1,1))\n    self.conv3_2 = nn.Conv2d(256, 256, (3,3), (1,1), (1,1))\n    self.conv4_1 = nn.Conv2d(256, 512, (3,3), (1,1), (1,1))\n    self.conv4_2 = nn.Conv2d(512, 512, (3,3), (1,1), (1,1))\n    self.linear1 = nn.Linear(7*7*512, 4096)\n    self.linear2 = nn.Linear(4096, 4096)\n    self.linear3 = nn.Linear(4096, 1000)\n\n  def forward(self, x):\n    x = self.relu(self.conv1_1(x))\n    x = self.relu(self.conv1_2(x))\n    x = self.pool(x)\n    x = self.relu(self.conv2_1(x))\n    x = self.relu(self.conv2_2(x))\n    x = self.pool(x)\n    x = self.relu(self.conv3_1(x))\n    x = self.relu(self.conv3_2(x))\n    x = self.relu(self.conv3_2(x))\n    x = self.pool(x)\n    x = self.relu(self.conv4_1(x))\n    x = self.relu(self.conv4_2(x))\n    x = self.relu(self.conv4_2(x))\n    x = self.pool(x)\n    x = self.relu(self.conv4_2(x))\n    x = self.relu(self.conv4_2(x))\n    x = self.relu(self.conv4_2(x))\n    x = self.pool(x)\n\n    x = self.flatten(x)\n    x = self.relu(self.linear1(x))\n    x = self.dropout(x)\n    x = self.relu(self.linear2(x))\n    x = self.dropout(x)\n    x = self.relu(self.linear3(x))\n    return x\n```\n\n\n\nto be continued...","slug":"神经网络模型","published":1,"updated":"2022-08-25T13:13:32.987Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clgyiaru2000ljku74yba2i6p","content":"<h1 id=\"神经网络模型\"><a href=\"#神经网络模型\" class=\"headerlink\" title=\"神经网络模型\"></a>神经网络模型</h1><h2 id=\"Lenet\"><a href=\"#Lenet\" class=\"headerlink\" title=\"Lenet\"></a>Lenet</h2><p>最初应用为识别手写数字</p>\n<p>早期成功的神经网络</p>\n<p>先使用卷积层学习图片的空间信息，池化层(平均)降低图片敏感度，然后使用全连接层来转换到类别空间</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220725190350958.png\" alt=\"image-20220725190350958\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">LeNet</span>(nn.Module):</span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">    <span class=\"built_in\">super</span>(LeNet, self).__init__()</span><br><span class=\"line\">    self.relu = nn.ReLU()</span><br><span class=\"line\">    self.pool = nn.AvgPool2d((<span class=\"number\">2</span>,<span class=\"number\">2</span>), (<span class=\"number\">2</span>,<span class=\"number\">2</span>))</span><br><span class=\"line\">    self.conv1 = nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">6</span>, (<span class=\"number\">5</span>,<span class=\"number\">5</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">0</span>,<span class=\"number\">0</span>))</span><br><span class=\"line\">    self.conv2 = nn.Conv2d(<span class=\"number\">6</span>, <span class=\"number\">16</span>, (<span class=\"number\">5</span>,<span class=\"number\">5</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">0</span>,<span class=\"number\">0</span>))</span><br><span class=\"line\">    self.conv3 = nn.Conv2d(<span class=\"number\">16</span>, <span class=\"number\">120</span>, (<span class=\"number\">5</span>,<span class=\"number\">5</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">0</span>,<span class=\"number\">0</span>))</span><br><span class=\"line\">    self.linear1 = nn.Linear(<span class=\"number\">120</span>, <span class=\"number\">84</span>)</span><br><span class=\"line\">    self.linear2 = nn.Linear(<span class=\"number\">84</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">    x = self.relu(self.conv1(x))</span><br><span class=\"line\">    x = self.pool(x)</span><br><span class=\"line\">    x = self.relu(self.conv2(x))</span><br><span class=\"line\">    x = self.pool(x)</span><br><span class=\"line\">    x = self.relu(self.conv3(x))</span><br><span class=\"line\">    x = x.reshape(x.shape[<span class=\"number\">0</span>], -<span class=\"number\">1</span>)</span><br><span class=\"line\">    x = self.relu(self.linear1(x))</span><br><span class=\"line\">    x = self.linear2(x)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n<h2 id=\"AlexNet\"><a href=\"#AlexNet\" class=\"headerlink\" title=\"AlexNet\"></a>AlexNet</h2><p>won 2012 ImageNet</p>\n<p>本质上是更深更大的 LeNet</p>\n<p>改进了：</p>\n<ul>\n<li>丢弃法</li>\n<li>ReLu</li>\n<li>MaxPooling</li>\n<li>Overlapping Pooling (stride=2, kernel_size=3)</li>\n</ul>\n<p>架构：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220727185015997.png\" alt=\"image-20220727185015997\"></p>\n<p>写代码的时候出现了一个问题：AlexNet 的输入图像的宽高到底是224还是227？因为这影响到卷积层参数的计算。</p>\n<p><a href=\"https://datascience.stackexchange.com/questions/29245/what-is-the-input-size-of-alex-net\">网络上的相关讨论</a></p>\n<p>这里我写代码的时候暂且按照 227 <em> 227 </em> 3 的输入作处理</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">AlexNet</span>(nn.Module):</span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">    <span class=\"built_in\">super</span>(AlexNet,self).__init__()</span><br><span class=\"line\">    self.relu = nn.ReLU()</span><br><span class=\"line\">    self.pool = nn.MaxPool2d((<span class=\"number\">3</span>,<span class=\"number\">3</span>), (<span class=\"number\">2</span>,<span class=\"number\">2</span>))</span><br><span class=\"line\">    self.dropout = nn.Dropout(<span class=\"number\">0.5</span>)</span><br><span class=\"line\">    self.flatten = nn.Flatten()</span><br><span class=\"line\">    self.conv1 = nn.Conv2d(<span class=\"number\">3</span>, <span class=\"number\">48</span>, (<span class=\"number\">11</span>,<span class=\"number\">11</span>), (<span class=\"number\">4</span>,<span class=\"number\">4</span>), (<span class=\"number\">0</span>,<span class=\"number\">0</span>))</span><br><span class=\"line\">    self.conv2 = nn.Conv2d(<span class=\"number\">48</span>, <span class=\"number\">128</span>, (<span class=\"number\">5</span>,<span class=\"number\">5</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">2</span>,<span class=\"number\">2</span>))</span><br><span class=\"line\">    self.conv3 = nn.Conv2d(<span class=\"number\">128</span>, <span class=\"number\">192</span>, (<span class=\"number\">3</span>,<span class=\"number\">3</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">    self.conv4 = nn.Conv2d(<span class=\"number\">192</span>, <span class=\"number\">192</span>, (<span class=\"number\">3</span>,<span class=\"number\">3</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">    self.conv5 = nn.Conv2d(<span class=\"number\">192</span>, <span class=\"number\">128</span>, (<span class=\"number\">3</span>,<span class=\"number\">3</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">    self.linear1 = nn.Linear(<span class=\"number\">6</span>*<span class=\"number\">6</span>*<span class=\"number\">128</span>, <span class=\"number\">2048</span>)</span><br><span class=\"line\">    self.linear2 = nn.Linear(<span class=\"number\">2048</span>, <span class=\"number\">2048</span>)</span><br><span class=\"line\">    self.linear3 = nn.Linear(<span class=\"number\">2048</span>, <span class=\"number\">1000</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">    x = self.relu(self.conv1(x))</span><br><span class=\"line\">    x = self.pool(x)</span><br><span class=\"line\">    x = self.relu(self.conv2(x))</span><br><span class=\"line\">    x = self.pool(x)</span><br><span class=\"line\">    x = self.relu(self.conv3(x))</span><br><span class=\"line\">    x = self.relu(self.conv4(x))</span><br><span class=\"line\">    x = self.relu(self.conv5(x))</span><br><span class=\"line\">    x = self.pool(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    x = self.flatten(x)</span><br><span class=\"line\">    x = self.relu(self.linear1(x))</span><br><span class=\"line\">    x = self.dropout(x)</span><br><span class=\"line\">    x = self.relu(self.linear2(x))</span><br><span class=\"line\">    x = self.dropout(x)</span><br><span class=\"line\">    x = self.linear3(x)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n<h2 id=\"VGG\"><a href=\"#VGG\" class=\"headerlink\" title=\"VGG\"></a>VGG</h2><p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220731073437523.png\" alt=\"image-20220731073437523\"></p>\n<p>特点：</p>\n<ul>\n<li>小卷积核、多卷积子层</li>\n<li>小池化核</li>\n<li>通道数多</li>\n</ul>\n<p>vgg代码(这里简单实现了一个VGG16)</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">VGG16</span>(nn.Module):</span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">    <span class=\"built_in\">super</span>(VGG16,self).__init__()</span><br><span class=\"line\">    self.relu = nn.ReLU()</span><br><span class=\"line\">    self.pool = nn.MaxPool2d((<span class=\"number\">2</span>,<span class=\"number\">2</span>), (<span class=\"number\">2</span>,<span class=\"number\">2</span>))</span><br><span class=\"line\">    self.dropout = nn.Dropout(<span class=\"number\">0.5</span>)</span><br><span class=\"line\">    self.flatten = nn.Flatten()</span><br><span class=\"line\">    self.conv1_1 = nn.Conv2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>, (<span class=\"number\">3</span>,<span class=\"number\">3</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">    self.conv1_2 = nn.Conv2d(<span class=\"number\">64</span>, <span class=\"number\">64</span>, (<span class=\"number\">3</span>,<span class=\"number\">3</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">    self.conv2_1 = nn.Conv2d(<span class=\"number\">64</span>, <span class=\"number\">128</span>, (<span class=\"number\">3</span>,<span class=\"number\">3</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">    self.conv2_2 = nn.Conv2d(<span class=\"number\">128</span>, <span class=\"number\">128</span>, (<span class=\"number\">3</span>,<span class=\"number\">3</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">    self.conv3_1 = nn.Conv2d(<span class=\"number\">128</span>, <span class=\"number\">256</span>, (<span class=\"number\">3</span>,<span class=\"number\">3</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">    self.conv3_2 = nn.Conv2d(<span class=\"number\">256</span>, <span class=\"number\">256</span>, (<span class=\"number\">3</span>,<span class=\"number\">3</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">    self.conv4_1 = nn.Conv2d(<span class=\"number\">256</span>, <span class=\"number\">512</span>, (<span class=\"number\">3</span>,<span class=\"number\">3</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">    self.conv4_2 = nn.Conv2d(<span class=\"number\">512</span>, <span class=\"number\">512</span>, (<span class=\"number\">3</span>,<span class=\"number\">3</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">    self.linear1 = nn.Linear(<span class=\"number\">7</span>*<span class=\"number\">7</span>*<span class=\"number\">512</span>, <span class=\"number\">4096</span>)</span><br><span class=\"line\">    self.linear2 = nn.Linear(<span class=\"number\">4096</span>, <span class=\"number\">4096</span>)</span><br><span class=\"line\">    self.linear3 = nn.Linear(<span class=\"number\">4096</span>, <span class=\"number\">1000</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">    x = self.relu(self.conv1_1(x))</span><br><span class=\"line\">    x = self.relu(self.conv1_2(x))</span><br><span class=\"line\">    x = self.pool(x)</span><br><span class=\"line\">    x = self.relu(self.conv2_1(x))</span><br><span class=\"line\">    x = self.relu(self.conv2_2(x))</span><br><span class=\"line\">    x = self.pool(x)</span><br><span class=\"line\">    x = self.relu(self.conv3_1(x))</span><br><span class=\"line\">    x = self.relu(self.conv3_2(x))</span><br><span class=\"line\">    x = self.relu(self.conv3_2(x))</span><br><span class=\"line\">    x = self.pool(x)</span><br><span class=\"line\">    x = self.relu(self.conv4_1(x))</span><br><span class=\"line\">    x = self.relu(self.conv4_2(x))</span><br><span class=\"line\">    x = self.relu(self.conv4_2(x))</span><br><span class=\"line\">    x = self.pool(x)</span><br><span class=\"line\">    x = self.relu(self.conv4_2(x))</span><br><span class=\"line\">    x = self.relu(self.conv4_2(x))</span><br><span class=\"line\">    x = self.relu(self.conv4_2(x))</span><br><span class=\"line\">    x = self.pool(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    x = self.flatten(x)</span><br><span class=\"line\">    x = self.relu(self.linear1(x))</span><br><span class=\"line\">    x = self.dropout(x)</span><br><span class=\"line\">    x = self.relu(self.linear2(x))</span><br><span class=\"line\">    x = self.dropout(x)</span><br><span class=\"line\">    x = self.relu(self.linear3(x))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n<p>to be continued…</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"神经网络模型\"><a href=\"#神经网络模型\" class=\"headerlink\" title=\"神经网络模型\"></a>神经网络模型</h1><h2 id=\"Lenet\"><a href=\"#Lenet\" class=\"headerlink\" title=\"Lenet\"></a>Lenet</h2><p>最初应用为识别手写数字</p>\n<p>早期成功的神经网络</p>\n<p>先使用卷积层学习图片的空间信息，池化层(平均)降低图片敏感度，然后使用全连接层来转换到类别空间</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220725190350958.png\" alt=\"image-20220725190350958\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">LeNet</span>(nn.Module):</span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">    <span class=\"built_in\">super</span>(LeNet, self).__init__()</span><br><span class=\"line\">    self.relu = nn.ReLU()</span><br><span class=\"line\">    self.pool = nn.AvgPool2d((<span class=\"number\">2</span>,<span class=\"number\">2</span>), (<span class=\"number\">2</span>,<span class=\"number\">2</span>))</span><br><span class=\"line\">    self.conv1 = nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">6</span>, (<span class=\"number\">5</span>,<span class=\"number\">5</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">0</span>,<span class=\"number\">0</span>))</span><br><span class=\"line\">    self.conv2 = nn.Conv2d(<span class=\"number\">6</span>, <span class=\"number\">16</span>, (<span class=\"number\">5</span>,<span class=\"number\">5</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">0</span>,<span class=\"number\">0</span>))</span><br><span class=\"line\">    self.conv3 = nn.Conv2d(<span class=\"number\">16</span>, <span class=\"number\">120</span>, (<span class=\"number\">5</span>,<span class=\"number\">5</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">0</span>,<span class=\"number\">0</span>))</span><br><span class=\"line\">    self.linear1 = nn.Linear(<span class=\"number\">120</span>, <span class=\"number\">84</span>)</span><br><span class=\"line\">    self.linear2 = nn.Linear(<span class=\"number\">84</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">    x = self.relu(self.conv1(x))</span><br><span class=\"line\">    x = self.pool(x)</span><br><span class=\"line\">    x = self.relu(self.conv2(x))</span><br><span class=\"line\">    x = self.pool(x)</span><br><span class=\"line\">    x = self.relu(self.conv3(x))</span><br><span class=\"line\">    x = x.reshape(x.shape[<span class=\"number\">0</span>], -<span class=\"number\">1</span>)</span><br><span class=\"line\">    x = self.relu(self.linear1(x))</span><br><span class=\"line\">    x = self.linear2(x)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n<h2 id=\"AlexNet\"><a href=\"#AlexNet\" class=\"headerlink\" title=\"AlexNet\"></a>AlexNet</h2><p>won 2012 ImageNet</p>\n<p>本质上是更深更大的 LeNet</p>\n<p>改进了：</p>\n<ul>\n<li>丢弃法</li>\n<li>ReLu</li>\n<li>MaxPooling</li>\n<li>Overlapping Pooling (stride=2, kernel_size=3)</li>\n</ul>\n<p>架构：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220727185015997.png\" alt=\"image-20220727185015997\"></p>\n<p>写代码的时候出现了一个问题：AlexNet 的输入图像的宽高到底是224还是227？因为这影响到卷积层参数的计算。</p>\n<p><a href=\"https://datascience.stackexchange.com/questions/29245/what-is-the-input-size-of-alex-net\">网络上的相关讨论</a></p>\n<p>这里我写代码的时候暂且按照 227 <em> 227 </em> 3 的输入作处理</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">AlexNet</span>(nn.Module):</span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">    <span class=\"built_in\">super</span>(AlexNet,self).__init__()</span><br><span class=\"line\">    self.relu = nn.ReLU()</span><br><span class=\"line\">    self.pool = nn.MaxPool2d((<span class=\"number\">3</span>,<span class=\"number\">3</span>), (<span class=\"number\">2</span>,<span class=\"number\">2</span>))</span><br><span class=\"line\">    self.dropout = nn.Dropout(<span class=\"number\">0.5</span>)</span><br><span class=\"line\">    self.flatten = nn.Flatten()</span><br><span class=\"line\">    self.conv1 = nn.Conv2d(<span class=\"number\">3</span>, <span class=\"number\">48</span>, (<span class=\"number\">11</span>,<span class=\"number\">11</span>), (<span class=\"number\">4</span>,<span class=\"number\">4</span>), (<span class=\"number\">0</span>,<span class=\"number\">0</span>))</span><br><span class=\"line\">    self.conv2 = nn.Conv2d(<span class=\"number\">48</span>, <span class=\"number\">128</span>, (<span class=\"number\">5</span>,<span class=\"number\">5</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">2</span>,<span class=\"number\">2</span>))</span><br><span class=\"line\">    self.conv3 = nn.Conv2d(<span class=\"number\">128</span>, <span class=\"number\">192</span>, (<span class=\"number\">3</span>,<span class=\"number\">3</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">    self.conv4 = nn.Conv2d(<span class=\"number\">192</span>, <span class=\"number\">192</span>, (<span class=\"number\">3</span>,<span class=\"number\">3</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">    self.conv5 = nn.Conv2d(<span class=\"number\">192</span>, <span class=\"number\">128</span>, (<span class=\"number\">3</span>,<span class=\"number\">3</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">    self.linear1 = nn.Linear(<span class=\"number\">6</span>*<span class=\"number\">6</span>*<span class=\"number\">128</span>, <span class=\"number\">2048</span>)</span><br><span class=\"line\">    self.linear2 = nn.Linear(<span class=\"number\">2048</span>, <span class=\"number\">2048</span>)</span><br><span class=\"line\">    self.linear3 = nn.Linear(<span class=\"number\">2048</span>, <span class=\"number\">1000</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">    x = self.relu(self.conv1(x))</span><br><span class=\"line\">    x = self.pool(x)</span><br><span class=\"line\">    x = self.relu(self.conv2(x))</span><br><span class=\"line\">    x = self.pool(x)</span><br><span class=\"line\">    x = self.relu(self.conv3(x))</span><br><span class=\"line\">    x = self.relu(self.conv4(x))</span><br><span class=\"line\">    x = self.relu(self.conv5(x))</span><br><span class=\"line\">    x = self.pool(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    x = self.flatten(x)</span><br><span class=\"line\">    x = self.relu(self.linear1(x))</span><br><span class=\"line\">    x = self.dropout(x)</span><br><span class=\"line\">    x = self.relu(self.linear2(x))</span><br><span class=\"line\">    x = self.dropout(x)</span><br><span class=\"line\">    x = self.linear3(x)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n<h2 id=\"VGG\"><a href=\"#VGG\" class=\"headerlink\" title=\"VGG\"></a>VGG</h2><p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220731073437523.png\" alt=\"image-20220731073437523\"></p>\n<p>特点：</p>\n<ul>\n<li>小卷积核、多卷积子层</li>\n<li>小池化核</li>\n<li>通道数多</li>\n</ul>\n<p>vgg代码(这里简单实现了一个VGG16)</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">VGG16</span>(nn.Module):</span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">    <span class=\"built_in\">super</span>(VGG16,self).__init__()</span><br><span class=\"line\">    self.relu = nn.ReLU()</span><br><span class=\"line\">    self.pool = nn.MaxPool2d((<span class=\"number\">2</span>,<span class=\"number\">2</span>), (<span class=\"number\">2</span>,<span class=\"number\">2</span>))</span><br><span class=\"line\">    self.dropout = nn.Dropout(<span class=\"number\">0.5</span>)</span><br><span class=\"line\">    self.flatten = nn.Flatten()</span><br><span class=\"line\">    self.conv1_1 = nn.Conv2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>, (<span class=\"number\">3</span>,<span class=\"number\">3</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">    self.conv1_2 = nn.Conv2d(<span class=\"number\">64</span>, <span class=\"number\">64</span>, (<span class=\"number\">3</span>,<span class=\"number\">3</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">    self.conv2_1 = nn.Conv2d(<span class=\"number\">64</span>, <span class=\"number\">128</span>, (<span class=\"number\">3</span>,<span class=\"number\">3</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">    self.conv2_2 = nn.Conv2d(<span class=\"number\">128</span>, <span class=\"number\">128</span>, (<span class=\"number\">3</span>,<span class=\"number\">3</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">    self.conv3_1 = nn.Conv2d(<span class=\"number\">128</span>, <span class=\"number\">256</span>, (<span class=\"number\">3</span>,<span class=\"number\">3</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">    self.conv3_2 = nn.Conv2d(<span class=\"number\">256</span>, <span class=\"number\">256</span>, (<span class=\"number\">3</span>,<span class=\"number\">3</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">    self.conv4_1 = nn.Conv2d(<span class=\"number\">256</span>, <span class=\"number\">512</span>, (<span class=\"number\">3</span>,<span class=\"number\">3</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">    self.conv4_2 = nn.Conv2d(<span class=\"number\">512</span>, <span class=\"number\">512</span>, (<span class=\"number\">3</span>,<span class=\"number\">3</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">1</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">    self.linear1 = nn.Linear(<span class=\"number\">7</span>*<span class=\"number\">7</span>*<span class=\"number\">512</span>, <span class=\"number\">4096</span>)</span><br><span class=\"line\">    self.linear2 = nn.Linear(<span class=\"number\">4096</span>, <span class=\"number\">4096</span>)</span><br><span class=\"line\">    self.linear3 = nn.Linear(<span class=\"number\">4096</span>, <span class=\"number\">1000</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">    x = self.relu(self.conv1_1(x))</span><br><span class=\"line\">    x = self.relu(self.conv1_2(x))</span><br><span class=\"line\">    x = self.pool(x)</span><br><span class=\"line\">    x = self.relu(self.conv2_1(x))</span><br><span class=\"line\">    x = self.relu(self.conv2_2(x))</span><br><span class=\"line\">    x = self.pool(x)</span><br><span class=\"line\">    x = self.relu(self.conv3_1(x))</span><br><span class=\"line\">    x = self.relu(self.conv3_2(x))</span><br><span class=\"line\">    x = self.relu(self.conv3_2(x))</span><br><span class=\"line\">    x = self.pool(x)</span><br><span class=\"line\">    x = self.relu(self.conv4_1(x))</span><br><span class=\"line\">    x = self.relu(self.conv4_2(x))</span><br><span class=\"line\">    x = self.relu(self.conv4_2(x))</span><br><span class=\"line\">    x = self.pool(x)</span><br><span class=\"line\">    x = self.relu(self.conv4_2(x))</span><br><span class=\"line\">    x = self.relu(self.conv4_2(x))</span><br><span class=\"line\">    x = self.relu(self.conv4_2(x))</span><br><span class=\"line\">    x = self.pool(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    x = self.flatten(x)</span><br><span class=\"line\">    x = self.relu(self.linear1(x))</span><br><span class=\"line\">    x = self.dropout(x)</span><br><span class=\"line\">    x = self.relu(self.linear2(x))</span><br><span class=\"line\">    x = self.dropout(x)</span><br><span class=\"line\">    x = self.relu(self.linear3(x))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n<p>to be continued…</p>\n"},{"title":"计算社会学课程 Project","date":"2022-01-19T07:25:08.000Z","_content":"\n课堂作业：对班里同学之间的社交关系进行分析\n\n## 使用的软件\n\n> Gephi 是一款网络分析领域的数据可视化软件\n>\n> 官方网站 - [Gephi](https://gephi.org/)\n\n![image-20220119091559013](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20220119091559013.png)\n\n## 流程\n\n![img](./img/flow.drawio.svg)\n\n## 数据处理与导入\n\n*仅单纯导入了班级内各同学之间认识的关系，并没有将个人其他信息加入*\n\n简单拉出关系表转换为`.csv`\n\n![image-20220119101906964](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20220119101906964.png)\n\n使用 `python` 建立**节点表格**与**边表格**，方便导入 `Gephi` 软件。\n\n![image-20220119102109357](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20220119102109357.png)\n\n## 报告\n\n### 导入时初始网络图\n\n![Snipaste_2022-01-19_10-07-44](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/Snipaste_2022-01-19_10-07-44.png)\n\n*使用的布局方式：Force Atlas，斥力强度 200.0*\n\n### 依据节点的度对节点上色与改变大小\n\n![](./degree.svg)\n\n\n\n*使用的布局方式：Force Atlas，斥力强度 2000.0，部分距离较远节点经过手动调整位置。*\n\n### 聚合系数 Cluster coefficient\n\n聚合系数是网络的局部特征，反映了相邻两个人之间朋友圈子的重合度，即该节点的朋友之间也是朋友的程度。\n\n![image-20220119094510712](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20220119094510712.png)\n\n使用`Gephi`计算该网络的**平均聚合系数**为 0.587\n\n### 平均度数 Avg degree\n\n使用`Gephi`计算该网络的**平均度数**为 4.119\n\n度数分布图：\n\n![degree-distribution](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/degree-distribution.png)\n\n### 平均路径长度(特征路径长度) Avg path length\n\n使用`Gephi`计算该网络的平均路径长度为 2.379\n\n网络直径：5\n\n体现**小世界现象**\n\n### 模块化 Modularity\n\n使用`Gephi`计算该网络的**模块化程度**为 0.414\n\n模块化程度：\n\n![](./img/m.jpg)\n\n利用模块化的计算，可以进行社区发现 ( Community Detection )\n\n![image-20220119103903270](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20220119103903270.png)\n\n社区发现可视化结果：\n\n![](./module.svg)\n\n与其它同学发现的对比：\n\n![image-20220119105008234](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20220119105008234.png)\n\n## 附\n\n`python` 源代码\n\n```python\nimport csv\n\nheader_namelist = ['Id', 'Label']\nheader_relation = ['Source', 'Target']\n\n# generate list\nwith open('namelist.csv', 'w', newline='', encoding='utf-8') as nf:\n    nfer = csv.writer(nf)\n    nfer.writerow(header_namelist)\n    with open('res.csv', encoding='utf-8') as f:\n        f_csv = csv.reader(f)\n        next(f_csv, None)\n        for row in f_csv:\n            nfer.writerow([row[0], row[0]])\n\n# generate relation\nwith open('relation.csv', 'w', newline='', encoding='utf-8') as nf:\n    nfer = csv.writer(nf)\n    nfer.writerow(header_relation)\n    with open('res.csv', encoding='utf-8') as f:\n        f_csv = csv.reader(f)\n        next(f_csv, None)\n        for row in f_csv:\n            source = row[0]\n            for target in row[1 : -1]:\n                if target:\n                    nfer.writerow([source, target])\n```\n\n","source":"_posts/计算社会学 project.md","raw":"---\ntitle: 计算社会学课程 Project\ndate: 2022-01-19 15:25:08\ntags:\n---\n\n课堂作业：对班里同学之间的社交关系进行分析\n\n## 使用的软件\n\n> Gephi 是一款网络分析领域的数据可视化软件\n>\n> 官方网站 - [Gephi](https://gephi.org/)\n\n![image-20220119091559013](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20220119091559013.png)\n\n## 流程\n\n![img](./img/flow.drawio.svg)\n\n## 数据处理与导入\n\n*仅单纯导入了班级内各同学之间认识的关系，并没有将个人其他信息加入*\n\n简单拉出关系表转换为`.csv`\n\n![image-20220119101906964](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20220119101906964.png)\n\n使用 `python` 建立**节点表格**与**边表格**，方便导入 `Gephi` 软件。\n\n![image-20220119102109357](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20220119102109357.png)\n\n## 报告\n\n### 导入时初始网络图\n\n![Snipaste_2022-01-19_10-07-44](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/Snipaste_2022-01-19_10-07-44.png)\n\n*使用的布局方式：Force Atlas，斥力强度 200.0*\n\n### 依据节点的度对节点上色与改变大小\n\n![](./degree.svg)\n\n\n\n*使用的布局方式：Force Atlas，斥力强度 2000.0，部分距离较远节点经过手动调整位置。*\n\n### 聚合系数 Cluster coefficient\n\n聚合系数是网络的局部特征，反映了相邻两个人之间朋友圈子的重合度，即该节点的朋友之间也是朋友的程度。\n\n![image-20220119094510712](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20220119094510712.png)\n\n使用`Gephi`计算该网络的**平均聚合系数**为 0.587\n\n### 平均度数 Avg degree\n\n使用`Gephi`计算该网络的**平均度数**为 4.119\n\n度数分布图：\n\n![degree-distribution](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/degree-distribution.png)\n\n### 平均路径长度(特征路径长度) Avg path length\n\n使用`Gephi`计算该网络的平均路径长度为 2.379\n\n网络直径：5\n\n体现**小世界现象**\n\n### 模块化 Modularity\n\n使用`Gephi`计算该网络的**模块化程度**为 0.414\n\n模块化程度：\n\n![](./img/m.jpg)\n\n利用模块化的计算，可以进行社区发现 ( Community Detection )\n\n![image-20220119103903270](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20220119103903270.png)\n\n社区发现可视化结果：\n\n![](./module.svg)\n\n与其它同学发现的对比：\n\n![image-20220119105008234](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20220119105008234.png)\n\n## 附\n\n`python` 源代码\n\n```python\nimport csv\n\nheader_namelist = ['Id', 'Label']\nheader_relation = ['Source', 'Target']\n\n# generate list\nwith open('namelist.csv', 'w', newline='', encoding='utf-8') as nf:\n    nfer = csv.writer(nf)\n    nfer.writerow(header_namelist)\n    with open('res.csv', encoding='utf-8') as f:\n        f_csv = csv.reader(f)\n        next(f_csv, None)\n        for row in f_csv:\n            nfer.writerow([row[0], row[0]])\n\n# generate relation\nwith open('relation.csv', 'w', newline='', encoding='utf-8') as nf:\n    nfer = csv.writer(nf)\n    nfer.writerow(header_relation)\n    with open('res.csv', encoding='utf-8') as f:\n        f_csv = csv.reader(f)\n        next(f_csv, None)\n        for row in f_csv:\n            source = row[0]\n            for target in row[1 : -1]:\n                if target:\n                    nfer.writerow([source, target])\n```\n\n","slug":"计算社会学 project","published":1,"updated":"2022-08-25T13:13:32.988Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clgyiaru2000mjku789qq5pe0","content":"<p>课堂作业：对班里同学之间的社交关系进行分析</p>\n<h2 id=\"使用的软件\"><a href=\"#使用的软件\" class=\"headerlink\" title=\"使用的软件\"></a>使用的软件</h2><blockquote>\n<p>Gephi 是一款网络分析领域的数据可视化软件</p>\n<p>官方网站 - <a href=\"https://gephi.org/\">Gephi</a></p>\n</blockquote>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20220119091559013.png\" alt=\"image-20220119091559013\"></p>\n<h2 id=\"流程\"><a href=\"#流程\" class=\"headerlink\" title=\"流程\"></a>流程</h2><p><img src=\"./img/flow.drawio.svg\" alt=\"img\"></p>\n<h2 id=\"数据处理与导入\"><a href=\"#数据处理与导入\" class=\"headerlink\" title=\"数据处理与导入\"></a>数据处理与导入</h2><p><em>仅单纯导入了班级内各同学之间认识的关系，并没有将个人其他信息加入</em></p>\n<p>简单拉出关系表转换为<code>.csv</code></p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20220119101906964.png\" alt=\"image-20220119101906964\"></p>\n<p>使用 <code>python</code> 建立<strong>节点表格</strong>与<strong>边表格</strong>，方便导入 <code>Gephi</code> 软件。</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20220119102109357.png\" alt=\"image-20220119102109357\"></p>\n<h2 id=\"报告\"><a href=\"#报告\" class=\"headerlink\" title=\"报告\"></a>报告</h2><h3 id=\"导入时初始网络图\"><a href=\"#导入时初始网络图\" class=\"headerlink\" title=\"导入时初始网络图\"></a>导入时初始网络图</h3><p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/Snipaste_2022-01-19_10-07-44.png\" alt=\"Snipaste_2022-01-19_10-07-44\"></p>\n<p><em>使用的布局方式：Force Atlas，斥力强度 200.0</em></p>\n<h3 id=\"依据节点的度对节点上色与改变大小\"><a href=\"#依据节点的度对节点上色与改变大小\" class=\"headerlink\" title=\"依据节点的度对节点上色与改变大小\"></a>依据节点的度对节点上色与改变大小</h3><p><img src=\"./degree.svg\" alt=\"\"></p>\n<p><em>使用的布局方式：Force Atlas，斥力强度 2000.0，部分距离较远节点经过手动调整位置。</em></p>\n<h3 id=\"聚合系数-Cluster-coefficient\"><a href=\"#聚合系数-Cluster-coefficient\" class=\"headerlink\" title=\"聚合系数 Cluster coefficient\"></a>聚合系数 Cluster coefficient</h3><p>聚合系数是网络的局部特征，反映了相邻两个人之间朋友圈子的重合度，即该节点的朋友之间也是朋友的程度。</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20220119094510712.png\" alt=\"image-20220119094510712\"></p>\n<p>使用<code>Gephi</code>计算该网络的<strong>平均聚合系数</strong>为 0.587</p>\n<h3 id=\"平均度数-Avg-degree\"><a href=\"#平均度数-Avg-degree\" class=\"headerlink\" title=\"平均度数 Avg degree\"></a>平均度数 Avg degree</h3><p>使用<code>Gephi</code>计算该网络的<strong>平均度数</strong>为 4.119</p>\n<p>度数分布图：</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/degree-distribution.png\" alt=\"degree-distribution\"></p>\n<h3 id=\"平均路径长度-特征路径长度-Avg-path-length\"><a href=\"#平均路径长度-特征路径长度-Avg-path-length\" class=\"headerlink\" title=\"平均路径长度(特征路径长度) Avg path length\"></a>平均路径长度(特征路径长度) Avg path length</h3><p>使用<code>Gephi</code>计算该网络的平均路径长度为 2.379</p>\n<p>网络直径：5</p>\n<p>体现<strong>小世界现象</strong></p>\n<h3 id=\"模块化-Modularity\"><a href=\"#模块化-Modularity\" class=\"headerlink\" title=\"模块化 Modularity\"></a>模块化 Modularity</h3><p>使用<code>Gephi</code>计算该网络的<strong>模块化程度</strong>为 0.414</p>\n<p>模块化程度：</p>\n<p><img src=\"./img/m.jpg\" alt=\"\"></p>\n<p>利用模块化的计算，可以进行社区发现 ( Community Detection )</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20220119103903270.png\" alt=\"image-20220119103903270\"></p>\n<p>社区发现可视化结果：</p>\n<p><img src=\"./module.svg\" alt=\"\"></p>\n<p>与其它同学发现的对比：</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20220119105008234.png\" alt=\"image-20220119105008234\"></p>\n<h2 id=\"附\"><a href=\"#附\" class=\"headerlink\" title=\"附\"></a>附</h2><p><code>python</code> 源代码</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> csv</span><br><span class=\"line\"></span><br><span class=\"line\">header_namelist = [<span class=\"string\">&#x27;Id&#x27;</span>, <span class=\"string\">&#x27;Label&#x27;</span>]</span><br><span class=\"line\">header_relation = [<span class=\"string\">&#x27;Source&#x27;</span>, <span class=\"string\">&#x27;Target&#x27;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># generate list</span></span><br><span class=\"line\"><span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;namelist.csv&#x27;</span>, <span class=\"string\">&#x27;w&#x27;</span>, newline=<span class=\"string\">&#x27;&#x27;</span>, encoding=<span class=\"string\">&#x27;utf-8&#x27;</span>) <span class=\"keyword\">as</span> nf:</span><br><span class=\"line\">    nfer = csv.writer(nf)</span><br><span class=\"line\">    nfer.writerow(header_namelist)</span><br><span class=\"line\">    <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;res.csv&#x27;</span>, encoding=<span class=\"string\">&#x27;utf-8&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">        f_csv = csv.reader(f)</span><br><span class=\"line\">        <span class=\"built_in\">next</span>(f_csv, <span class=\"literal\">None</span>)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> row <span class=\"keyword\">in</span> f_csv:</span><br><span class=\"line\">            nfer.writerow([row[<span class=\"number\">0</span>], row[<span class=\"number\">0</span>]])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># generate relation</span></span><br><span class=\"line\"><span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;relation.csv&#x27;</span>, <span class=\"string\">&#x27;w&#x27;</span>, newline=<span class=\"string\">&#x27;&#x27;</span>, encoding=<span class=\"string\">&#x27;utf-8&#x27;</span>) <span class=\"keyword\">as</span> nf:</span><br><span class=\"line\">    nfer = csv.writer(nf)</span><br><span class=\"line\">    nfer.writerow(header_relation)</span><br><span class=\"line\">    <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;res.csv&#x27;</span>, encoding=<span class=\"string\">&#x27;utf-8&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">        f_csv = csv.reader(f)</span><br><span class=\"line\">        <span class=\"built_in\">next</span>(f_csv, <span class=\"literal\">None</span>)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> row <span class=\"keyword\">in</span> f_csv:</span><br><span class=\"line\">            source = row[<span class=\"number\">0</span>]</span><br><span class=\"line\">            <span class=\"keyword\">for</span> target <span class=\"keyword\">in</span> row[<span class=\"number\">1</span> : -<span class=\"number\">1</span>]:</span><br><span class=\"line\">                <span class=\"keyword\">if</span> target:</span><br><span class=\"line\">                    nfer.writerow([source, target])</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<p>课堂作业：对班里同学之间的社交关系进行分析</p>\n<h2 id=\"使用的软件\"><a href=\"#使用的软件\" class=\"headerlink\" title=\"使用的软件\"></a>使用的软件</h2><blockquote>\n<p>Gephi 是一款网络分析领域的数据可视化软件</p>\n<p>官方网站 - <a href=\"https://gephi.org/\">Gephi</a></p>\n</blockquote>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20220119091559013.png\" alt=\"image-20220119091559013\"></p>\n<h2 id=\"流程\"><a href=\"#流程\" class=\"headerlink\" title=\"流程\"></a>流程</h2><p><img src=\"./img/flow.drawio.svg\" alt=\"img\"></p>\n<h2 id=\"数据处理与导入\"><a href=\"#数据处理与导入\" class=\"headerlink\" title=\"数据处理与导入\"></a>数据处理与导入</h2><p><em>仅单纯导入了班级内各同学之间认识的关系，并没有将个人其他信息加入</em></p>\n<p>简单拉出关系表转换为<code>.csv</code></p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20220119101906964.png\" alt=\"image-20220119101906964\"></p>\n<p>使用 <code>python</code> 建立<strong>节点表格</strong>与<strong>边表格</strong>，方便导入 <code>Gephi</code> 软件。</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20220119102109357.png\" alt=\"image-20220119102109357\"></p>\n<h2 id=\"报告\"><a href=\"#报告\" class=\"headerlink\" title=\"报告\"></a>报告</h2><h3 id=\"导入时初始网络图\"><a href=\"#导入时初始网络图\" class=\"headerlink\" title=\"导入时初始网络图\"></a>导入时初始网络图</h3><p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/Snipaste_2022-01-19_10-07-44.png\" alt=\"Snipaste_2022-01-19_10-07-44\"></p>\n<p><em>使用的布局方式：Force Atlas，斥力强度 200.0</em></p>\n<h3 id=\"依据节点的度对节点上色与改变大小\"><a href=\"#依据节点的度对节点上色与改变大小\" class=\"headerlink\" title=\"依据节点的度对节点上色与改变大小\"></a>依据节点的度对节点上色与改变大小</h3><p><img src=\"./degree.svg\" alt=\"\"></p>\n<p><em>使用的布局方式：Force Atlas，斥力强度 2000.0，部分距离较远节点经过手动调整位置。</em></p>\n<h3 id=\"聚合系数-Cluster-coefficient\"><a href=\"#聚合系数-Cluster-coefficient\" class=\"headerlink\" title=\"聚合系数 Cluster coefficient\"></a>聚合系数 Cluster coefficient</h3><p>聚合系数是网络的局部特征，反映了相邻两个人之间朋友圈子的重合度，即该节点的朋友之间也是朋友的程度。</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20220119094510712.png\" alt=\"image-20220119094510712\"></p>\n<p>使用<code>Gephi</code>计算该网络的<strong>平均聚合系数</strong>为 0.587</p>\n<h3 id=\"平均度数-Avg-degree\"><a href=\"#平均度数-Avg-degree\" class=\"headerlink\" title=\"平均度数 Avg degree\"></a>平均度数 Avg degree</h3><p>使用<code>Gephi</code>计算该网络的<strong>平均度数</strong>为 4.119</p>\n<p>度数分布图：</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/degree-distribution.png\" alt=\"degree-distribution\"></p>\n<h3 id=\"平均路径长度-特征路径长度-Avg-path-length\"><a href=\"#平均路径长度-特征路径长度-Avg-path-length\" class=\"headerlink\" title=\"平均路径长度(特征路径长度) Avg path length\"></a>平均路径长度(特征路径长度) Avg path length</h3><p>使用<code>Gephi</code>计算该网络的平均路径长度为 2.379</p>\n<p>网络直径：5</p>\n<p>体现<strong>小世界现象</strong></p>\n<h3 id=\"模块化-Modularity\"><a href=\"#模块化-Modularity\" class=\"headerlink\" title=\"模块化 Modularity\"></a>模块化 Modularity</h3><p>使用<code>Gephi</code>计算该网络的<strong>模块化程度</strong>为 0.414</p>\n<p>模块化程度：</p>\n<p><img src=\"./img/m.jpg\" alt=\"\"></p>\n<p>利用模块化的计算，可以进行社区发现 ( Community Detection )</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20220119103903270.png\" alt=\"image-20220119103903270\"></p>\n<p>社区发现可视化结果：</p>\n<p><img src=\"./module.svg\" alt=\"\"></p>\n<p>与其它同学发现的对比：</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20220119105008234.png\" alt=\"image-20220119105008234\"></p>\n<h2 id=\"附\"><a href=\"#附\" class=\"headerlink\" title=\"附\"></a>附</h2><p><code>python</code> 源代码</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> csv</span><br><span class=\"line\"></span><br><span class=\"line\">header_namelist = [<span class=\"string\">&#x27;Id&#x27;</span>, <span class=\"string\">&#x27;Label&#x27;</span>]</span><br><span class=\"line\">header_relation = [<span class=\"string\">&#x27;Source&#x27;</span>, <span class=\"string\">&#x27;Target&#x27;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># generate list</span></span><br><span class=\"line\"><span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;namelist.csv&#x27;</span>, <span class=\"string\">&#x27;w&#x27;</span>, newline=<span class=\"string\">&#x27;&#x27;</span>, encoding=<span class=\"string\">&#x27;utf-8&#x27;</span>) <span class=\"keyword\">as</span> nf:</span><br><span class=\"line\">    nfer = csv.writer(nf)</span><br><span class=\"line\">    nfer.writerow(header_namelist)</span><br><span class=\"line\">    <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;res.csv&#x27;</span>, encoding=<span class=\"string\">&#x27;utf-8&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">        f_csv = csv.reader(f)</span><br><span class=\"line\">        <span class=\"built_in\">next</span>(f_csv, <span class=\"literal\">None</span>)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> row <span class=\"keyword\">in</span> f_csv:</span><br><span class=\"line\">            nfer.writerow([row[<span class=\"number\">0</span>], row[<span class=\"number\">0</span>]])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># generate relation</span></span><br><span class=\"line\"><span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;relation.csv&#x27;</span>, <span class=\"string\">&#x27;w&#x27;</span>, newline=<span class=\"string\">&#x27;&#x27;</span>, encoding=<span class=\"string\">&#x27;utf-8&#x27;</span>) <span class=\"keyword\">as</span> nf:</span><br><span class=\"line\">    nfer = csv.writer(nf)</span><br><span class=\"line\">    nfer.writerow(header_relation)</span><br><span class=\"line\">    <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;res.csv&#x27;</span>, encoding=<span class=\"string\">&#x27;utf-8&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">        f_csv = csv.reader(f)</span><br><span class=\"line\">        <span class=\"built_in\">next</span>(f_csv, <span class=\"literal\">None</span>)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> row <span class=\"keyword\">in</span> f_csv:</span><br><span class=\"line\">            source = row[<span class=\"number\">0</span>]</span><br><span class=\"line\">            <span class=\"keyword\">for</span> target <span class=\"keyword\">in</span> row[<span class=\"number\">1</span> : -<span class=\"number\">1</span>]:</span><br><span class=\"line\">                <span class=\"keyword\">if</span> target:</span><br><span class=\"line\">                    nfer.writerow([source, target])</span><br></pre></td></tr></table></figure>\n"},{"title":"自动文本摘要 (TF-ISF)","date":"2022-07-10T04:00:06.000Z","mathjax":true,"_content":"\n[项目开源地址](https://github.com/1099255210/Auto-Text-Summarization-Based-on-TF-ISF)\n\n## 目的\n\n对于 `DUC2004` 数据集的 50 个 topic 生成文本摘要，并将生成的摘要与 baseline，共同和专家组的摘要做 Rouge 评分。\n\n数据集地址：(https://www-nlpir.nist.gov/projects/duc/data/2004_data.html)\n\n## 预处理数据\n\n预处理数据分为以下几步：\n\n### 句子切分\n\n句子切分一开始我使用的是用 `.` 即英文的句号进行分割，但是后来在检查结果的时候发现有些单词比如 `U. N.` 会被误认为是多个句子。于是我借用了 `nltk` 库的 `tokenize` 函数进行句子的分割，有效避免了这个问题，当然也同时避免了无法区分以非英文句号为结尾的句子的情况。\n\n```python\ntokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\nfilestat.sentenselist = tokenizer.tokenize(file.content)\n```\n\n### 去除停用词\n\n句子切分完之后，就可以以空格为分割符提取句子中的每个单词。提取的时候要注意的地方是：单词前后可能有特殊符号需要去除；单词可以直接统一转为小写；单词中的数字也要去除。这里用正则表达式对单词处理。\n\n```python\nword = re.sub(\"[^A-Za-z]+\", ' ', str(word)).lower().strip()\n```\n\n得到每一个单词之后，需要去除停用词，比如 `a, the` 等等。\n\n停用词去除的工作也是调用 `nltk` 库中的 `stopwords` ，只要检查单词是否在停用词中然后省略即可。\n\n```python\nfrom nltk.corpus import stopwords\nif word in stopwords.words('english'):\n    # Get rid of this word.\n```\n\n### 单词词干化（https://tartarus.org/martin/PorterStemmer/）\n\n 这一步课件里给出了一个停用词去除的库 `PorterStemmer` 网站上直接可以下载到源码。保存至 `porterstemming.py` 之后实例化一个 `PorterStemmer` 对象，之后进行单词词干化。\n\n```python\nimport porterstemming\nPS = porterstemming.PorterStemmer()\nword = PS.stem(word, 0, len(word) - 1)\n```\n\n对每一个句子进行如上操作之后，将句子与句子中经过处理的单词序列存入一个数据结构中待使用；每篇文章里的所有句子也存入一个数据结构中，用于下一步分析单词与句子的统计特性。\n\n## DUC 文本数据转化为数值型数据\n\n这里课件里引入 `TF-IDF` 的概念：\n\n> $词频(TF) = \\frac{某个词在文章中的出现次数}{文章的总词数}$\n>\n> $逆文档频率(IDF)=log(\\frac{语料库的文档总数}{包含该词的文档数+1})$\n>\n> $TF-IDF=词频(TF)×逆文档频率(IDF)$\n\n但是要用这样的概念生成 句子-词语 矩阵，似乎不可行，因为这其中没有句子的概念。这里可以采用另一个概念 `TF-ISF` 来生成矩阵：\n\n> $词频(TF) = \\frac{某个词在句子中的出现次数}{句子的总词数}$\n>\n> $逆文档频率(ISF)=log(\\frac{语料库的句子总数}{包含该词的句子数+1})$\n>\n> $TF-ISF=词频(TF)×逆文档频率(ISF)$\n\n通过这样的公式，对每一个 `topic` 中的句子与单词分别计算 句子-词语 矩阵。\n\n为了方便后期的处理，在矩阵的最后一列加入一行 同一个主题里所有句子拼接起来的一个长句子。\n\n## 计算相似度\n\n生成的矩阵现在可以用于计算句子与句子间的相似度。我们将所有的句子与最后的长句子（所有句子拼接而成的句子）进行余弦相似度的计算，公式如下：\n\n$\\cos\\theta=\\frac{\\sum_{i=1}^n(A_i×B_i)}{\\sqrt{\\sum_{i=1}^n(A_i)^2}×{\\sqrt{\\sum_{i=1}^n(B_i)^2}}} \\\\ = \\frac{A\\cdot{B}}{\\mid{A}\\mid×\\mid{B}\\mid}$\n\n定义函数 `calSimilarity(arr1:numpy.ndarray, arr2:numpy.ndarray)` 输入两个向量计算相似度。\n\n```python\n# Similarity Calculation\ndef calSimilarity(arr1:numpy.ndarray, arr2:numpy.ndarray):\n  return numpy.dot(arr1, arr2) / (numpy.linalg.norm(arr1) * numpy.linalg.norm(arr2))\n```\n\n得到每个句子对最后的长句子的相似度之后进行排序。\n\n```python\nsimvaluedict = sorted(simvaluedict.items(), key=lambda item: item[1], reverse=True)\n```\n\n最终得到一个按照相似度数值高低排序的矩阵，每行代表一个句子以及其相似度值。\n\n## 冗余控制与摘要生成\n\n上一步得到的矩阵中，我们选取相似度值最高的句子放入最终的摘要中，然后选取第二句，与第一句作相似度计算，如果高于某个阈值，则丢弃，直到文本的大小达到 665byte 为止。这里的阈值我经过试验之后设为了 0.4，实验中如果阈值过高很可能导致摘要长度还未达到规定大小就生成结束，最终导致摘要过短。\n\n至此，每个 `topic` 的摘要内容生成完毕。\n\n## 摘要性能评测\n\n对于生成的摘要，我将其与数据集中专家给出的几份摘要都作 ROUGE 的计算，得到的最高分计入最终结果。这里的 ROUGE 评分主要有三项：ROUGE-1, ROUGE-2, ROUGE-l，着重关注的是回归率 r。\n\n为了作对比，还需要提供一个 `Baseline` 结果，这里对于每一个 `topic` 里的每一篇文章选取第一句话拼接为一个摘要，长度同样为 665byte，再对比专家组结果作 ROUGE 评分。\n\n将两者进行对比，绘制直方图：\n\n![](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/rouge-1.png)\n\n![](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/rouge-2.png)\n\n![](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/rouge-l.png)\n\n\n\n红色部分为算法生成的结果，蓝色部分为基线结果，可以看到在大部分 `topic` 中得到的效果要高于或等于基线效果(33/50)。\n\n## 关于算法的改进\n\n改进方法中我选择了改进方法2进行实验。\n\n调查了一下 `sentence2vec` 的方法，发现首先需要有个词向量的数据 (word2vec)。网络上找到的方法是使用 `gensim` 库对语料库进行训练，得到 `word2vec` 的数据之后再计算句向量，剩下的内容就和之前差不多了，也是计算相似度之后排序生成摘要。\n\n我尝试用原始的文档训练词向量，然后计算句向量，但是得到的结果有很大的问题。问题主要在于使用原始文档训练出来的词向量计算出的句向量，这些向量之间的相似度极高，基本都高达 99% 以上，对这样的相似度数据进行排序显然没有任何意义。\n\n于是我找到了一个预训练的数据集 `GloVe: Global Vectors for Word Representation`，选取了其中一个 50 维的词向量数据集作为参考。这里就注意不需要对每个词进行词干化了，否则在词向量数据中是找不到对应向量的，默认会置0，也就是没有意义的。\n\n`sentence2vec` 句向量的计算参照以下公式：\n\n$v_s\\leftarrow{\\frac{1}{\\mid s \\mid}}\\sum_{w\\in s}{\\frac{a}{a+p(w)}v_w}$\n\n算出句向量之后就可以像之前一样进行相似度计算排序然后生成了。\n\n但是令人意外的是这样生成的摘要得到的效果反而不如之前的算法生成的效果，结果如下：\n\n![](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/srouge-1.png)\n\n![](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/srouge-2.png)\n\n![](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/srouge-l.png)\n\n\n但是结果确实是如此，我检查代码也没有发现有什么明显的问题。可能这个词向量的数据集对于该任务来说并不适合吧。","source":"_posts/自动文本摘要(TF-ISF).md","raw":"---\ntitle: 自动文本摘要 (TF-ISF)\ndate: 2022-07-10 12:00:06\ntags:\nmathjax: true\n---\n\n[项目开源地址](https://github.com/1099255210/Auto-Text-Summarization-Based-on-TF-ISF)\n\n## 目的\n\n对于 `DUC2004` 数据集的 50 个 topic 生成文本摘要，并将生成的摘要与 baseline，共同和专家组的摘要做 Rouge 评分。\n\n数据集地址：(https://www-nlpir.nist.gov/projects/duc/data/2004_data.html)\n\n## 预处理数据\n\n预处理数据分为以下几步：\n\n### 句子切分\n\n句子切分一开始我使用的是用 `.` 即英文的句号进行分割，但是后来在检查结果的时候发现有些单词比如 `U. N.` 会被误认为是多个句子。于是我借用了 `nltk` 库的 `tokenize` 函数进行句子的分割，有效避免了这个问题，当然也同时避免了无法区分以非英文句号为结尾的句子的情况。\n\n```python\ntokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\nfilestat.sentenselist = tokenizer.tokenize(file.content)\n```\n\n### 去除停用词\n\n句子切分完之后，就可以以空格为分割符提取句子中的每个单词。提取的时候要注意的地方是：单词前后可能有特殊符号需要去除；单词可以直接统一转为小写；单词中的数字也要去除。这里用正则表达式对单词处理。\n\n```python\nword = re.sub(\"[^A-Za-z]+\", ' ', str(word)).lower().strip()\n```\n\n得到每一个单词之后，需要去除停用词，比如 `a, the` 等等。\n\n停用词去除的工作也是调用 `nltk` 库中的 `stopwords` ，只要检查单词是否在停用词中然后省略即可。\n\n```python\nfrom nltk.corpus import stopwords\nif word in stopwords.words('english'):\n    # Get rid of this word.\n```\n\n### 单词词干化（https://tartarus.org/martin/PorterStemmer/）\n\n 这一步课件里给出了一个停用词去除的库 `PorterStemmer` 网站上直接可以下载到源码。保存至 `porterstemming.py` 之后实例化一个 `PorterStemmer` 对象，之后进行单词词干化。\n\n```python\nimport porterstemming\nPS = porterstemming.PorterStemmer()\nword = PS.stem(word, 0, len(word) - 1)\n```\n\n对每一个句子进行如上操作之后，将句子与句子中经过处理的单词序列存入一个数据结构中待使用；每篇文章里的所有句子也存入一个数据结构中，用于下一步分析单词与句子的统计特性。\n\n## DUC 文本数据转化为数值型数据\n\n这里课件里引入 `TF-IDF` 的概念：\n\n> $词频(TF) = \\frac{某个词在文章中的出现次数}{文章的总词数}$\n>\n> $逆文档频率(IDF)=log(\\frac{语料库的文档总数}{包含该词的文档数+1})$\n>\n> $TF-IDF=词频(TF)×逆文档频率(IDF)$\n\n但是要用这样的概念生成 句子-词语 矩阵，似乎不可行，因为这其中没有句子的概念。这里可以采用另一个概念 `TF-ISF` 来生成矩阵：\n\n> $词频(TF) = \\frac{某个词在句子中的出现次数}{句子的总词数}$\n>\n> $逆文档频率(ISF)=log(\\frac{语料库的句子总数}{包含该词的句子数+1})$\n>\n> $TF-ISF=词频(TF)×逆文档频率(ISF)$\n\n通过这样的公式，对每一个 `topic` 中的句子与单词分别计算 句子-词语 矩阵。\n\n为了方便后期的处理，在矩阵的最后一列加入一行 同一个主题里所有句子拼接起来的一个长句子。\n\n## 计算相似度\n\n生成的矩阵现在可以用于计算句子与句子间的相似度。我们将所有的句子与最后的长句子（所有句子拼接而成的句子）进行余弦相似度的计算，公式如下：\n\n$\\cos\\theta=\\frac{\\sum_{i=1}^n(A_i×B_i)}{\\sqrt{\\sum_{i=1}^n(A_i)^2}×{\\sqrt{\\sum_{i=1}^n(B_i)^2}}} \\\\ = \\frac{A\\cdot{B}}{\\mid{A}\\mid×\\mid{B}\\mid}$\n\n定义函数 `calSimilarity(arr1:numpy.ndarray, arr2:numpy.ndarray)` 输入两个向量计算相似度。\n\n```python\n# Similarity Calculation\ndef calSimilarity(arr1:numpy.ndarray, arr2:numpy.ndarray):\n  return numpy.dot(arr1, arr2) / (numpy.linalg.norm(arr1) * numpy.linalg.norm(arr2))\n```\n\n得到每个句子对最后的长句子的相似度之后进行排序。\n\n```python\nsimvaluedict = sorted(simvaluedict.items(), key=lambda item: item[1], reverse=True)\n```\n\n最终得到一个按照相似度数值高低排序的矩阵，每行代表一个句子以及其相似度值。\n\n## 冗余控制与摘要生成\n\n上一步得到的矩阵中，我们选取相似度值最高的句子放入最终的摘要中，然后选取第二句，与第一句作相似度计算，如果高于某个阈值，则丢弃，直到文本的大小达到 665byte 为止。这里的阈值我经过试验之后设为了 0.4，实验中如果阈值过高很可能导致摘要长度还未达到规定大小就生成结束，最终导致摘要过短。\n\n至此，每个 `topic` 的摘要内容生成完毕。\n\n## 摘要性能评测\n\n对于生成的摘要，我将其与数据集中专家给出的几份摘要都作 ROUGE 的计算，得到的最高分计入最终结果。这里的 ROUGE 评分主要有三项：ROUGE-1, ROUGE-2, ROUGE-l，着重关注的是回归率 r。\n\n为了作对比，还需要提供一个 `Baseline` 结果，这里对于每一个 `topic` 里的每一篇文章选取第一句话拼接为一个摘要，长度同样为 665byte，再对比专家组结果作 ROUGE 评分。\n\n将两者进行对比，绘制直方图：\n\n![](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/rouge-1.png)\n\n![](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/rouge-2.png)\n\n![](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/rouge-l.png)\n\n\n\n红色部分为算法生成的结果，蓝色部分为基线结果，可以看到在大部分 `topic` 中得到的效果要高于或等于基线效果(33/50)。\n\n## 关于算法的改进\n\n改进方法中我选择了改进方法2进行实验。\n\n调查了一下 `sentence2vec` 的方法，发现首先需要有个词向量的数据 (word2vec)。网络上找到的方法是使用 `gensim` 库对语料库进行训练，得到 `word2vec` 的数据之后再计算句向量，剩下的内容就和之前差不多了，也是计算相似度之后排序生成摘要。\n\n我尝试用原始的文档训练词向量，然后计算句向量，但是得到的结果有很大的问题。问题主要在于使用原始文档训练出来的词向量计算出的句向量，这些向量之间的相似度极高，基本都高达 99% 以上，对这样的相似度数据进行排序显然没有任何意义。\n\n于是我找到了一个预训练的数据集 `GloVe: Global Vectors for Word Representation`，选取了其中一个 50 维的词向量数据集作为参考。这里就注意不需要对每个词进行词干化了，否则在词向量数据中是找不到对应向量的，默认会置0，也就是没有意义的。\n\n`sentence2vec` 句向量的计算参照以下公式：\n\n$v_s\\leftarrow{\\frac{1}{\\mid s \\mid}}\\sum_{w\\in s}{\\frac{a}{a+p(w)}v_w}$\n\n算出句向量之后就可以像之前一样进行相似度计算排序然后生成了。\n\n但是令人意外的是这样生成的摘要得到的效果反而不如之前的算法生成的效果，结果如下：\n\n![](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/srouge-1.png)\n\n![](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/srouge-2.png)\n\n![](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/srouge-l.png)\n\n\n但是结果确实是如此，我检查代码也没有发现有什么明显的问题。可能这个词向量的数据集对于该任务来说并不适合吧。","slug":"自动文本摘要(TF-ISF)","published":1,"updated":"2022-08-25T13:13:32.988Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clgyiaru3000njku74pr423dt","content":"<p><a href=\"https://github.com/1099255210/Auto-Text-Summarization-Based-on-TF-ISF\">项目开源地址</a></p>\n<h2 id=\"目的\"><a href=\"#目的\" class=\"headerlink\" title=\"目的\"></a>目的</h2><p>对于 <code>DUC2004</code> 数据集的 50 个 topic 生成文本摘要，并将生成的摘要与 baseline，共同和专家组的摘要做 Rouge 评分。</p>\n<p>数据集地址：(<a href=\"https://www-nlpir.nist.gov/projects/duc/data/2004_data.html\">https://www-nlpir.nist.gov/projects/duc/data/2004_data.html</a>)</p>\n<h2 id=\"预处理数据\"><a href=\"#预处理数据\" class=\"headerlink\" title=\"预处理数据\"></a>预处理数据</h2><p>预处理数据分为以下几步：</p>\n<h3 id=\"句子切分\"><a href=\"#句子切分\" class=\"headerlink\" title=\"句子切分\"></a>句子切分</h3><p>句子切分一开始我使用的是用 <code>.</code> 即英文的句号进行分割，但是后来在检查结果的时候发现有些单词比如 <code>U. N.</code> 会被误认为是多个句子。于是我借用了 <code>nltk</code> 库的 <code>tokenize</code> 函数进行句子的分割，有效避免了这个问题，当然也同时避免了无法区分以非英文句号为结尾的句子的情况。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tokenizer = nltk.data.load(<span class=\"string\">&#x27;tokenizers/punkt/english.pickle&#x27;</span>)</span><br><span class=\"line\">filestat.sentenselist = tokenizer.tokenize(file.content)</span><br></pre></td></tr></table></figure>\n<h3 id=\"去除停用词\"><a href=\"#去除停用词\" class=\"headerlink\" title=\"去除停用词\"></a>去除停用词</h3><p>句子切分完之后，就可以以空格为分割符提取句子中的每个单词。提取的时候要注意的地方是：单词前后可能有特殊符号需要去除；单词可以直接统一转为小写；单词中的数字也要去除。这里用正则表达式对单词处理。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">word = re.sub(<span class=\"string\">&quot;[^A-Za-z]+&quot;</span>, <span class=\"string\">&#x27; &#x27;</span>, <span class=\"built_in\">str</span>(word)).lower().strip()</span><br></pre></td></tr></table></figure>\n<p>得到每一个单词之后，需要去除停用词，比如 <code>a, the</code> 等等。</p>\n<p>停用词去除的工作也是调用 <code>nltk</code> 库中的 <code>stopwords</code> ，只要检查单词是否在停用词中然后省略即可。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> nltk.corpus <span class=\"keyword\">import</span> stopwords</span><br><span class=\"line\"><span class=\"keyword\">if</span> word <span class=\"keyword\">in</span> stopwords.words(<span class=\"string\">&#x27;english&#x27;</span>):</span><br><span class=\"line\">    <span class=\"comment\"># Get rid of this word.</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"单词词干化（https-tartarus-org-martin-PorterStemmer-）\"><a href=\"#单词词干化（https-tartarus-org-martin-PorterStemmer-）\" class=\"headerlink\" title=\"单词词干化（https://tartarus.org/martin/PorterStemmer/）\"></a>单词词干化（<a href=\"https://tartarus.org/martin/PorterStemmer/）\">https://tartarus.org/martin/PorterStemmer/）</a></h3><p> 这一步课件里给出了一个停用词去除的库 <code>PorterStemmer</code> 网站上直接可以下载到源码。保存至 <code>porterstemming.py</code> 之后实例化一个 <code>PorterStemmer</code> 对象，之后进行单词词干化。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> porterstemming</span><br><span class=\"line\">PS = porterstemming.PorterStemmer()</span><br><span class=\"line\">word = PS.stem(word, <span class=\"number\">0</span>, <span class=\"built_in\">len</span>(word) - <span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n<p>对每一个句子进行如上操作之后，将句子与句子中经过处理的单词序列存入一个数据结构中待使用；每篇文章里的所有句子也存入一个数据结构中，用于下一步分析单词与句子的统计特性。</p>\n<h2 id=\"DUC-文本数据转化为数值型数据\"><a href=\"#DUC-文本数据转化为数值型数据\" class=\"headerlink\" title=\"DUC 文本数据转化为数值型数据\"></a>DUC 文本数据转化为数值型数据</h2><p>这里课件里引入 <code>TF-IDF</code> 的概念：</p>\n<blockquote>\n<p>$词频(TF) = \\frac{某个词在文章中的出现次数}{文章的总词数}$</p>\n<p>$逆文档频率(IDF)=log(\\frac{语料库的文档总数}{包含该词的文档数+1})$</p>\n<p>$TF-IDF=词频(TF)×逆文档频率(IDF)$</p>\n</blockquote>\n<p>但是要用这样的概念生成 句子-词语 矩阵，似乎不可行，因为这其中没有句子的概念。这里可以采用另一个概念 <code>TF-ISF</code> 来生成矩阵：</p>\n<blockquote>\n<p>$词频(TF) = \\frac{某个词在句子中的出现次数}{句子的总词数}$</p>\n<p>$逆文档频率(ISF)=log(\\frac{语料库的句子总数}{包含该词的句子数+1})$</p>\n<p>$TF-ISF=词频(TF)×逆文档频率(ISF)$</p>\n</blockquote>\n<p>通过这样的公式，对每一个 <code>topic</code> 中的句子与单词分别计算 句子-词语 矩阵。</p>\n<p>为了方便后期的处理，在矩阵的最后一列加入一行 同一个主题里所有句子拼接起来的一个长句子。</p>\n<h2 id=\"计算相似度\"><a href=\"#计算相似度\" class=\"headerlink\" title=\"计算相似度\"></a>计算相似度</h2><p>生成的矩阵现在可以用于计算句子与句子间的相似度。我们将所有的句子与最后的长句子（所有句子拼接而成的句子）进行余弦相似度的计算，公式如下：</p>\n<p>$\\cos\\theta=\\frac{\\sum_{i=1}^n(A_i×B_i)}{\\sqrt{\\sum_{i=1}^n(A_i)^2}×{\\sqrt{\\sum_{i=1}^n(B_i)^2}}} \\\\ = \\frac{A\\cdot{B}}{\\mid{A}\\mid×\\mid{B}\\mid}$</p>\n<p>定义函数 <code>calSimilarity(arr1:numpy.ndarray, arr2:numpy.ndarray)</code> 输入两个向量计算相似度。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Similarity Calculation</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">calSimilarity</span>(<span class=\"params\">arr1:numpy.ndarray, arr2:numpy.ndarray</span>):</span><br><span class=\"line\">  <span class=\"keyword\">return</span> numpy.dot(arr1, arr2) / (numpy.linalg.norm(arr1) * numpy.linalg.norm(arr2))</span><br></pre></td></tr></table></figure>\n<p>得到每个句子对最后的长句子的相似度之后进行排序。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">simvaluedict = <span class=\"built_in\">sorted</span>(simvaluedict.items(), key=<span class=\"keyword\">lambda</span> item: item[<span class=\"number\">1</span>], reverse=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n<p>最终得到一个按照相似度数值高低排序的矩阵，每行代表一个句子以及其相似度值。</p>\n<h2 id=\"冗余控制与摘要生成\"><a href=\"#冗余控制与摘要生成\" class=\"headerlink\" title=\"冗余控制与摘要生成\"></a>冗余控制与摘要生成</h2><p>上一步得到的矩阵中，我们选取相似度值最高的句子放入最终的摘要中，然后选取第二句，与第一句作相似度计算，如果高于某个阈值，则丢弃，直到文本的大小达到 665byte 为止。这里的阈值我经过试验之后设为了 0.4，实验中如果阈值过高很可能导致摘要长度还未达到规定大小就生成结束，最终导致摘要过短。</p>\n<p>至此，每个 <code>topic</code> 的摘要内容生成完毕。</p>\n<h2 id=\"摘要性能评测\"><a href=\"#摘要性能评测\" class=\"headerlink\" title=\"摘要性能评测\"></a>摘要性能评测</h2><p>对于生成的摘要，我将其与数据集中专家给出的几份摘要都作 ROUGE 的计算，得到的最高分计入最终结果。这里的 ROUGE 评分主要有三项：ROUGE-1, ROUGE-2, ROUGE-l，着重关注的是回归率 r。</p>\n<p>为了作对比，还需要提供一个 <code>Baseline</code> 结果，这里对于每一个 <code>topic</code> 里的每一篇文章选取第一句话拼接为一个摘要，长度同样为 665byte，再对比专家组结果作 ROUGE 评分。</p>\n<p>将两者进行对比，绘制直方图：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/rouge-1.png\" alt=\"\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/rouge-2.png\" alt=\"\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/rouge-l.png\" alt=\"\"></p>\n<p>红色部分为算法生成的结果，蓝色部分为基线结果，可以看到在大部分 <code>topic</code> 中得到的效果要高于或等于基线效果(33/50)。</p>\n<h2 id=\"关于算法的改进\"><a href=\"#关于算法的改进\" class=\"headerlink\" title=\"关于算法的改进\"></a>关于算法的改进</h2><p>改进方法中我选择了改进方法2进行实验。</p>\n<p>调查了一下 <code>sentence2vec</code> 的方法，发现首先需要有个词向量的数据 (word2vec)。网络上找到的方法是使用 <code>gensim</code> 库对语料库进行训练，得到 <code>word2vec</code> 的数据之后再计算句向量，剩下的内容就和之前差不多了，也是计算相似度之后排序生成摘要。</p>\n<p>我尝试用原始的文档训练词向量，然后计算句向量，但是得到的结果有很大的问题。问题主要在于使用原始文档训练出来的词向量计算出的句向量，这些向量之间的相似度极高，基本都高达 99% 以上，对这样的相似度数据进行排序显然没有任何意义。</p>\n<p>于是我找到了一个预训练的数据集 <code>GloVe: Global Vectors for Word Representation</code>，选取了其中一个 50 维的词向量数据集作为参考。这里就注意不需要对每个词进行词干化了，否则在词向量数据中是找不到对应向量的，默认会置0，也就是没有意义的。</p>\n<p><code>sentence2vec</code> 句向量的计算参照以下公式：</p>\n<p>$v_s\\leftarrow{\\frac{1}{\\mid s \\mid}}\\sum_{w\\in s}{\\frac{a}{a+p(w)}v_w}$</p>\n<p>算出句向量之后就可以像之前一样进行相似度计算排序然后生成了。</p>\n<p>但是令人意外的是这样生成的摘要得到的效果反而不如之前的算法生成的效果，结果如下：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/srouge-1.png\" alt=\"\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/srouge-2.png\" alt=\"\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/srouge-l.png\" alt=\"\"></p>\n<p>但是结果确实是如此，我检查代码也没有发现有什么明显的问题。可能这个词向量的数据集对于该任务来说并不适合吧。</p>\n","site":{"data":{}},"excerpt":"","more":"<p><a href=\"https://github.com/1099255210/Auto-Text-Summarization-Based-on-TF-ISF\">项目开源地址</a></p>\n<h2 id=\"目的\"><a href=\"#目的\" class=\"headerlink\" title=\"目的\"></a>目的</h2><p>对于 <code>DUC2004</code> 数据集的 50 个 topic 生成文本摘要，并将生成的摘要与 baseline，共同和专家组的摘要做 Rouge 评分。</p>\n<p>数据集地址：(<a href=\"https://www-nlpir.nist.gov/projects/duc/data/2004_data.html\">https://www-nlpir.nist.gov/projects/duc/data/2004_data.html</a>)</p>\n<h2 id=\"预处理数据\"><a href=\"#预处理数据\" class=\"headerlink\" title=\"预处理数据\"></a>预处理数据</h2><p>预处理数据分为以下几步：</p>\n<h3 id=\"句子切分\"><a href=\"#句子切分\" class=\"headerlink\" title=\"句子切分\"></a>句子切分</h3><p>句子切分一开始我使用的是用 <code>.</code> 即英文的句号进行分割，但是后来在检查结果的时候发现有些单词比如 <code>U. N.</code> 会被误认为是多个句子。于是我借用了 <code>nltk</code> 库的 <code>tokenize</code> 函数进行句子的分割，有效避免了这个问题，当然也同时避免了无法区分以非英文句号为结尾的句子的情况。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tokenizer = nltk.data.load(<span class=\"string\">&#x27;tokenizers/punkt/english.pickle&#x27;</span>)</span><br><span class=\"line\">filestat.sentenselist = tokenizer.tokenize(file.content)</span><br></pre></td></tr></table></figure>\n<h3 id=\"去除停用词\"><a href=\"#去除停用词\" class=\"headerlink\" title=\"去除停用词\"></a>去除停用词</h3><p>句子切分完之后，就可以以空格为分割符提取句子中的每个单词。提取的时候要注意的地方是：单词前后可能有特殊符号需要去除；单词可以直接统一转为小写；单词中的数字也要去除。这里用正则表达式对单词处理。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">word = re.sub(<span class=\"string\">&quot;[^A-Za-z]+&quot;</span>, <span class=\"string\">&#x27; &#x27;</span>, <span class=\"built_in\">str</span>(word)).lower().strip()</span><br></pre></td></tr></table></figure>\n<p>得到每一个单词之后，需要去除停用词，比如 <code>a, the</code> 等等。</p>\n<p>停用词去除的工作也是调用 <code>nltk</code> 库中的 <code>stopwords</code> ，只要检查单词是否在停用词中然后省略即可。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> nltk.corpus <span class=\"keyword\">import</span> stopwords</span><br><span class=\"line\"><span class=\"keyword\">if</span> word <span class=\"keyword\">in</span> stopwords.words(<span class=\"string\">&#x27;english&#x27;</span>):</span><br><span class=\"line\">    <span class=\"comment\"># Get rid of this word.</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"单词词干化（https-tartarus-org-martin-PorterStemmer-）\"><a href=\"#单词词干化（https-tartarus-org-martin-PorterStemmer-）\" class=\"headerlink\" title=\"单词词干化（https://tartarus.org/martin/PorterStemmer/）\"></a>单词词干化（<a href=\"https://tartarus.org/martin/PorterStemmer/）\">https://tartarus.org/martin/PorterStemmer/）</a></h3><p> 这一步课件里给出了一个停用词去除的库 <code>PorterStemmer</code> 网站上直接可以下载到源码。保存至 <code>porterstemming.py</code> 之后实例化一个 <code>PorterStemmer</code> 对象，之后进行单词词干化。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> porterstemming</span><br><span class=\"line\">PS = porterstemming.PorterStemmer()</span><br><span class=\"line\">word = PS.stem(word, <span class=\"number\">0</span>, <span class=\"built_in\">len</span>(word) - <span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n<p>对每一个句子进行如上操作之后，将句子与句子中经过处理的单词序列存入一个数据结构中待使用；每篇文章里的所有句子也存入一个数据结构中，用于下一步分析单词与句子的统计特性。</p>\n<h2 id=\"DUC-文本数据转化为数值型数据\"><a href=\"#DUC-文本数据转化为数值型数据\" class=\"headerlink\" title=\"DUC 文本数据转化为数值型数据\"></a>DUC 文本数据转化为数值型数据</h2><p>这里课件里引入 <code>TF-IDF</code> 的概念：</p>\n<blockquote>\n<p>$词频(TF) = \\frac{某个词在文章中的出现次数}{文章的总词数}$</p>\n<p>$逆文档频率(IDF)=log(\\frac{语料库的文档总数}{包含该词的文档数+1})$</p>\n<p>$TF-IDF=词频(TF)×逆文档频率(IDF)$</p>\n</blockquote>\n<p>但是要用这样的概念生成 句子-词语 矩阵，似乎不可行，因为这其中没有句子的概念。这里可以采用另一个概念 <code>TF-ISF</code> 来生成矩阵：</p>\n<blockquote>\n<p>$词频(TF) = \\frac{某个词在句子中的出现次数}{句子的总词数}$</p>\n<p>$逆文档频率(ISF)=log(\\frac{语料库的句子总数}{包含该词的句子数+1})$</p>\n<p>$TF-ISF=词频(TF)×逆文档频率(ISF)$</p>\n</blockquote>\n<p>通过这样的公式，对每一个 <code>topic</code> 中的句子与单词分别计算 句子-词语 矩阵。</p>\n<p>为了方便后期的处理，在矩阵的最后一列加入一行 同一个主题里所有句子拼接起来的一个长句子。</p>\n<h2 id=\"计算相似度\"><a href=\"#计算相似度\" class=\"headerlink\" title=\"计算相似度\"></a>计算相似度</h2><p>生成的矩阵现在可以用于计算句子与句子间的相似度。我们将所有的句子与最后的长句子（所有句子拼接而成的句子）进行余弦相似度的计算，公式如下：</p>\n<p>$\\cos\\theta=\\frac{\\sum_{i=1}^n(A_i×B_i)}{\\sqrt{\\sum_{i=1}^n(A_i)^2}×{\\sqrt{\\sum_{i=1}^n(B_i)^2}}} \\\\ = \\frac{A\\cdot{B}}{\\mid{A}\\mid×\\mid{B}\\mid}$</p>\n<p>定义函数 <code>calSimilarity(arr1:numpy.ndarray, arr2:numpy.ndarray)</code> 输入两个向量计算相似度。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Similarity Calculation</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">calSimilarity</span>(<span class=\"params\">arr1:numpy.ndarray, arr2:numpy.ndarray</span>):</span><br><span class=\"line\">  <span class=\"keyword\">return</span> numpy.dot(arr1, arr2) / (numpy.linalg.norm(arr1) * numpy.linalg.norm(arr2))</span><br></pre></td></tr></table></figure>\n<p>得到每个句子对最后的长句子的相似度之后进行排序。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">simvaluedict = <span class=\"built_in\">sorted</span>(simvaluedict.items(), key=<span class=\"keyword\">lambda</span> item: item[<span class=\"number\">1</span>], reverse=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n<p>最终得到一个按照相似度数值高低排序的矩阵，每行代表一个句子以及其相似度值。</p>\n<h2 id=\"冗余控制与摘要生成\"><a href=\"#冗余控制与摘要生成\" class=\"headerlink\" title=\"冗余控制与摘要生成\"></a>冗余控制与摘要生成</h2><p>上一步得到的矩阵中，我们选取相似度值最高的句子放入最终的摘要中，然后选取第二句，与第一句作相似度计算，如果高于某个阈值，则丢弃，直到文本的大小达到 665byte 为止。这里的阈值我经过试验之后设为了 0.4，实验中如果阈值过高很可能导致摘要长度还未达到规定大小就生成结束，最终导致摘要过短。</p>\n<p>至此，每个 <code>topic</code> 的摘要内容生成完毕。</p>\n<h2 id=\"摘要性能评测\"><a href=\"#摘要性能评测\" class=\"headerlink\" title=\"摘要性能评测\"></a>摘要性能评测</h2><p>对于生成的摘要，我将其与数据集中专家给出的几份摘要都作 ROUGE 的计算，得到的最高分计入最终结果。这里的 ROUGE 评分主要有三项：ROUGE-1, ROUGE-2, ROUGE-l，着重关注的是回归率 r。</p>\n<p>为了作对比，还需要提供一个 <code>Baseline</code> 结果，这里对于每一个 <code>topic</code> 里的每一篇文章选取第一句话拼接为一个摘要，长度同样为 665byte，再对比专家组结果作 ROUGE 评分。</p>\n<p>将两者进行对比，绘制直方图：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/rouge-1.png\" alt=\"\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/rouge-2.png\" alt=\"\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/rouge-l.png\" alt=\"\"></p>\n<p>红色部分为算法生成的结果，蓝色部分为基线结果，可以看到在大部分 <code>topic</code> 中得到的效果要高于或等于基线效果(33/50)。</p>\n<h2 id=\"关于算法的改进\"><a href=\"#关于算法的改进\" class=\"headerlink\" title=\"关于算法的改进\"></a>关于算法的改进</h2><p>改进方法中我选择了改进方法2进行实验。</p>\n<p>调查了一下 <code>sentence2vec</code> 的方法，发现首先需要有个词向量的数据 (word2vec)。网络上找到的方法是使用 <code>gensim</code> 库对语料库进行训练，得到 <code>word2vec</code> 的数据之后再计算句向量，剩下的内容就和之前差不多了，也是计算相似度之后排序生成摘要。</p>\n<p>我尝试用原始的文档训练词向量，然后计算句向量，但是得到的结果有很大的问题。问题主要在于使用原始文档训练出来的词向量计算出的句向量，这些向量之间的相似度极高，基本都高达 99% 以上，对这样的相似度数据进行排序显然没有任何意义。</p>\n<p>于是我找到了一个预训练的数据集 <code>GloVe: Global Vectors for Word Representation</code>，选取了其中一个 50 维的词向量数据集作为参考。这里就注意不需要对每个词进行词干化了，否则在词向量数据中是找不到对应向量的，默认会置0，也就是没有意义的。</p>\n<p><code>sentence2vec</code> 句向量的计算参照以下公式：</p>\n<p>$v_s\\leftarrow{\\frac{1}{\\mid s \\mid}}\\sum_{w\\in s}{\\frac{a}{a+p(w)}v_w}$</p>\n<p>算出句向量之后就可以像之前一样进行相似度计算排序然后生成了。</p>\n<p>但是令人意外的是这样生成的摘要得到的效果反而不如之前的算法生成的效果，结果如下：</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/srouge-1.png\" alt=\"\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/srouge-2.png\" alt=\"\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/srouge-l.png\" alt=\"\"></p>\n<p>但是结果确实是如此，我检查代码也没有发现有什么明显的问题。可能这个词向量的数据集对于该任务来说并不适合吧。</p>\n"},{"title":"知识蒸馏 (Knowledge Distillation)","date":"2022-08-05T11:09:08.000Z","mathjax":true,"_content":"\n# 知识蒸馏 (Knowledge Distillation)\n\n## 简介\n\n知识蒸馏（Knowledge Distillation, KD）旨在把一个大模型或者多个集成模型学到的知识迁移到另一个轻量级的简单模型上，在模型压缩的思想下实现知识迁移，并方便部署。\n\n虽然在一般情况下，我们不会去区分训练和部署使用的模型，但是训练和部署之间存在着一定的不一致性:   \n\n在训练过程中，我们需要使用复杂的模型，大量的计算资源，以便从非常大、高度冗余的数据集中提取出信息。在实验中，效果最好的模型往往规模很大，甚至由多个模型集成得到。而大模型不方便部署到服务中去，常见的瓶颈如下: \n\n- 推断速度慢        \n- 对部署资源要求高(内存，显存等)    \n\n在部署时，我们对延迟以及计算资源都有着严格的限制。 因此，模型压缩（在保证性能的前提下减少模型的参数量） 成为了一个重要的问题，而”模型蒸馏“属于模型压缩的一种方法。\n\n一个模型的参数量基本决定了其所能捕获到的数据内蕴含的“知识”的量。 这样的想法是基本正确的，但是需要注意的是:     模型的参数量和其所能捕获的“知识“量之间并非稳定的线性关系，而是接近边际收益逐渐减少的一种增长曲线。\n\n![image-20220807174932466](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220807174932466.png)\n\n知识蒸馏分为：**教师-学生迁移**（离线学习）和**学生互学习迁移**（在线学习）。\n\n## 知识蒸馏的方法\n\n### 教师-学生迁移（T-S）\n\nTeacher是“知识”的输出者，Student是“知识”的接受者\n\n分为两个阶段：\n\n- 原始模型训练: 训练\"Teacher模型\", 简称为Net-T，它的特点是模型相对复杂，也可以由多个分别训练的模型集成而成。我们对\"Teacher模型\"不作任何关于模型架构、参数量、是否集成方面的限制，唯一的要求就是，对于输入X, 其都能输出Y，其中Y经过softmax的映射，输出值对应相应类别的概率值。\n- 精简模型训练: 训练\"Student模型\", 简称为Net-S，它是参数量较小、模型结构相对简单的单模型。同样的，对于输入X，其都能输出Y，Y经过softmax映射后同样能输出对应相应类别的概率值。\n\n关键点：机器学习最根本的目的在于训练出在某个问题上泛化能力强的模型。模型 Net-T 本身泛化能力较强，在训练 Net-S 时可以直接让它学习 Net-T 的泛化能力，具体方法就是用 Net-T softmax层输出的各类别的概率作为 soft target，而不是只使用基于 ground truth 的 hard target。\n\n在 softmax 层的输出中除了正例之外，负标签也具有大量的信息，尤其当 soft target 的分布熵相对高时，蕴含的知识就更加丰富。\n\n![image-20220807180402205](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220807180402205.png)\n\n这里如果直接用原始的 softmax 输出作为 soft target，在概率分布熵较小（即负标签值都比较接近于0）的情况下对损失函数的贡献偏小，因此需要引入“温度”这个参数。\n\n原始的 softmax：\n$$\nq_i=\\frac{exp(z_i)}{\\sum_jexp(z_j)}\n$$\n引入温度：\n$$\nq_i=\\frac{exp(z_i/T)}{\\sum_jexp(z_j/T)}\n$$\n$T$的值越大，分布熵越大，负标签携带的信息会相对放大，模型训练将更加关注负标签。\n\n总体的Loss函数由 soft target 和 hard target 加权得到：\n$$\nL={\\alpha}L_{soft}+{\\beta}L_{hard}\n$$\n需要 hard target 的原因是，Net-T 也有一定的错误率，加入 ground truth 的判别可以有效降低错误被传播给 Net-S 的可能性。\n\n在温度的选取方面，如果 Net-S 的参数量较小，那么选取相对比较低的温度就可以了，因为参数量小的模型可能不能学习到所有的知识，可以适当忽略掉一些负标签的信息。\n\n（这部分的后续内容有待以后扩充）\n\n### 学生互学习迁移\n\n","source":"_posts/知识蒸馏 (Knowledge Distillation).md","raw":"---\ntitle: 知识蒸馏 (Knowledge Distillation)\ndate: 2022-08-05 19:09:08\ntags:\nmathjax: true\n---\n\n# 知识蒸馏 (Knowledge Distillation)\n\n## 简介\n\n知识蒸馏（Knowledge Distillation, KD）旨在把一个大模型或者多个集成模型学到的知识迁移到另一个轻量级的简单模型上，在模型压缩的思想下实现知识迁移，并方便部署。\n\n虽然在一般情况下，我们不会去区分训练和部署使用的模型，但是训练和部署之间存在着一定的不一致性:   \n\n在训练过程中，我们需要使用复杂的模型，大量的计算资源，以便从非常大、高度冗余的数据集中提取出信息。在实验中，效果最好的模型往往规模很大，甚至由多个模型集成得到。而大模型不方便部署到服务中去，常见的瓶颈如下: \n\n- 推断速度慢        \n- 对部署资源要求高(内存，显存等)    \n\n在部署时，我们对延迟以及计算资源都有着严格的限制。 因此，模型压缩（在保证性能的前提下减少模型的参数量） 成为了一个重要的问题，而”模型蒸馏“属于模型压缩的一种方法。\n\n一个模型的参数量基本决定了其所能捕获到的数据内蕴含的“知识”的量。 这样的想法是基本正确的，但是需要注意的是:     模型的参数量和其所能捕获的“知识“量之间并非稳定的线性关系，而是接近边际收益逐渐减少的一种增长曲线。\n\n![image-20220807174932466](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220807174932466.png)\n\n知识蒸馏分为：**教师-学生迁移**（离线学习）和**学生互学习迁移**（在线学习）。\n\n## 知识蒸馏的方法\n\n### 教师-学生迁移（T-S）\n\nTeacher是“知识”的输出者，Student是“知识”的接受者\n\n分为两个阶段：\n\n- 原始模型训练: 训练\"Teacher模型\", 简称为Net-T，它的特点是模型相对复杂，也可以由多个分别训练的模型集成而成。我们对\"Teacher模型\"不作任何关于模型架构、参数量、是否集成方面的限制，唯一的要求就是，对于输入X, 其都能输出Y，其中Y经过softmax的映射，输出值对应相应类别的概率值。\n- 精简模型训练: 训练\"Student模型\", 简称为Net-S，它是参数量较小、模型结构相对简单的单模型。同样的，对于输入X，其都能输出Y，Y经过softmax映射后同样能输出对应相应类别的概率值。\n\n关键点：机器学习最根本的目的在于训练出在某个问题上泛化能力强的模型。模型 Net-T 本身泛化能力较强，在训练 Net-S 时可以直接让它学习 Net-T 的泛化能力，具体方法就是用 Net-T softmax层输出的各类别的概率作为 soft target，而不是只使用基于 ground truth 的 hard target。\n\n在 softmax 层的输出中除了正例之外，负标签也具有大量的信息，尤其当 soft target 的分布熵相对高时，蕴含的知识就更加丰富。\n\n![image-20220807180402205](https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220807180402205.png)\n\n这里如果直接用原始的 softmax 输出作为 soft target，在概率分布熵较小（即负标签值都比较接近于0）的情况下对损失函数的贡献偏小，因此需要引入“温度”这个参数。\n\n原始的 softmax：\n$$\nq_i=\\frac{exp(z_i)}{\\sum_jexp(z_j)}\n$$\n引入温度：\n$$\nq_i=\\frac{exp(z_i/T)}{\\sum_jexp(z_j/T)}\n$$\n$T$的值越大，分布熵越大，负标签携带的信息会相对放大，模型训练将更加关注负标签。\n\n总体的Loss函数由 soft target 和 hard target 加权得到：\n$$\nL={\\alpha}L_{soft}+{\\beta}L_{hard}\n$$\n需要 hard target 的原因是，Net-T 也有一定的错误率，加入 ground truth 的判别可以有效降低错误被传播给 Net-S 的可能性。\n\n在温度的选取方面，如果 Net-S 的参数量较小，那么选取相对比较低的温度就可以了，因为参数量小的模型可能不能学习到所有的知识，可以适当忽略掉一些负标签的信息。\n\n（这部分的后续内容有待以后扩充）\n\n### 学生互学习迁移\n\n","slug":"知识蒸馏 (Knowledge Distillation)","published":1,"updated":"2022-08-25T13:13:32.987Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clgyiaru3000ojku77bu886n1","content":"<h1 id=\"知识蒸馏-Knowledge-Distillation\"><a href=\"#知识蒸馏-Knowledge-Distillation\" class=\"headerlink\" title=\"知识蒸馏 (Knowledge Distillation)\"></a>知识蒸馏 (Knowledge Distillation)</h1><h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>知识蒸馏（Knowledge Distillation, KD）旨在把一个大模型或者多个集成模型学到的知识迁移到另一个轻量级的简单模型上，在模型压缩的思想下实现知识迁移，并方便部署。</p>\n<p>虽然在一般情况下，我们不会去区分训练和部署使用的模型，但是训练和部署之间存在着一定的不一致性:   </p>\n<p>在训练过程中，我们需要使用复杂的模型，大量的计算资源，以便从非常大、高度冗余的数据集中提取出信息。在实验中，效果最好的模型往往规模很大，甚至由多个模型集成得到。而大模型不方便部署到服务中去，常见的瓶颈如下: </p>\n<ul>\n<li>推断速度慢        </li>\n<li>对部署资源要求高(内存，显存等)    </li>\n</ul>\n<p>在部署时，我们对延迟以及计算资源都有着严格的限制。 因此，模型压缩（在保证性能的前提下减少模型的参数量） 成为了一个重要的问题，而”模型蒸馏“属于模型压缩的一种方法。</p>\n<p>一个模型的参数量基本决定了其所能捕获到的数据内蕴含的“知识”的量。 这样的想法是基本正确的，但是需要注意的是:     模型的参数量和其所能捕获的“知识“量之间并非稳定的线性关系，而是接近边际收益逐渐减少的一种增长曲线。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220807174932466.png\" alt=\"image-20220807174932466\"></p>\n<p>知识蒸馏分为：<strong>教师-学生迁移</strong>（离线学习）和<strong>学生互学习迁移</strong>（在线学习）。</p>\n<h2 id=\"知识蒸馏的方法\"><a href=\"#知识蒸馏的方法\" class=\"headerlink\" title=\"知识蒸馏的方法\"></a>知识蒸馏的方法</h2><h3 id=\"教师-学生迁移（T-S）\"><a href=\"#教师-学生迁移（T-S）\" class=\"headerlink\" title=\"教师-学生迁移（T-S）\"></a>教师-学生迁移（T-S）</h3><p>Teacher是“知识”的输出者，Student是“知识”的接受者</p>\n<p>分为两个阶段：</p>\n<ul>\n<li>原始模型训练: 训练”Teacher模型”, 简称为Net-T，它的特点是模型相对复杂，也可以由多个分别训练的模型集成而成。我们对”Teacher模型”不作任何关于模型架构、参数量、是否集成方面的限制，唯一的要求就是，对于输入X, 其都能输出Y，其中Y经过softmax的映射，输出值对应相应类别的概率值。</li>\n<li>精简模型训练: 训练”Student模型”, 简称为Net-S，它是参数量较小、模型结构相对简单的单模型。同样的，对于输入X，其都能输出Y，Y经过softmax映射后同样能输出对应相应类别的概率值。</li>\n</ul>\n<p>关键点：机器学习最根本的目的在于训练出在某个问题上泛化能力强的模型。模型 Net-T 本身泛化能力较强，在训练 Net-S 时可以直接让它学习 Net-T 的泛化能力，具体方法就是用 Net-T softmax层输出的各类别的概率作为 soft target，而不是只使用基于 ground truth 的 hard target。</p>\n<p>在 softmax 层的输出中除了正例之外，负标签也具有大量的信息，尤其当 soft target 的分布熵相对高时，蕴含的知识就更加丰富。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220807180402205.png\" alt=\"image-20220807180402205\"></p>\n<p>这里如果直接用原始的 softmax 输出作为 soft target，在概率分布熵较小（即负标签值都比较接近于0）的情况下对损失函数的贡献偏小，因此需要引入“温度”这个参数。</p>\n<p>原始的 softmax：</p>\n<script type=\"math/tex; mode=display\">\nq_i=\\frac{exp(z_i)}{\\sum_jexp(z_j)}</script><p>引入温度：</p>\n<script type=\"math/tex; mode=display\">\nq_i=\\frac{exp(z_i/T)}{\\sum_jexp(z_j/T)}</script><p>$T$的值越大，分布熵越大，负标签携带的信息会相对放大，模型训练将更加关注负标签。</p>\n<p>总体的Loss函数由 soft target 和 hard target 加权得到：</p>\n<script type=\"math/tex; mode=display\">\nL={\\alpha}L_{soft}+{\\beta}L_{hard}</script><p>需要 hard target 的原因是，Net-T 也有一定的错误率，加入 ground truth 的判别可以有效降低错误被传播给 Net-S 的可能性。</p>\n<p>在温度的选取方面，如果 Net-S 的参数量较小，那么选取相对比较低的温度就可以了，因为参数量小的模型可能不能学习到所有的知识，可以适当忽略掉一些负标签的信息。</p>\n<p>（这部分的后续内容有待以后扩充）</p>\n<h3 id=\"学生互学习迁移\"><a href=\"#学生互学习迁移\" class=\"headerlink\" title=\"学生互学习迁移\"></a>学生互学习迁移</h3>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"知识蒸馏-Knowledge-Distillation\"><a href=\"#知识蒸馏-Knowledge-Distillation\" class=\"headerlink\" title=\"知识蒸馏 (Knowledge Distillation)\"></a>知识蒸馏 (Knowledge Distillation)</h1><h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>知识蒸馏（Knowledge Distillation, KD）旨在把一个大模型或者多个集成模型学到的知识迁移到另一个轻量级的简单模型上，在模型压缩的思想下实现知识迁移，并方便部署。</p>\n<p>虽然在一般情况下，我们不会去区分训练和部署使用的模型，但是训练和部署之间存在着一定的不一致性:   </p>\n<p>在训练过程中，我们需要使用复杂的模型，大量的计算资源，以便从非常大、高度冗余的数据集中提取出信息。在实验中，效果最好的模型往往规模很大，甚至由多个模型集成得到。而大模型不方便部署到服务中去，常见的瓶颈如下: </p>\n<ul>\n<li>推断速度慢        </li>\n<li>对部署资源要求高(内存，显存等)    </li>\n</ul>\n<p>在部署时，我们对延迟以及计算资源都有着严格的限制。 因此，模型压缩（在保证性能的前提下减少模型的参数量） 成为了一个重要的问题，而”模型蒸馏“属于模型压缩的一种方法。</p>\n<p>一个模型的参数量基本决定了其所能捕获到的数据内蕴含的“知识”的量。 这样的想法是基本正确的，但是需要注意的是:     模型的参数量和其所能捕获的“知识“量之间并非稳定的线性关系，而是接近边际收益逐渐减少的一种增长曲线。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220807174932466.png\" alt=\"image-20220807174932466\"></p>\n<p>知识蒸馏分为：<strong>教师-学生迁移</strong>（离线学习）和<strong>学生互学习迁移</strong>（在线学习）。</p>\n<h2 id=\"知识蒸馏的方法\"><a href=\"#知识蒸馏的方法\" class=\"headerlink\" title=\"知识蒸馏的方法\"></a>知识蒸馏的方法</h2><h3 id=\"教师-学生迁移（T-S）\"><a href=\"#教师-学生迁移（T-S）\" class=\"headerlink\" title=\"教师-学生迁移（T-S）\"></a>教师-学生迁移（T-S）</h3><p>Teacher是“知识”的输出者，Student是“知识”的接受者</p>\n<p>分为两个阶段：</p>\n<ul>\n<li>原始模型训练: 训练”Teacher模型”, 简称为Net-T，它的特点是模型相对复杂，也可以由多个分别训练的模型集成而成。我们对”Teacher模型”不作任何关于模型架构、参数量、是否集成方面的限制，唯一的要求就是，对于输入X, 其都能输出Y，其中Y经过softmax的映射，输出值对应相应类别的概率值。</li>\n<li>精简模型训练: 训练”Student模型”, 简称为Net-S，它是参数量较小、模型结构相对简单的单模型。同样的，对于输入X，其都能输出Y，Y经过softmax映射后同样能输出对应相应类别的概率值。</li>\n</ul>\n<p>关键点：机器学习最根本的目的在于训练出在某个问题上泛化能力强的模型。模型 Net-T 本身泛化能力较强，在训练 Net-S 时可以直接让它学习 Net-T 的泛化能力，具体方法就是用 Net-T softmax层输出的各类别的概率作为 soft target，而不是只使用基于 ground truth 的 hard target。</p>\n<p>在 softmax 层的输出中除了正例之外，负标签也具有大量的信息，尤其当 soft target 的分布熵相对高时，蕴含的知识就更加丰富。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/1099255210/blogimgrepo@main/img/image-20220807180402205.png\" alt=\"image-20220807180402205\"></p>\n<p>这里如果直接用原始的 softmax 输出作为 soft target，在概率分布熵较小（即负标签值都比较接近于0）的情况下对损失函数的贡献偏小，因此需要引入“温度”这个参数。</p>\n<p>原始的 softmax：</p>\n<script type=\"math/tex; mode=display\">\nq_i=\\frac{exp(z_i)}{\\sum_jexp(z_j)}</script><p>引入温度：</p>\n<script type=\"math/tex; mode=display\">\nq_i=\\frac{exp(z_i/T)}{\\sum_jexp(z_j/T)}</script><p>$T$的值越大，分布熵越大，负标签携带的信息会相对放大，模型训练将更加关注负标签。</p>\n<p>总体的Loss函数由 soft target 和 hard target 加权得到：</p>\n<script type=\"math/tex; mode=display\">\nL={\\alpha}L_{soft}+{\\beta}L_{hard}</script><p>需要 hard target 的原因是，Net-T 也有一定的错误率，加入 ground truth 的判别可以有效降低错误被传播给 Net-S 的可能性。</p>\n<p>在温度的选取方面，如果 Net-S 的参数量较小，那么选取相对比较低的温度就可以了，因为参数量小的模型可能不能学习到所有的知识，可以适当忽略掉一些负标签的信息。</p>\n<p>（这部分的后续内容有待以后扩充）</p>\n<h3 id=\"学生互学习迁移\"><a href=\"#学生互学习迁移\" class=\"headerlink\" title=\"学生互学习迁移\"></a>学生互学习迁移</h3>"},{"title":"键盘笔记一","date":"2021-05-11T23:27:08.000Z","_content":"\n# 键盘组装笔记 202105\n\n![P1030789](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1030789.JPG)\n\n## 前言\n\n这把键盘是我入坑客制化键盘的第二把键盘。第一把键盘我并未想要做过多总结，主要原因在于当时的我对于客制化的了解并不多，虽然上手就采用了比较硬核的焊接方式，中间也有一些有意思的组装过程，但终究没有太多客制化的意味在里面，仅仅算是尝了个鲜；而这回则是一次从内到外的客制化流程体验，让我更多地感受到了客制化的魅力所在，也算是第一次知道键盘的优秀手感能有多让人上瘾。这次的键盘我从 2021 四月初开始筹划，到五月份彻底完成，前前后后是经过很多的调整与修改的，下面我就慢慢讲述整个键盘组装的过程。\n\n## 时间线\n\n### 第一批购物\n\n最初购买了 aws84键三模白光版本 的套件，包含 外壳、定位板、PCB、卫星轴、热插拔轴座，同时购买了多款我比较好奇的 TTC轴体，回来装在套件上体验之后我决定使用月白轴作为主要键位使用的轴体、赤瞳ACE轴作为其他键位使用的轴体。接着我购买了 ToxicLab 的 84布局消音棉，Kelowna 的 润轴笔、22058润滑脂、拔轴器、铁氟龙胶带，KBDfans 的佳达隆白色钢板卫星轴。\n\n我做的第一件事是换卫星轴，原套件中是包含卫星轴的，但是那款卫星轴是白色略透，而定位板是白色铁板，颜色有些不符合。换成 佳达隆变色钢板卫星轴，颜色匹配多了。卫星轴需要剪脚，并拆开，使用 22058润滑脂 进行重新润滑，润滑脂加的可能有一些多，不过最终的效果我还是比较满意的。佳达隆的卫星轴卡在这个定位板上比较紧，卡上去就不是很好拆下来了。卫星轴在PCB上对应的位置我贴上了铁氟龙胶带，效果更佳稳健，噪声更小。\n\n![P1040135](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040135.JPG)\n\n![P1040133](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040133.JPG)\n\n接着是消音棉的处理。这一步其实出现了一点问题：我购买的84布局消音棉（毛毡）实际上和我的套件在螺丝孔位上不是很匹配，炸虾粒 店里应该有为我这个套件定制的消音棉。但是既然都已经买了，就不想再退回去了，反正差不多，我直接对着PCB与定位板之间的孔位在毛毡上开孔，剪开了一些不需要的部分，最后比较顺利地把消音棉夹在了两者之间。唯一的问题是空格键下方的消音棉直接被我抛弃了，也许会产生一些额外的空腔音，不过总体是填满了。\n\n![IMG_20210411_180650](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/IMG_20210411_180650.jpg)\n\n![IMG_20210411_181243](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/IMG_20210411_181243.jpg)\n\n下一步就是装上轴体，没什么好说的。字母区与数字区全部采用 TTC月白轴 ，其余键位均使用线性轴，比如 TTC赤瞳轴，金红轴，凯华奶油轴。线材购买了 酷品世家 的 灰色加粗螺旋线Type-C接口，键帽选的是 KBDfans 的 pbt原厂深灰+浅灰蓝日文键帽，这套键帽算是我买过最贵的键帽了，但是实际效果很让我满意。\n\n![P1040139](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040139.JPG)\n\n![MVIMG_20210414_182904](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/MVIMG_20210414_182904.jpg)\n\n![IMG_20210414_125210](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/IMG_20210414_125210.jpg)\n\n至此，整部键盘第一次完整地出现在我眼前了。键盘的整体上手的时候还是很让我惊艳的，月白轴的手感在当时的我看来十分新鲜，回弹的手感非常带感。键帽的选择不仅带来手感上的舒适，也很养眼。键盘外框可以简易拆卸，制造悬浮键的视觉效果。\n\n![P1030796](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1030796.JPG)\n\n![IMG_20210414_190403](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/IMG_20210414_190403.jpg)\n\n![P1040124](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040124.JPG)\n\n### 第二批购物\n\n没过几天，我又在b站上看到了几个润轴的视频，于是重新打开淘宝，又进行了一轮的购物。这一次为了键盘的“完整性”，购买了 KBDfans 的 KBD75键盘包，然后是 FBB视物所 的 头文字D 贴纸，贴在键盘外框上，为键盘注入了一波灵魂。最后是让我这次客制化感到极具体验的润滑套装：105与205调配润滑脂、轴体开盖器 与 润轴板。\n\n键盘包质量很不错，除了出厂有些许异味，内部绒毛材质很不错，做工精良，基本完全包裹键盘外壳，好评。\n\n![P1040141](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040141.JPG)\n\n![P1040143](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040143.JPG)\n\n![P1040144](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040144.JPG)\n\n贴纸比想象中质量要好（毕竟也是要二十的），贴纸并不是一整块印刷，而是实体切割的，每个笔画都有凸起的质感，贴完立马有内味了是不是？\n\n![P1040147](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040147.JPG)\n\n![P1040146](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040146.JPG)\n\n![P1040118](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040118.JPG)\n\n关于润滑套装，润轴板我直接一步到位上了个顶配：毕竟顶配也就三十几的亚克力，槽位多多益善嘛。润滑脂现在总共是有三种：从左至右分别是 22058、105 & 205 MIX和105，分别对应卫星轴、轴体与弹簧的润滑（其中105因为我懒的原因还没有使用，轴体和弹簧我都是用中间的 “万金油” 润滑的）。开轴器是可乐蛙的什么CNC工艺制作的，可开不同类型的轴体，很有分量和设计感，两部分有磁吸设计，合体后能作为试轴器一枚，个人很喜欢。\n\n![P1040127](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040127.JPG)\n\n![P1040126](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040126.JPG)\n\n![P1040128](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040128.JPG)\n\n![P1040131](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040131.JPG)\n\n这次的快递到的时候，快递盒里有一些泡泡纸，我想着这键盘 PCB 与 底壳中间还有一段空间，不如把泡泡纸塞在里面。塞完后装回去，仿佛有了那么一丢丢的声音提升（\n\n![P1040136](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040136.JPG)\n\n接下来就是：开始很新鲜，后来有些枯燥的润轴过程了。也不知道自己怎么有耐心的，就能坐着一个一个拆开，然后一片一片刷上润滑脂。彼时天气已经开始转热，寝室开起了空调，我听着音乐，悠然自得地润着轴，心情又变得很愉快起来。\n\n![image-20210512055059071](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20210512055059071.png)\n\n![P1040105](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040105.JPG)\n\n经历了漫长的润滑过程，所有轴体被我装回了键盘，扣回键帽，上手体验一番。到这时候我终于才体会到客制化键盘带来的结果上的愉悦：润滑之后的键盘不单单是敲击手感上有了质的飞跃，键盘音也有了十分可观的改善，可以称得上是悦耳了。具体的打字音如何，后面会拍一个视频，自行体会吧。\n\n## 总结\n\n### 满意之处\n\n执行力还是不错的，当时想要完成的内容基本都完成了，没有因为偷懒省去某个步骤，最终成品的完成度令我十分满意。\n\n热插拔轴座使得我不断体验、调整和更换轴体的过程变得轻松惬意。\n\n所有购入的物品在我看来基本都算是物有所值，没有让我比较失望的购买内容。\n\n### 不足之处\n\n这个套件已经是能看的程度了，虽然有些地方还是想吐槽，比如这个套件的底部，廉价感还是很足：\n\n![P1040145](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040145.JPG)\n\n这个套件也没有脚撑（当然很多高端套件也没有），只有两个磁吸增高垫 ，不过200块钱，要什么自行车嘛!\n\n然后就是本轻微强迫症患者比较头疼的部分：这个键帽也有一些问题，部分键的高度放在键盘上很奇怪，于是我只能使用其它键代替，于是就有了右侧这个神秘的区域：\n\n![P1040125](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040125.JPG)\n\n真的不是我故意要这么做的，而是原装的键帽放在上面高低不平实在过于违和。这里下面的 `PgUp` 与 `PgDn` 实际上是 `Home` 和 `End` 键，而右上角的 `ScrollLock` 键实际上应该是 `Delete` 键。日常使用当然没有什么问题，这个键盘的布局算是比较常规的，因此没有给我带来太大的困扰，于是就这么凑合了。\n\n## 购物清单（仅包含实装在此键盘上的项目）\n\n| 物品                     | 店家             | 数量   | 价格   |\n| ------------------------ | ---------------- | ------ | ------ |\n| aws84键三模白光套件      | 爱外设旗舰店     | 1      | 209    |\n| 84布局消音棉             | 致命毒物ToxicLab | 1      | 16.9   |\n| 佳达隆白色钢板卫星轴     | 怡科外设KBDfans  | 1      | 48     |\n| pbt原厂日文键帽          | 怡科外设KBDfans  | 1      | 202.43 |\n| KBD75键盘包              | 怡科外设KBDfans  | 1      | 104    |\n| TTC月白轴                | 兴伟达电子       | 60     | 177.6  |\n| TTC赤瞳轴                | 兴伟达电子       | 10     | 36     |\n| 凯华奶油轴               | 兴伟达电子       | 4      | 18.2   |\n| TTC快银轴                | ttc旗舰店        | 10     | 45     |\n| 电话线螺旋机械键盘数据线 | 酷品世家         | 1      | 49     |\n| 头文字D 贴纸             | FBB视物所        | 1      | 20     |\n|                          |                  | 总计： | 926.13 |\n\n真不愧是 **“氪”制化键盘** 啊!\n\n## 附\n\n*本文章中图片均由本人使用 小米9 与 松下GX85(由czw赞助) 拍摄，没有经过任何修图。*\n\n*购物清单中的价格仅供参考，实际购买可能会有价格浮动*\n\n*Blog : <https://1099255210.github.io/>*\n\n*Bilibili : [不是吴昊的wh](https://space.bilibili.com/7405917)*\n\n","source":"_posts/键盘笔记一.md","raw":"---\ntitle: 键盘笔记一\ndate: 2021-05-12 07:27:08\ntags:\n---\n\n# 键盘组装笔记 202105\n\n![P1030789](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1030789.JPG)\n\n## 前言\n\n这把键盘是我入坑客制化键盘的第二把键盘。第一把键盘我并未想要做过多总结，主要原因在于当时的我对于客制化的了解并不多，虽然上手就采用了比较硬核的焊接方式，中间也有一些有意思的组装过程，但终究没有太多客制化的意味在里面，仅仅算是尝了个鲜；而这回则是一次从内到外的客制化流程体验，让我更多地感受到了客制化的魅力所在，也算是第一次知道键盘的优秀手感能有多让人上瘾。这次的键盘我从 2021 四月初开始筹划，到五月份彻底完成，前前后后是经过很多的调整与修改的，下面我就慢慢讲述整个键盘组装的过程。\n\n## 时间线\n\n### 第一批购物\n\n最初购买了 aws84键三模白光版本 的套件，包含 外壳、定位板、PCB、卫星轴、热插拔轴座，同时购买了多款我比较好奇的 TTC轴体，回来装在套件上体验之后我决定使用月白轴作为主要键位使用的轴体、赤瞳ACE轴作为其他键位使用的轴体。接着我购买了 ToxicLab 的 84布局消音棉，Kelowna 的 润轴笔、22058润滑脂、拔轴器、铁氟龙胶带，KBDfans 的佳达隆白色钢板卫星轴。\n\n我做的第一件事是换卫星轴，原套件中是包含卫星轴的，但是那款卫星轴是白色略透，而定位板是白色铁板，颜色有些不符合。换成 佳达隆变色钢板卫星轴，颜色匹配多了。卫星轴需要剪脚，并拆开，使用 22058润滑脂 进行重新润滑，润滑脂加的可能有一些多，不过最终的效果我还是比较满意的。佳达隆的卫星轴卡在这个定位板上比较紧，卡上去就不是很好拆下来了。卫星轴在PCB上对应的位置我贴上了铁氟龙胶带，效果更佳稳健，噪声更小。\n\n![P1040135](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040135.JPG)\n\n![P1040133](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040133.JPG)\n\n接着是消音棉的处理。这一步其实出现了一点问题：我购买的84布局消音棉（毛毡）实际上和我的套件在螺丝孔位上不是很匹配，炸虾粒 店里应该有为我这个套件定制的消音棉。但是既然都已经买了，就不想再退回去了，反正差不多，我直接对着PCB与定位板之间的孔位在毛毡上开孔，剪开了一些不需要的部分，最后比较顺利地把消音棉夹在了两者之间。唯一的问题是空格键下方的消音棉直接被我抛弃了，也许会产生一些额外的空腔音，不过总体是填满了。\n\n![IMG_20210411_180650](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/IMG_20210411_180650.jpg)\n\n![IMG_20210411_181243](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/IMG_20210411_181243.jpg)\n\n下一步就是装上轴体，没什么好说的。字母区与数字区全部采用 TTC月白轴 ，其余键位均使用线性轴，比如 TTC赤瞳轴，金红轴，凯华奶油轴。线材购买了 酷品世家 的 灰色加粗螺旋线Type-C接口，键帽选的是 KBDfans 的 pbt原厂深灰+浅灰蓝日文键帽，这套键帽算是我买过最贵的键帽了，但是实际效果很让我满意。\n\n![P1040139](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040139.JPG)\n\n![MVIMG_20210414_182904](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/MVIMG_20210414_182904.jpg)\n\n![IMG_20210414_125210](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/IMG_20210414_125210.jpg)\n\n至此，整部键盘第一次完整地出现在我眼前了。键盘的整体上手的时候还是很让我惊艳的，月白轴的手感在当时的我看来十分新鲜，回弹的手感非常带感。键帽的选择不仅带来手感上的舒适，也很养眼。键盘外框可以简易拆卸，制造悬浮键的视觉效果。\n\n![P1030796](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1030796.JPG)\n\n![IMG_20210414_190403](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/IMG_20210414_190403.jpg)\n\n![P1040124](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040124.JPG)\n\n### 第二批购物\n\n没过几天，我又在b站上看到了几个润轴的视频，于是重新打开淘宝，又进行了一轮的购物。这一次为了键盘的“完整性”，购买了 KBDfans 的 KBD75键盘包，然后是 FBB视物所 的 头文字D 贴纸，贴在键盘外框上，为键盘注入了一波灵魂。最后是让我这次客制化感到极具体验的润滑套装：105与205调配润滑脂、轴体开盖器 与 润轴板。\n\n键盘包质量很不错，除了出厂有些许异味，内部绒毛材质很不错，做工精良，基本完全包裹键盘外壳，好评。\n\n![P1040141](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040141.JPG)\n\n![P1040143](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040143.JPG)\n\n![P1040144](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040144.JPG)\n\n贴纸比想象中质量要好（毕竟也是要二十的），贴纸并不是一整块印刷，而是实体切割的，每个笔画都有凸起的质感，贴完立马有内味了是不是？\n\n![P1040147](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040147.JPG)\n\n![P1040146](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040146.JPG)\n\n![P1040118](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040118.JPG)\n\n关于润滑套装，润轴板我直接一步到位上了个顶配：毕竟顶配也就三十几的亚克力，槽位多多益善嘛。润滑脂现在总共是有三种：从左至右分别是 22058、105 & 205 MIX和105，分别对应卫星轴、轴体与弹簧的润滑（其中105因为我懒的原因还没有使用，轴体和弹簧我都是用中间的 “万金油” 润滑的）。开轴器是可乐蛙的什么CNC工艺制作的，可开不同类型的轴体，很有分量和设计感，两部分有磁吸设计，合体后能作为试轴器一枚，个人很喜欢。\n\n![P1040127](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040127.JPG)\n\n![P1040126](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040126.JPG)\n\n![P1040128](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040128.JPG)\n\n![P1040131](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040131.JPG)\n\n这次的快递到的时候，快递盒里有一些泡泡纸，我想着这键盘 PCB 与 底壳中间还有一段空间，不如把泡泡纸塞在里面。塞完后装回去，仿佛有了那么一丢丢的声音提升（\n\n![P1040136](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040136.JPG)\n\n接下来就是：开始很新鲜，后来有些枯燥的润轴过程了。也不知道自己怎么有耐心的，就能坐着一个一个拆开，然后一片一片刷上润滑脂。彼时天气已经开始转热，寝室开起了空调，我听着音乐，悠然自得地润着轴，心情又变得很愉快起来。\n\n![image-20210512055059071](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20210512055059071.png)\n\n![P1040105](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040105.JPG)\n\n经历了漫长的润滑过程，所有轴体被我装回了键盘，扣回键帽，上手体验一番。到这时候我终于才体会到客制化键盘带来的结果上的愉悦：润滑之后的键盘不单单是敲击手感上有了质的飞跃，键盘音也有了十分可观的改善，可以称得上是悦耳了。具体的打字音如何，后面会拍一个视频，自行体会吧。\n\n## 总结\n\n### 满意之处\n\n执行力还是不错的，当时想要完成的内容基本都完成了，没有因为偷懒省去某个步骤，最终成品的完成度令我十分满意。\n\n热插拔轴座使得我不断体验、调整和更换轴体的过程变得轻松惬意。\n\n所有购入的物品在我看来基本都算是物有所值，没有让我比较失望的购买内容。\n\n### 不足之处\n\n这个套件已经是能看的程度了，虽然有些地方还是想吐槽，比如这个套件的底部，廉价感还是很足：\n\n![P1040145](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040145.JPG)\n\n这个套件也没有脚撑（当然很多高端套件也没有），只有两个磁吸增高垫 ，不过200块钱，要什么自行车嘛!\n\n然后就是本轻微强迫症患者比较头疼的部分：这个键帽也有一些问题，部分键的高度放在键盘上很奇怪，于是我只能使用其它键代替，于是就有了右侧这个神秘的区域：\n\n![P1040125](https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040125.JPG)\n\n真的不是我故意要这么做的，而是原装的键帽放在上面高低不平实在过于违和。这里下面的 `PgUp` 与 `PgDn` 实际上是 `Home` 和 `End` 键，而右上角的 `ScrollLock` 键实际上应该是 `Delete` 键。日常使用当然没有什么问题，这个键盘的布局算是比较常规的，因此没有给我带来太大的困扰，于是就这么凑合了。\n\n## 购物清单（仅包含实装在此键盘上的项目）\n\n| 物品                     | 店家             | 数量   | 价格   |\n| ------------------------ | ---------------- | ------ | ------ |\n| aws84键三模白光套件      | 爱外设旗舰店     | 1      | 209    |\n| 84布局消音棉             | 致命毒物ToxicLab | 1      | 16.9   |\n| 佳达隆白色钢板卫星轴     | 怡科外设KBDfans  | 1      | 48     |\n| pbt原厂日文键帽          | 怡科外设KBDfans  | 1      | 202.43 |\n| KBD75键盘包              | 怡科外设KBDfans  | 1      | 104    |\n| TTC月白轴                | 兴伟达电子       | 60     | 177.6  |\n| TTC赤瞳轴                | 兴伟达电子       | 10     | 36     |\n| 凯华奶油轴               | 兴伟达电子       | 4      | 18.2   |\n| TTC快银轴                | ttc旗舰店        | 10     | 45     |\n| 电话线螺旋机械键盘数据线 | 酷品世家         | 1      | 49     |\n| 头文字D 贴纸             | FBB视物所        | 1      | 20     |\n|                          |                  | 总计： | 926.13 |\n\n真不愧是 **“氪”制化键盘** 啊!\n\n## 附\n\n*本文章中图片均由本人使用 小米9 与 松下GX85(由czw赞助) 拍摄，没有经过任何修图。*\n\n*购物清单中的价格仅供参考，实际购买可能会有价格浮动*\n\n*Blog : <https://1099255210.github.io/>*\n\n*Bilibili : [不是吴昊的wh](https://space.bilibili.com/7405917)*\n\n","slug":"键盘笔记一","published":1,"updated":"2022-08-25T13:13:32.988Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clgyiaru4000pjku7ai3cgfmm","content":"<h1 id=\"键盘组装笔记-202105\"><a href=\"#键盘组装笔记-202105\" class=\"headerlink\" title=\"键盘组装笔记 202105\"></a>键盘组装笔记 202105</h1><p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1030789.JPG\" alt=\"P1030789\"></p>\n<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>这把键盘是我入坑客制化键盘的第二把键盘。第一把键盘我并未想要做过多总结，主要原因在于当时的我对于客制化的了解并不多，虽然上手就采用了比较硬核的焊接方式，中间也有一些有意思的组装过程，但终究没有太多客制化的意味在里面，仅仅算是尝了个鲜；而这回则是一次从内到外的客制化流程体验，让我更多地感受到了客制化的魅力所在，也算是第一次知道键盘的优秀手感能有多让人上瘾。这次的键盘我从 2021 四月初开始筹划，到五月份彻底完成，前前后后是经过很多的调整与修改的，下面我就慢慢讲述整个键盘组装的过程。</p>\n<h2 id=\"时间线\"><a href=\"#时间线\" class=\"headerlink\" title=\"时间线\"></a>时间线</h2><h3 id=\"第一批购物\"><a href=\"#第一批购物\" class=\"headerlink\" title=\"第一批购物\"></a>第一批购物</h3><p>最初购买了 aws84键三模白光版本 的套件，包含 外壳、定位板、PCB、卫星轴、热插拔轴座，同时购买了多款我比较好奇的 TTC轴体，回来装在套件上体验之后我决定使用月白轴作为主要键位使用的轴体、赤瞳ACE轴作为其他键位使用的轴体。接着我购买了 ToxicLab 的 84布局消音棉，Kelowna 的 润轴笔、22058润滑脂、拔轴器、铁氟龙胶带，KBDfans 的佳达隆白色钢板卫星轴。</p>\n<p>我做的第一件事是换卫星轴，原套件中是包含卫星轴的，但是那款卫星轴是白色略透，而定位板是白色铁板，颜色有些不符合。换成 佳达隆变色钢板卫星轴，颜色匹配多了。卫星轴需要剪脚，并拆开，使用 22058润滑脂 进行重新润滑，润滑脂加的可能有一些多，不过最终的效果我还是比较满意的。佳达隆的卫星轴卡在这个定位板上比较紧，卡上去就不是很好拆下来了。卫星轴在PCB上对应的位置我贴上了铁氟龙胶带，效果更佳稳健，噪声更小。</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040135.JPG\" alt=\"P1040135\"></p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040133.JPG\" alt=\"P1040133\"></p>\n<p>接着是消音棉的处理。这一步其实出现了一点问题：我购买的84布局消音棉（毛毡）实际上和我的套件在螺丝孔位上不是很匹配，炸虾粒 店里应该有为我这个套件定制的消音棉。但是既然都已经买了，就不想再退回去了，反正差不多，我直接对着PCB与定位板之间的孔位在毛毡上开孔，剪开了一些不需要的部分，最后比较顺利地把消音棉夹在了两者之间。唯一的问题是空格键下方的消音棉直接被我抛弃了，也许会产生一些额外的空腔音，不过总体是填满了。</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/IMG_20210411_180650.jpg\" alt=\"IMG_20210411_180650\"></p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/IMG_20210411_181243.jpg\" alt=\"IMG_20210411_181243\"></p>\n<p>下一步就是装上轴体，没什么好说的。字母区与数字区全部采用 TTC月白轴 ，其余键位均使用线性轴，比如 TTC赤瞳轴，金红轴，凯华奶油轴。线材购买了 酷品世家 的 灰色加粗螺旋线Type-C接口，键帽选的是 KBDfans 的 pbt原厂深灰+浅灰蓝日文键帽，这套键帽算是我买过最贵的键帽了，但是实际效果很让我满意。</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040139.JPG\" alt=\"P1040139\"></p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/MVIMG_20210414_182904.jpg\" alt=\"MVIMG_20210414_182904\"></p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/IMG_20210414_125210.jpg\" alt=\"IMG_20210414_125210\"></p>\n<p>至此，整部键盘第一次完整地出现在我眼前了。键盘的整体上手的时候还是很让我惊艳的，月白轴的手感在当时的我看来十分新鲜，回弹的手感非常带感。键帽的选择不仅带来手感上的舒适，也很养眼。键盘外框可以简易拆卸，制造悬浮键的视觉效果。</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1030796.JPG\" alt=\"P1030796\"></p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/IMG_20210414_190403.jpg\" alt=\"IMG_20210414_190403\"></p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040124.JPG\" alt=\"P1040124\"></p>\n<h3 id=\"第二批购物\"><a href=\"#第二批购物\" class=\"headerlink\" title=\"第二批购物\"></a>第二批购物</h3><p>没过几天，我又在b站上看到了几个润轴的视频，于是重新打开淘宝，又进行了一轮的购物。这一次为了键盘的“完整性”，购买了 KBDfans 的 KBD75键盘包，然后是 FBB视物所 的 头文字D 贴纸，贴在键盘外框上，为键盘注入了一波灵魂。最后是让我这次客制化感到极具体验的润滑套装：105与205调配润滑脂、轴体开盖器 与 润轴板。</p>\n<p>键盘包质量很不错，除了出厂有些许异味，内部绒毛材质很不错，做工精良，基本完全包裹键盘外壳，好评。</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040141.JPG\" alt=\"P1040141\"></p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040143.JPG\" alt=\"P1040143\"></p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040144.JPG\" alt=\"P1040144\"></p>\n<p>贴纸比想象中质量要好（毕竟也是要二十的），贴纸并不是一整块印刷，而是实体切割的，每个笔画都有凸起的质感，贴完立马有内味了是不是？</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040147.JPG\" alt=\"P1040147\"></p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040146.JPG\" alt=\"P1040146\"></p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040118.JPG\" alt=\"P1040118\"></p>\n<p>关于润滑套装，润轴板我直接一步到位上了个顶配：毕竟顶配也就三十几的亚克力，槽位多多益善嘛。润滑脂现在总共是有三种：从左至右分别是 22058、105 &amp; 205 MIX和105，分别对应卫星轴、轴体与弹簧的润滑（其中105因为我懒的原因还没有使用，轴体和弹簧我都是用中间的 “万金油” 润滑的）。开轴器是可乐蛙的什么CNC工艺制作的，可开不同类型的轴体，很有分量和设计感，两部分有磁吸设计，合体后能作为试轴器一枚，个人很喜欢。</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040127.JPG\" alt=\"P1040127\"></p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040126.JPG\" alt=\"P1040126\"></p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040128.JPG\" alt=\"P1040128\"></p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040131.JPG\" alt=\"P1040131\"></p>\n<p>这次的快递到的时候，快递盒里有一些泡泡纸，我想着这键盘 PCB 与 底壳中间还有一段空间，不如把泡泡纸塞在里面。塞完后装回去，仿佛有了那么一丢丢的声音提升（</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040136.JPG\" alt=\"P1040136\"></p>\n<p>接下来就是：开始很新鲜，后来有些枯燥的润轴过程了。也不知道自己怎么有耐心的，就能坐着一个一个拆开，然后一片一片刷上润滑脂。彼时天气已经开始转热，寝室开起了空调，我听着音乐，悠然自得地润着轴，心情又变得很愉快起来。</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20210512055059071.png\" alt=\"image-20210512055059071\"></p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040105.JPG\" alt=\"P1040105\"></p>\n<p>经历了漫长的润滑过程，所有轴体被我装回了键盘，扣回键帽，上手体验一番。到这时候我终于才体会到客制化键盘带来的结果上的愉悦：润滑之后的键盘不单单是敲击手感上有了质的飞跃，键盘音也有了十分可观的改善，可以称得上是悦耳了。具体的打字音如何，后面会拍一个视频，自行体会吧。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><h3 id=\"满意之处\"><a href=\"#满意之处\" class=\"headerlink\" title=\"满意之处\"></a>满意之处</h3><p>执行力还是不错的，当时想要完成的内容基本都完成了，没有因为偷懒省去某个步骤，最终成品的完成度令我十分满意。</p>\n<p>热插拔轴座使得我不断体验、调整和更换轴体的过程变得轻松惬意。</p>\n<p>所有购入的物品在我看来基本都算是物有所值，没有让我比较失望的购买内容。</p>\n<h3 id=\"不足之处\"><a href=\"#不足之处\" class=\"headerlink\" title=\"不足之处\"></a>不足之处</h3><p>这个套件已经是能看的程度了，虽然有些地方还是想吐槽，比如这个套件的底部，廉价感还是很足：</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040145.JPG\" alt=\"P1040145\"></p>\n<p>这个套件也没有脚撑（当然很多高端套件也没有），只有两个磁吸增高垫 ，不过200块钱，要什么自行车嘛!</p>\n<p>然后就是本轻微强迫症患者比较头疼的部分：这个键帽也有一些问题，部分键的高度放在键盘上很奇怪，于是我只能使用其它键代替，于是就有了右侧这个神秘的区域：</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040125.JPG\" alt=\"P1040125\"></p>\n<p>真的不是我故意要这么做的，而是原装的键帽放在上面高低不平实在过于违和。这里下面的 <code>PgUp</code> 与 <code>PgDn</code> 实际上是 <code>Home</code> 和 <code>End</code> 键，而右上角的 <code>ScrollLock</code> 键实际上应该是 <code>Delete</code> 键。日常使用当然没有什么问题，这个键盘的布局算是比较常规的，因此没有给我带来太大的困扰，于是就这么凑合了。</p>\n<h2 id=\"购物清单（仅包含实装在此键盘上的项目）\"><a href=\"#购物清单（仅包含实装在此键盘上的项目）\" class=\"headerlink\" title=\"购物清单（仅包含实装在此键盘上的项目）\"></a>购物清单（仅包含实装在此键盘上的项目）</h2><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>物品</th>\n<th>店家</th>\n<th>数量</th>\n<th>价格</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>aws84键三模白光套件</td>\n<td>爱外设旗舰店</td>\n<td>1</td>\n<td>209</td>\n</tr>\n<tr>\n<td>84布局消音棉</td>\n<td>致命毒物ToxicLab</td>\n<td>1</td>\n<td>16.9</td>\n</tr>\n<tr>\n<td>佳达隆白色钢板卫星轴</td>\n<td>怡科外设KBDfans</td>\n<td>1</td>\n<td>48</td>\n</tr>\n<tr>\n<td>pbt原厂日文键帽</td>\n<td>怡科外设KBDfans</td>\n<td>1</td>\n<td>202.43</td>\n</tr>\n<tr>\n<td>KBD75键盘包</td>\n<td>怡科外设KBDfans</td>\n<td>1</td>\n<td>104</td>\n</tr>\n<tr>\n<td>TTC月白轴</td>\n<td>兴伟达电子</td>\n<td>60</td>\n<td>177.6</td>\n</tr>\n<tr>\n<td>TTC赤瞳轴</td>\n<td>兴伟达电子</td>\n<td>10</td>\n<td>36</td>\n</tr>\n<tr>\n<td>凯华奶油轴</td>\n<td>兴伟达电子</td>\n<td>4</td>\n<td>18.2</td>\n</tr>\n<tr>\n<td>TTC快银轴</td>\n<td>ttc旗舰店</td>\n<td>10</td>\n<td>45</td>\n</tr>\n<tr>\n<td>电话线螺旋机械键盘数据线</td>\n<td>酷品世家</td>\n<td>1</td>\n<td>49</td>\n</tr>\n<tr>\n<td>头文字D 贴纸</td>\n<td>FBB视物所</td>\n<td>1</td>\n<td>20</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n<td>总计：</td>\n<td>926.13</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>真不愧是 <strong>“氪”制化键盘</strong> 啊!</p>\n<h2 id=\"附\"><a href=\"#附\" class=\"headerlink\" title=\"附\"></a>附</h2><p><em>本文章中图片均由本人使用 小米9 与 松下GX85(由czw赞助) 拍摄，没有经过任何修图。</em></p>\n<p><em>购物清单中的价格仅供参考，实际购买可能会有价格浮动</em></p>\n<p><em>Blog : <a href=\"https://1099255210.github.io/\">https://1099255210.github.io/</a></em></p>\n<p><em>Bilibili : <a href=\"https://space.bilibili.com/7405917\">不是吴昊的wh</a></em></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"键盘组装笔记-202105\"><a href=\"#键盘组装笔记-202105\" class=\"headerlink\" title=\"键盘组装笔记 202105\"></a>键盘组装笔记 202105</h1><p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1030789.JPG\" alt=\"P1030789\"></p>\n<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>这把键盘是我入坑客制化键盘的第二把键盘。第一把键盘我并未想要做过多总结，主要原因在于当时的我对于客制化的了解并不多，虽然上手就采用了比较硬核的焊接方式，中间也有一些有意思的组装过程，但终究没有太多客制化的意味在里面，仅仅算是尝了个鲜；而这回则是一次从内到外的客制化流程体验，让我更多地感受到了客制化的魅力所在，也算是第一次知道键盘的优秀手感能有多让人上瘾。这次的键盘我从 2021 四月初开始筹划，到五月份彻底完成，前前后后是经过很多的调整与修改的，下面我就慢慢讲述整个键盘组装的过程。</p>\n<h2 id=\"时间线\"><a href=\"#时间线\" class=\"headerlink\" title=\"时间线\"></a>时间线</h2><h3 id=\"第一批购物\"><a href=\"#第一批购物\" class=\"headerlink\" title=\"第一批购物\"></a>第一批购物</h3><p>最初购买了 aws84键三模白光版本 的套件，包含 外壳、定位板、PCB、卫星轴、热插拔轴座，同时购买了多款我比较好奇的 TTC轴体，回来装在套件上体验之后我决定使用月白轴作为主要键位使用的轴体、赤瞳ACE轴作为其他键位使用的轴体。接着我购买了 ToxicLab 的 84布局消音棉，Kelowna 的 润轴笔、22058润滑脂、拔轴器、铁氟龙胶带，KBDfans 的佳达隆白色钢板卫星轴。</p>\n<p>我做的第一件事是换卫星轴，原套件中是包含卫星轴的，但是那款卫星轴是白色略透，而定位板是白色铁板，颜色有些不符合。换成 佳达隆变色钢板卫星轴，颜色匹配多了。卫星轴需要剪脚，并拆开，使用 22058润滑脂 进行重新润滑，润滑脂加的可能有一些多，不过最终的效果我还是比较满意的。佳达隆的卫星轴卡在这个定位板上比较紧，卡上去就不是很好拆下来了。卫星轴在PCB上对应的位置我贴上了铁氟龙胶带，效果更佳稳健，噪声更小。</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040135.JPG\" alt=\"P1040135\"></p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040133.JPG\" alt=\"P1040133\"></p>\n<p>接着是消音棉的处理。这一步其实出现了一点问题：我购买的84布局消音棉（毛毡）实际上和我的套件在螺丝孔位上不是很匹配，炸虾粒 店里应该有为我这个套件定制的消音棉。但是既然都已经买了，就不想再退回去了，反正差不多，我直接对着PCB与定位板之间的孔位在毛毡上开孔，剪开了一些不需要的部分，最后比较顺利地把消音棉夹在了两者之间。唯一的问题是空格键下方的消音棉直接被我抛弃了，也许会产生一些额外的空腔音，不过总体是填满了。</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/IMG_20210411_180650.jpg\" alt=\"IMG_20210411_180650\"></p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/IMG_20210411_181243.jpg\" alt=\"IMG_20210411_181243\"></p>\n<p>下一步就是装上轴体，没什么好说的。字母区与数字区全部采用 TTC月白轴 ，其余键位均使用线性轴，比如 TTC赤瞳轴，金红轴，凯华奶油轴。线材购买了 酷品世家 的 灰色加粗螺旋线Type-C接口，键帽选的是 KBDfans 的 pbt原厂深灰+浅灰蓝日文键帽，这套键帽算是我买过最贵的键帽了，但是实际效果很让我满意。</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040139.JPG\" alt=\"P1040139\"></p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/MVIMG_20210414_182904.jpg\" alt=\"MVIMG_20210414_182904\"></p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/IMG_20210414_125210.jpg\" alt=\"IMG_20210414_125210\"></p>\n<p>至此，整部键盘第一次完整地出现在我眼前了。键盘的整体上手的时候还是很让我惊艳的，月白轴的手感在当时的我看来十分新鲜，回弹的手感非常带感。键帽的选择不仅带来手感上的舒适，也很养眼。键盘外框可以简易拆卸，制造悬浮键的视觉效果。</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1030796.JPG\" alt=\"P1030796\"></p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/IMG_20210414_190403.jpg\" alt=\"IMG_20210414_190403\"></p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040124.JPG\" alt=\"P1040124\"></p>\n<h3 id=\"第二批购物\"><a href=\"#第二批购物\" class=\"headerlink\" title=\"第二批购物\"></a>第二批购物</h3><p>没过几天，我又在b站上看到了几个润轴的视频，于是重新打开淘宝，又进行了一轮的购物。这一次为了键盘的“完整性”，购买了 KBDfans 的 KBD75键盘包，然后是 FBB视物所 的 头文字D 贴纸，贴在键盘外框上，为键盘注入了一波灵魂。最后是让我这次客制化感到极具体验的润滑套装：105与205调配润滑脂、轴体开盖器 与 润轴板。</p>\n<p>键盘包质量很不错，除了出厂有些许异味，内部绒毛材质很不错，做工精良，基本完全包裹键盘外壳，好评。</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040141.JPG\" alt=\"P1040141\"></p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040143.JPG\" alt=\"P1040143\"></p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040144.JPG\" alt=\"P1040144\"></p>\n<p>贴纸比想象中质量要好（毕竟也是要二十的），贴纸并不是一整块印刷，而是实体切割的，每个笔画都有凸起的质感，贴完立马有内味了是不是？</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040147.JPG\" alt=\"P1040147\"></p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040146.JPG\" alt=\"P1040146\"></p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040118.JPG\" alt=\"P1040118\"></p>\n<p>关于润滑套装，润轴板我直接一步到位上了个顶配：毕竟顶配也就三十几的亚克力，槽位多多益善嘛。润滑脂现在总共是有三种：从左至右分别是 22058、105 &amp; 205 MIX和105，分别对应卫星轴、轴体与弹簧的润滑（其中105因为我懒的原因还没有使用，轴体和弹簧我都是用中间的 “万金油” 润滑的）。开轴器是可乐蛙的什么CNC工艺制作的，可开不同类型的轴体，很有分量和设计感，两部分有磁吸设计，合体后能作为试轴器一枚，个人很喜欢。</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040127.JPG\" alt=\"P1040127\"></p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040126.JPG\" alt=\"P1040126\"></p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040128.JPG\" alt=\"P1040128\"></p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040131.JPG\" alt=\"P1040131\"></p>\n<p>这次的快递到的时候，快递盒里有一些泡泡纸，我想着这键盘 PCB 与 底壳中间还有一段空间，不如把泡泡纸塞在里面。塞完后装回去，仿佛有了那么一丢丢的声音提升（</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040136.JPG\" alt=\"P1040136\"></p>\n<p>接下来就是：开始很新鲜，后来有些枯燥的润轴过程了。也不知道自己怎么有耐心的，就能坐着一个一个拆开，然后一片一片刷上润滑脂。彼时天气已经开始转热，寝室开起了空调，我听着音乐，悠然自得地润着轴，心情又变得很愉快起来。</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/image-20210512055059071.png\" alt=\"image-20210512055059071\"></p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040105.JPG\" alt=\"P1040105\"></p>\n<p>经历了漫长的润滑过程，所有轴体被我装回了键盘，扣回键帽，上手体验一番。到这时候我终于才体会到客制化键盘带来的结果上的愉悦：润滑之后的键盘不单单是敲击手感上有了质的飞跃，键盘音也有了十分可观的改善，可以称得上是悦耳了。具体的打字音如何，后面会拍一个视频，自行体会吧。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><h3 id=\"满意之处\"><a href=\"#满意之处\" class=\"headerlink\" title=\"满意之处\"></a>满意之处</h3><p>执行力还是不错的，当时想要完成的内容基本都完成了，没有因为偷懒省去某个步骤，最终成品的完成度令我十分满意。</p>\n<p>热插拔轴座使得我不断体验、调整和更换轴体的过程变得轻松惬意。</p>\n<p>所有购入的物品在我看来基本都算是物有所值，没有让我比较失望的购买内容。</p>\n<h3 id=\"不足之处\"><a href=\"#不足之处\" class=\"headerlink\" title=\"不足之处\"></a>不足之处</h3><p>这个套件已经是能看的程度了，虽然有些地方还是想吐槽，比如这个套件的底部，廉价感还是很足：</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040145.JPG\" alt=\"P1040145\"></p>\n<p>这个套件也没有脚撑（当然很多高端套件也没有），只有两个磁吸增高垫 ，不过200块钱，要什么自行车嘛!</p>\n<p>然后就是本轻微强迫症患者比较头疼的部分：这个键帽也有一些问题，部分键的高度放在键盘上很奇怪，于是我只能使用其它键代替，于是就有了右侧这个神秘的区域：</p>\n<p><img src=\"https://hgserver-1301261080.cos.ap-seoul.myqcloud.com/snipaste/P1040125.JPG\" alt=\"P1040125\"></p>\n<p>真的不是我故意要这么做的，而是原装的键帽放在上面高低不平实在过于违和。这里下面的 <code>PgUp</code> 与 <code>PgDn</code> 实际上是 <code>Home</code> 和 <code>End</code> 键，而右上角的 <code>ScrollLock</code> 键实际上应该是 <code>Delete</code> 键。日常使用当然没有什么问题，这个键盘的布局算是比较常规的，因此没有给我带来太大的困扰，于是就这么凑合了。</p>\n<h2 id=\"购物清单（仅包含实装在此键盘上的项目）\"><a href=\"#购物清单（仅包含实装在此键盘上的项目）\" class=\"headerlink\" title=\"购物清单（仅包含实装在此键盘上的项目）\"></a>购物清单（仅包含实装在此键盘上的项目）</h2><div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>物品</th>\n<th>店家</th>\n<th>数量</th>\n<th>价格</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>aws84键三模白光套件</td>\n<td>爱外设旗舰店</td>\n<td>1</td>\n<td>209</td>\n</tr>\n<tr>\n<td>84布局消音棉</td>\n<td>致命毒物ToxicLab</td>\n<td>1</td>\n<td>16.9</td>\n</tr>\n<tr>\n<td>佳达隆白色钢板卫星轴</td>\n<td>怡科外设KBDfans</td>\n<td>1</td>\n<td>48</td>\n</tr>\n<tr>\n<td>pbt原厂日文键帽</td>\n<td>怡科外设KBDfans</td>\n<td>1</td>\n<td>202.43</td>\n</tr>\n<tr>\n<td>KBD75键盘包</td>\n<td>怡科外设KBDfans</td>\n<td>1</td>\n<td>104</td>\n</tr>\n<tr>\n<td>TTC月白轴</td>\n<td>兴伟达电子</td>\n<td>60</td>\n<td>177.6</td>\n</tr>\n<tr>\n<td>TTC赤瞳轴</td>\n<td>兴伟达电子</td>\n<td>10</td>\n<td>36</td>\n</tr>\n<tr>\n<td>凯华奶油轴</td>\n<td>兴伟达电子</td>\n<td>4</td>\n<td>18.2</td>\n</tr>\n<tr>\n<td>TTC快银轴</td>\n<td>ttc旗舰店</td>\n<td>10</td>\n<td>45</td>\n</tr>\n<tr>\n<td>电话线螺旋机械键盘数据线</td>\n<td>酷品世家</td>\n<td>1</td>\n<td>49</td>\n</tr>\n<tr>\n<td>头文字D 贴纸</td>\n<td>FBB视物所</td>\n<td>1</td>\n<td>20</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n<td>总计：</td>\n<td>926.13</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>真不愧是 <strong>“氪”制化键盘</strong> 啊!</p>\n<h2 id=\"附\"><a href=\"#附\" class=\"headerlink\" title=\"附\"></a>附</h2><p><em>本文章中图片均由本人使用 小米9 与 松下GX85(由czw赞助) 拍摄，没有经过任何修图。</em></p>\n<p><em>购物清单中的价格仅供参考，实际购买可能会有价格浮动</em></p>\n<p><em>Blog : <a href=\"https://1099255210.github.io/\">https://1099255210.github.io/</a></em></p>\n<p><em>Bilibili : <a href=\"https://space.bilibili.com/7405917\">不是吴昊的wh</a></em></p>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[],"Tag":[]}}